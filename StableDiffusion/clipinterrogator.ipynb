{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## CLIP Interrogator, OFA and ViT model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:47:04.652429Z","iopub.status.busy":"2023-04-17T10:47:04.651644Z","iopub.status.idle":"2023-04-17T10:47:04.679286Z","shell.execute_reply":"2023-04-17T10:47:04.678381Z","shell.execute_reply.started":"2023-04-17T10:47:04.652347Z"},"trusted":true},"outputs":[],"source":["ratio_ViT_B_16          = 0.74880\n","ratio_CLIP_Interrogator = 0.21120\n","ratio_OFA               = 0.04000"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:47:04.681639Z","iopub.status.busy":"2023-04-17T10:47:04.681268Z","iopub.status.idle":"2023-04-17T10:47:42.254421Z","shell.execute_reply":"2023-04-17T10:47:42.253253Z","shell.execute_reply.started":"2023-04-17T10:47:04.681592Z"},"trusted":true},"outputs":[],"source":["%%catch\n","!pip install -q /kaggle/input/stable-diffusion-data/transformers-4.18.0.dev0-py3-none-any.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:47:42.256605Z","iopub.status.busy":"2023-04-17T10:47:42.256206Z","iopub.status.idle":"2023-04-17T10:47:44.610793Z","shell.execute_reply":"2023-04-17T10:47:44.609881Z","shell.execute_reply.started":"2023-04-17T10:47:42.256565Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import glob\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from transformers import OFATokenizer, OFAModel\n","from transformers.models.ofa.generate import sequence_generator\n","\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:47:44.614324Z","iopub.status.busy":"2023-04-17T10:47:44.613674Z","iopub.status.idle":"2023-04-17T10:47:44.620459Z","shell.execute_reply":"2023-04-17T10:47:44.619066Z","shell.execute_reply.started":"2023-04-17T10:47:44.614285Z"},"trusted":true},"outputs":[],"source":["CKPT_DIR = \"/kaggle/input/stable-diffusion-data/OFA-large-caption/\"\n","IMAGE_DIR = \"/kaggle/input/stable-diffusion-image-to-prompts/images\"\n","\n","BATCH_SIZE = 24"]},{"cell_type":"markdown","metadata":{},"source":["# Loading the pretrained OFA model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:47:44.623503Z","iopub.status.busy":"2023-04-17T10:47:44.622768Z","iopub.status.idle":"2023-04-17T10:48:13.842717Z","shell.execute_reply":"2023-04-17T10:48:13.841733Z","shell.execute_reply.started":"2023-04-17T10:47:44.623466Z"},"trusted":true},"outputs":[],"source":["mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n","resolution = 480\n","patch_resize_transform = transforms.Compose([\n","        lambda image: image.convert(\"RGB\"),\n","        transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n","        transforms.ToTensor(), \n","        transforms.Normalize(mean=mean, std=std)\n","    ])\n","\n","tokenizer = OFATokenizer.from_pretrained(CKPT_DIR)\n","model = OFAModel.from_pretrained(CKPT_DIR, use_cache=False).cuda()\n","txt = \" what does the image describe?\"\n","inputs = tokenizer([txt], return_tensors=\"pt\").input_ids"]},{"cell_type":"markdown","metadata":{},"source":["# Model EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:48:13.844682Z","iopub.status.busy":"2023-04-17T10:48:13.844232Z","iopub.status.idle":"2023-04-17T10:48:34.246584Z","shell.execute_reply":"2023-04-17T10:48:34.245231Z","shell.execute_reply.started":"2023-04-17T10:48:13.844645Z"},"trusted":true},"outputs":[],"source":["sample_images = glob.glob(\"/kaggle/input/stable-diffusion-image-to-prompts/images/*\")[:7]\n","fig, ax = plt.subplots(7,1, figsize=(4,35))\n","\n","for i,impath in enumerate(sample_images):\n","    image = Image.open(impath)\n","    image_t = patch_resize_transform(image).cuda().unsqueeze(0)\n","    out = model.generate(inputs.cuda(), patch_images=image_t.cuda(), num_beams=5, no_repeat_ngram_size=2)\n","    out_captions = tokenizer.batch_decode(out, skip_special_tokens=True)\n","    ax[i].imshow(image)\n","    ax[i].text(1.1, .5, out_captions[0], horizontalalignment='left', verticalalignment='center', transform=ax[i].transAxes)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:48:34.248201Z","iopub.status.busy":"2023-04-17T10:48:34.247876Z","iopub.status.idle":"2023-04-17T10:48:36.979780Z","shell.execute_reply":"2023-04-17T10:48:36.978780Z","shell.execute_reply.started":"2023-04-17T10:48:34.248170Z"},"trusted":true},"outputs":[],"source":["sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n","from sentence_transformers import SentenceTransformer, models\n","\n","comp_path = Path('../input/stable-diffusion-image-to-prompts/')\n","st_model = SentenceTransformer('/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:48:36.982187Z","iopub.status.busy":"2023-04-17T10:48:36.981495Z","iopub.status.idle":"2023-04-17T10:48:36.992871Z","shell.execute_reply":"2023-04-17T10:48:36.991310Z","shell.execute_reply.started":"2023-04-17T10:48:36.982148Z"},"trusted":true},"outputs":[],"source":["class ImageGen(Dataset):\n","    def __init__(self, root, batch_size=32):\n","        self.root = root\n","        self.im_paths = os.listdir(self.root)\n","        self.batch_size = batch_size\n","        self.sz = len(self.im_paths)\n","        self.genlen = self.sz//self.batch_size + int(self.sz%self.batch_size > 0)\n","        \n","    def __getitem__(self, index):\n","        if index >= self.genlen:\n","            raise IndexError(\"Out of bounds\")\n","        \n","        l, r = index*self.batch_size, min(self.sz, (index+1)*self.batch_size)\n","        \n","        f_paths = [os.path.join(self.root, self.im_paths[i]) for i in range(l,r)]\n","        f_ids = [self.im_paths[i][:-4] for i in range(l,r)]\n","        \n","        ims = [Image.open(f_path) for f_path in f_paths]\n","        ims = [patch_resize_transform(im).cuda().unsqueeze(0) for im in ims]\n","        ims = torch.cat(ims)\n","        \n","        return ims, f_ids\n","    \n","    def __len__(self):\n","        return self.genlen"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:48:36.995984Z","iopub.status.busy":"2023-04-17T10:48:36.995097Z","iopub.status.idle":"2023-04-17T10:48:43.414618Z","shell.execute_reply":"2023-04-17T10:48:43.413515Z","shell.execute_reply.started":"2023-04-17T10:48:36.995948Z"},"trusted":true},"outputs":[],"source":["sub_ids = []\n","sub_embeds = []\n","\n","imgen = ImageGen(IMAGE_DIR, BATCH_SIZE)\n","\n","for b in imgen:\n","    for j in range(len(b[1])):\n","        sub_ids.extend([f\"{b[1][j]}_{i}\" for i in range(384)])\n","    \n","    img_batch = b[0]\n","    out = model.generate(inputs.repeat(len(img_batch), 1).cuda(), patch_images=img_batch, num_beams=5, no_repeat_ngram_size=2)\n","    out_captions = tokenizer.batch_decode(out, skip_special_tokens=True)\n","    out_captions = [cap + \", fine details, masterpiece\" for cap in out_captions]\n","    \n","    embeddings = st_model.encode(out_captions).flatten()\n","    sub_embeds.extend(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:48:43.419583Z","iopub.status.busy":"2023-04-17T10:48:43.419100Z","iopub.status.idle":"2023-04-17T10:48:43.426894Z","shell.execute_reply":"2023-04-17T10:48:43.425899Z","shell.execute_reply.started":"2023-04-17T10:48:43.419542Z"},"trusted":true},"outputs":[],"source":["embeddings1 = np.array(sub_embeds)\n","embeddings1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:48:43.428915Z","iopub.status.busy":"2023-04-17T10:48:43.428101Z","iopub.status.idle":"2023-04-17T10:48:43.795792Z","shell.execute_reply":"2023-04-17T10:48:43.794732Z","shell.execute_reply.started":"2023-04-17T10:48:43.428880Z"},"trusted":true},"outputs":[],"source":["del model, tokenizer, st_model\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# CLIP Interrogator information"]},{"cell_type":"markdown","metadata":{},"source":["<p style=\"font-family: consolas; font-size: 16px;\">âšª The CLIP Interrogator is a prompt engineering tool that combines OpenAI's <a href=\"https://openai.com/blog/clip/\"><strong>CLIP</strong></a> and Salesforce's <a href=\"https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/\"><strong>BLIP</strong></a> to optimize text prompts to match a given image.</p>\n","\n","<p style=\"font-family: consolas; font-size: 16px;\">âšª CLIP Interrogator uses OpenCLIP which supports many different pretrained CLIP models. For the best prompts for Stable Diffusion 2.0 uses <b>ViT-H-14/laion2b_s32b_b79k</b>.</p>\n","\n","\n","<p style=\"font-family: consolas; font-size: 16px;\">âšª CLIP Interrogator pipeline looks as follows:</p>\n","\n","* <p style=\"font-family: consolas; font-size: 16px;\">An image is passed to the input to BLIP to obtain the main description.</p>\n","\n","* <p style=\"font-family: consolas; font-size: 16px;\">An image is passed to the input to CLIP to receive its embedding.</p>\n","\n","* <p style=\"font-family: consolas; font-size: 16px;\">Embeddings received from the image are compared with embeddings received from labels from the lists and the top 4 with the greatest similarity are selected.</p>\n","<p style=\"font-family: consolas; font-size: 16px;\">There are 4 main lists on which the outgoing prompt for the CLIP part is formed: <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/artists.txt\"><strong>artists.txt</strong></a> (list with artists), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/flavors.txt\"><strong>flavors.txt</strong></a> (main list for image description), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/mediums.txt\"><strong>mediums.txt</strong></a> (image type), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/movements.txt\"><strong>movements.txt</strong></a> (image style) and <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/clip_interrogator.py#L115\"><strong>sites</strong></a> (popular artwork sites). As I wrote earlier, removing the <b>artists.txt</b> and the <b>sites</b> lists can significantly improve the output score.</p>\n","\n","* <p style=\"font-family: consolas; font-size: 16px;\">The resulting texts are concatenated and returned as an image description (or promt on which an image was generated).</p>\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p style=\"font-family: consolas; font-size: 16px;\">ðŸ”´ CLIP Interrogator pipeline, schematic image [<a href=\"https://medium.com/@silkworm/diversify-photo-database-with-clip-interrogator-5dd1833be9f5\"><strong>source</strong></a>]:</p>\n","\n","<img src=\"https://raw.githubusercontent.com/akscent/kaggle-competitions/main/StableDiffusion/img/scheme.PNG\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p style=\"font-family: consolas; font-size: 16px;\">âšª If you want, you can experiment with this tool on the Hugging Face Space -- *<a href=\"https://huggingface.co/spaces/pharma/CLIP-Interrogator\"><strong>Click</strong></a>*. Example of an output:</p>\n","\n","<img src=\"https://raw.githubusercontent.com/akscent/kaggle-competitions/main/StableDiffusion/img/example.PNG\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Install & Import dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:48:43.797767Z","iopub.status.busy":"2023-04-17T10:48:43.797369Z","iopub.status.idle":"2023-04-17T10:48:43.803048Z","shell.execute_reply":"2023-04-17T10:48:43.802008Z","shell.execute_reply.started":"2023-04-17T10:48:43.797717Z"},"trusted":true},"outputs":[],"source":["wheels_path = \"/kaggle/input/clip-interrogator-wheels-x\"\n","clip_interrogator_whl_path = f\"{wheels_path}/clip_interrogator-0.4.3-py3-none-any.whl\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-04-17T10:48:43.805251Z","iopub.status.busy":"2023-04-17T10:48:43.804472Z","iopub.status.idle":"2023-04-17T10:49:09.948587Z","shell.execute_reply":"2023-04-17T10:49:09.947358Z","shell.execute_reply.started":"2023-04-17T10:48:43.805209Z"},"trusted":true},"outputs":[],"source":["%%catch\n","!pip install --no-index --find-links $wheels_path $clip_interrogator_whl_path -q"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:49:09.952037Z","iopub.status.busy":"2023-04-17T10:49:09.950468Z","iopub.status.idle":"2023-04-17T10:49:33.378258Z","shell.execute_reply":"2023-04-17T10:49:33.376953Z","shell.execute_reply.started":"2023-04-17T10:49:09.951994Z"},"trusted":true},"outputs":[],"source":["!pip list | grep transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-04-17T10:49:33.380324Z","iopub.status.busy":"2023-04-17T10:49:33.379909Z","iopub.status.idle":"2023-04-17T10:49:34.401217Z","shell.execute_reply":"2023-04-17T10:49:34.400135Z","shell.execute_reply.started":"2023-04-17T10:49:33.380277Z"},"trusted":true},"outputs":[],"source":["import inspect\n","import importlib\n","\n","from blip.models import blip\n","from clip_interrogator import clip_interrogator"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-04-17T10:49:34.404587Z","iopub.status.busy":"2023-04-17T10:49:34.403346Z","iopub.status.idle":"2023-04-17T10:49:34.419393Z","shell.execute_reply":"2023-04-17T10:49:34.418240Z","shell.execute_reply.started":"2023-04-17T10:49:34.404544Z"},"trusted":true},"outputs":[],"source":["# replace tokenizer path to prevent downloading\n","blip_path = inspect.getfile(blip)\n","\n","fin = open(blip_path, \"rt\")\n","data = fin.read()\n","data = data.replace(\n","    \"BertTokenizer.from_pretrained('bert-base-uncased')\", \n","    \"BertTokenizer.from_pretrained('/kaggle/input/clip-interrogator-models-x/bert-base-uncased')\"\n",")\n","fin.close()\n","\n","fin = open(blip_path, \"wt\")\n","fin.write(data)\n","fin.close()\n","\n","importlib.reload(blip)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-04-17T10:49:34.421831Z","iopub.status.busy":"2023-04-17T10:49:34.421201Z","iopub.status.idle":"2023-04-17T10:49:34.441577Z","shell.execute_reply":"2023-04-17T10:49:34.440472Z","shell.execute_reply.started":"2023-04-17T10:49:34.421758Z"},"trusted":true},"outputs":[],"source":["# fix clip_interrogator bug\n","clip_interrogator_path = inspect.getfile(clip_interrogator.Interrogator)\n","\n","fin = open(clip_interrogator_path, \"rt\")\n","data = fin.read()\n","data = data.replace(\n","    'open_clip.get_tokenizer(clip_model_name)', \n","    'open_clip.get_tokenizer(config.clip_model_name.split(\"/\", 2)[0])'\n",")\n","fin.close()\n","\n","fin = open(clip_interrogator_path, \"wt\")\n","fin.write(data)\n","fin.close()\n","\n","importlib.reload(clip_interrogator)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:49:34.443916Z","iopub.status.busy":"2023-04-17T10:49:34.443519Z","iopub.status.idle":"2023-04-17T10:49:34.449578Z","shell.execute_reply":"2023-04-17T10:49:34.448594Z","shell.execute_reply.started":"2023-04-17T10:49:34.443880Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","from PIL import Image\n","from pathlib import Path\n","import matplotlib.pyplot as plt \n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import open_clip\n","\n","\n","sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n","from sentence_transformers import SentenceTransformer, models\n","\n","comp_path = Path('/kaggle/input/stable-diffusion-image-to-prompts/')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Set configs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:49:34.452025Z","iopub.status.busy":"2023-04-17T10:49:34.451326Z","iopub.status.idle":"2023-04-17T10:49:34.460067Z","shell.execute_reply":"2023-04-17T10:49:34.459137Z","shell.execute_reply.started":"2023-04-17T10:49:34.451984Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    device = \"cuda\"\n","    seed = 42\n","    embedding_length = 384\n","    sentence_model_path = \"/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2\"\n","    blip_model_path = \"/kaggle/input/clip-interrogator-models-x/model_large_caption.pth\"\n","    ci_clip_model_name = \"ViT-H-14/laion2b_s32b_b79k\"\n","    clip_model_name = \"ViT-H-14\"\n","    clip_model_path = \"/kaggle/input/clip-interrogator-models-x/CLIP-ViT-H-14-laion2B-s32B-b79K/open_clip_pytorch_model.bin\"\n","    cache_path = \"/kaggle/input/clip-interrogator-models-x\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Load Sample Sub"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:49:34.462324Z","iopub.status.busy":"2023-04-17T10:49:34.461709Z","iopub.status.idle":"2023-04-17T10:49:34.503618Z","shell.execute_reply":"2023-04-17T10:49:34.502658Z","shell.execute_reply.started":"2023-04-17T10:49:34.462249Z"},"trusted":true},"outputs":[],"source":["df_submission = pd.read_csv(comp_path / 'sample_submission.csv', index_col='imgId_eId')\n","df_submission.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Build index from images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:49:34.505479Z","iopub.status.busy":"2023-04-17T10:49:34.505063Z","iopub.status.idle":"2023-04-17T10:49:34.517510Z","shell.execute_reply":"2023-04-17T10:49:34.516475Z","shell.execute_reply.started":"2023-04-17T10:49:34.505444Z"},"trusted":true},"outputs":[],"source":["images = os.listdir(comp_path / 'images')\n","imgIds = [i.split('.')[0] for i in images]\n","\n","eIds = list(range(CFG.embedding_length))\n","\n","imgId_eId = [\n","    '_'.join(map(str, i)) for i in zip(\n","        np.repeat(imgIds, CFG.embedding_length),\n","        np.tile(range(CFG.embedding_length), len(imgIds))\n","    )\n","]\n","\n","assert sorted(imgId_eId) == sorted(df_submission.index)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:49:34.519877Z","iopub.status.busy":"2023-04-17T10:49:34.518824Z","iopub.status.idle":"2023-04-17T10:49:34.837098Z","shell.execute_reply":"2023-04-17T10:49:34.836128Z","shell.execute_reply.started":"2023-04-17T10:49:34.519839Z"},"trusted":true},"outputs":[],"source":["st_model = SentenceTransformer(CFG.sentence_model_path)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Define interrogate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:49:34.841458Z","iopub.status.busy":"2023-04-17T10:49:34.840476Z","iopub.status.idle":"2023-04-17T10:49:34.846453Z","shell.execute_reply":"2023-04-17T10:49:34.845508Z","shell.execute_reply.started":"2023-04-17T10:49:34.841418Z"},"trusted":true},"outputs":[],"source":["model_config = clip_interrogator.Config(clip_model_name=CFG.ci_clip_model_name)\n","model_config.cache_path = CFG.cache_path"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:49:34.849046Z","iopub.status.busy":"2023-04-17T10:49:34.847966Z","iopub.status.idle":"2023-04-17T10:50:03.406577Z","shell.execute_reply":"2023-04-17T10:50:03.405515Z","shell.execute_reply.started":"2023-04-17T10:49:34.849019Z"},"trusted":true},"outputs":[],"source":["configs_path = os.path.join(os.path.dirname(os.path.dirname(blip_path)), 'configs')\n","med_config = os.path.join(configs_path, 'med_config.json')\n","blip_model = blip.blip_decoder(\n","    pretrained=CFG.blip_model_path,\n","    image_size=model_config.blip_image_eval_size, \n","    vit=model_config.blip_model_type, \n","    med_config=med_config\n",")\n","blip_model.eval()\n","blip_model = blip_model.to(model_config.device)\n","model_config.blip_model = blip_model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:03.408613Z","iopub.status.busy":"2023-04-17T10:50:03.408231Z","iopub.status.idle":"2023-04-17T10:50:49.448371Z","shell.execute_reply":"2023-04-17T10:50:49.443853Z","shell.execute_reply.started":"2023-04-17T10:50:03.408577Z"},"trusted":true},"outputs":[],"source":["clip_model = open_clip.create_model(CFG.clip_model_name, precision='fp16' if model_config.device == 'cuda' else 'fp32')\n","open_clip.load_checkpoint(clip_model, CFG.clip_model_path)\n","clip_model.to(model_config.device).eval()\n","model_config.clip_model = clip_model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:49.455935Z","iopub.status.busy":"2023-04-17T10:50:49.454761Z","iopub.status.idle":"2023-04-17T10:50:49.471595Z","shell.execute_reply":"2023-04-17T10:50:49.469990Z","shell.execute_reply.started":"2023-04-17T10:50:49.455878Z"},"trusted":true},"outputs":[],"source":["clip_preprocess = open_clip.image_transform(\n","    clip_model.visual.image_size,\n","    is_train = False,\n","    mean = getattr(clip_model.visual, 'image_mean', None),\n","    std = getattr(clip_model.visual, 'image_std', None),\n",")\n","model_config.clip_preprocess = clip_preprocess"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:49.480975Z","iopub.status.busy":"2023-04-17T10:50:49.476934Z","iopub.status.idle":"2023-04-17T10:50:51.938466Z","shell.execute_reply":"2023-04-17T10:50:51.937433Z","shell.execute_reply.started":"2023-04-17T10:50:49.480903Z"},"trusted":true},"outputs":[],"source":["ci = clip_interrogator.Interrogator(model_config)"]},{"cell_type":"markdown","metadata":{},"source":["<p style=\"font-family: consolas; font-size: 16px;\">âšª Original CLIP Interrogator uses image_features and text_embeds matrix multiplication to fine the similarity between the corresponding image and text label. But I found that using cosine similarity is much faster and the resulting score is almost identical. So take that into account.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:51.945904Z","iopub.status.busy":"2023-04-17T10:50:51.945597Z","iopub.status.idle":"2023-04-17T10:50:52.617375Z","shell.execute_reply":"2023-04-17T10:50:52.616317Z","shell.execute_reply.started":"2023-04-17T10:50:51.945877Z"},"trusted":true},"outputs":[],"source":["cos = torch.nn.CosineSimilarity(dim=1)\n","\n","mediums_features_array = torch.stack([torch.from_numpy(t) for t in ci.mediums.embeds]).to(ci.device)\n","movements_features_array = torch.stack([torch.from_numpy(t) for t in ci.movements.embeds]).to(ci.device)\n","flavors_features_array = torch.stack([torch.from_numpy(t) for t in ci.flavors.embeds]).to(ci.device)"]},{"cell_type":"markdown","metadata":{},"source":["<p style=\"font-family: consolas; font-size: 16px;\">âšª It's modified version of the original <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/clip_interrogator.py#L213\"><strong>interrogate_classic</strong></a> method.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:52.619266Z","iopub.status.busy":"2023-04-17T10:50:52.618895Z","iopub.status.idle":"2023-04-17T10:50:52.627231Z","shell.execute_reply":"2023-04-17T10:50:52.626263Z","shell.execute_reply.started":"2023-04-17T10:50:52.619229Z"},"trusted":true},"outputs":[],"source":["def interrogate(image: Image) -> str:\n","    caption = ci.generate_caption(image)\n","    image_features = ci.image_to_features(image)\n","    \n","    medium = [ci.mediums.labels[i] for i in cos(image_features, mediums_features_array).topk(1).indices][0]\n","    movement = [ci.movements.labels[i] for i in cos(image_features, movements_features_array).topk(1).indices][0]\n","    flaves = \", \".join([ci.flavors.labels[i] for i in cos(image_features, flavors_features_array).topk(3).indices])\n","\n","    if caption.startswith(medium):\n","        prompt = f\"{caption}, {movement}, {flaves}\"\n","    else:\n","        prompt = f\"{caption}, {medium}, {movement}, {flaves}\"\n","\n","    return clip_interrogator._truncate_to_fit(prompt, ci.tokenize)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:52.629503Z","iopub.status.busy":"2023-04-17T10:50:52.628461Z","iopub.status.idle":"2023-04-17T10:50:57.242425Z","shell.execute_reply":"2023-04-17T10:50:57.239856Z","shell.execute_reply.started":"2023-04-17T10:50:52.629468Z"},"trusted":true},"outputs":[],"source":["prompts = []\n","\n","images_path = \"../input/stable-diffusion-image-to-prompts/images/\"\n","for image_name in images:\n","    img = Image.open(images_path + image_name).convert(\"RGB\")\n","\n","    generated = interrogate(img)\n","    \n","    prompts.append(generated)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:57.250245Z","iopub.status.busy":"2023-04-17T10:50:57.247608Z","iopub.status.idle":"2023-04-17T10:50:57.260954Z","shell.execute_reply":"2023-04-17T10:50:57.259960Z","shell.execute_reply.started":"2023-04-17T10:50:57.250197Z"},"trusted":true},"outputs":[],"source":["def add_text_limiters(text: str) -> str:\n","    return \" \".join([\n","        word + \"\\n\" if i % 15 == 0 else word \n","        for i, word in enumerate(text.split(\" \"), start=1)\n","    ])\n","\n","def plot_image(image: np.ndarray, original_prompt: str, generated_prompt: str) -> None:\n","    plt.figure(figsize=(10, 10))\n","    plt.imshow(image)\n","    plt.annotate(\n","        \"Original prompt:\\n\" + add_text_limiters(original_prompt) + \"\\n\\nGenerated prompt:\\n\" + add_text_limiters(generated_prompt), \n","        xy=(1.05, 0.5), xycoords='axes fraction', ha='left', va='center', \n","        fontsize=16, rotation=0, color=\"#104a6e\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:57.269060Z","iopub.status.busy":"2023-04-17T10:50:57.266038Z","iopub.status.idle":"2023-04-17T10:50:57.276943Z","shell.execute_reply":"2023-04-17T10:50:57.275916Z","shell.execute_reply.started":"2023-04-17T10:50:57.268930Z"},"trusted":true},"outputs":[],"source":["#original_prompts_df = pd.read_csv(\"/kaggle/input/stable-diffusion-image-to-prompts/prompts.csv\")\n","\n","#for image_name, prompt in zip(images, prompts):\n","#    img = Image.open(images_path + image_name).convert(\"RGB\")\n","#    original_prompt = original_prompts_df[\n","#        original_prompts_df.imgId == image_name.split(\".\")[0]\n","#    ].prompt.iloc[0]\n","#    plot_image(img, original_prompt, prompt)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Create a sample sub"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:57.284339Z","iopub.status.busy":"2023-04-17T10:50:57.281586Z","iopub.status.idle":"2023-04-17T10:50:57.400048Z","shell.execute_reply":"2023-04-17T10:50:57.398821Z","shell.execute_reply.started":"2023-04-17T10:50:57.284305Z"},"trusted":true},"outputs":[],"source":["embeddings2 = st_model.encode(prompts).flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:57.401869Z","iopub.status.busy":"2023-04-17T10:50:57.401333Z","iopub.status.idle":"2023-04-17T10:50:57.732458Z","shell.execute_reply":"2023-04-17T10:50:57.731597Z","shell.execute_reply.started":"2023-04-17T10:50:57.401832Z"},"trusted":true},"outputs":[],"source":["del ci\n","del blip_model, clip_model\n","del st_model\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:57.738450Z","iopub.status.busy":"2023-04-17T10:50:57.736372Z","iopub.status.idle":"2023-04-17T10:50:57.744685Z","shell.execute_reply":"2023-04-17T10:50:57.743807Z","shell.execute_reply.started":"2023-04-17T10:50:57.738409Z"},"trusted":true},"outputs":[],"source":["embeddings12 = ratio_OFA * embeddings1 + ratio_CLIP_Interrogator * embeddings2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:57.751823Z","iopub.status.busy":"2023-04-17T10:50:57.749225Z","iopub.status.idle":"2023-04-17T10:50:57.998682Z","shell.execute_reply":"2023-04-17T10:50:57.997649Z","shell.execute_reply.started":"2023-04-17T10:50:57.751785Z"},"trusted":true},"outputs":[],"source":["del embeddings1\n","del embeddings2\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:58.000597Z","iopub.status.busy":"2023-04-17T10:50:58.000147Z","iopub.status.idle":"2023-04-17T10:50:58.007784Z","shell.execute_reply":"2023-04-17T10:50:58.006686Z","shell.execute_reply.started":"2023-04-17T10:50:58.000560Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import timm\n","from sklearn.preprocessing import normalize"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:58.009825Z","iopub.status.busy":"2023-04-17T10:50:58.009412Z","iopub.status.idle":"2023-04-17T10:50:58.017042Z","shell.execute_reply":"2023-04-17T10:50:58.016104Z","shell.execute_reply.started":"2023-04-17T10:50:58.009790Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    model_path = '/kaggle/input/stable-diffusion-vit-baseline-train/vit_base_patch16_224.pth'\n","    model_name = 'vit_base_patch16_224'\n","    input_size = 224\n","    batch_size = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:58.020336Z","iopub.status.busy":"2023-04-17T10:50:58.020040Z","iopub.status.idle":"2023-04-17T10:50:58.027866Z","shell.execute_reply":"2023-04-17T10:50:58.026895Z","shell.execute_reply.started":"2023-04-17T10:50:58.020312Z"},"trusted":true},"outputs":[],"source":["class DiffusionTestDataset(Dataset):\n","    def __init__(self, images, transform):\n","        self.images = images\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.images[idx])\n","        image = self.transform(image)\n","        return image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:58.029674Z","iopub.status.busy":"2023-04-17T10:50:58.029322Z","iopub.status.idle":"2023-04-17T10:50:58.042125Z","shell.execute_reply":"2023-04-17T10:50:58.041227Z","shell.execute_reply.started":"2023-04-17T10:50:58.029640Z"},"trusted":true},"outputs":[],"source":["def predict(\n","    images,\n","    model_path,\n","    model_name,\n","    input_size,\n","    batch_size\n","):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    transform = transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","#         transforms.RandomRotation(degrees=10),\n","\n","        # transforms.RandomVerticalFlip(p=0.5),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ])\n","    dataset = DiffusionTestDataset(images, transform)\n","    dataloader = DataLoader(\n","        dataset=dataset,\n","        shuffle=False,\n","        batch_size=batch_size,\n","        pin_memory=True,\n","        num_workers=2,\n","        drop_last=False\n","    )\n","\n","    model = timm.create_model(\n","        model_name,\n","        pretrained=False,\n","        num_classes=384\n","    )\n","    state_dict = torch.load(model_path)\n","    model.load_state_dict(state_dict)\n","    model.to(device)\n","    model.eval()\n","    \n","    tta_preds = None\n","    for _ in range(2):\n","        preds = []\n","        for X in tqdm(dataloader, leave=False):\n","            X = X.to(device)\n","\n","            with torch.no_grad():\n","                X_out = model(X).cpu().numpy()\n","                # L2 normalize -- Start\n","                X_out = X_out / ( np.abs(X_out).max(axis=-1, keepdims=True) + 0.0000001)  # To avoid to overflow at normalize()\n","                X_out = normalize( X_out )\n","                # L2 normalize -- End\n","                preds.append(X_out)\n","                \n","        if tta_preds is None:\n","            tta_preds = np.vstack(preds).flatten()\n","        else:\n","            tta_preds += np.vstack(preds).flatten()\n","    \n","    return tta_preds / 2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:50:58.043905Z","iopub.status.busy":"2023-04-17T10:50:58.043520Z","iopub.status.idle":"2023-04-17T10:51:04.314462Z","shell.execute_reply":"2023-04-17T10:51:04.313233Z","shell.execute_reply.started":"2023-04-17T10:50:58.043871Z"},"trusted":true},"outputs":[],"source":["images = list(Path('/kaggle/input/stable-diffusion-image-to-prompts/images').glob('*.png'))\n","imgIds = [i.stem for i in images]\n","EMBEDDING_LENGTH = 384\n","imgId_eId = [\n","    '_'.join(map(str, i)) for i in zip(\n","        np.repeat(imgIds, EMBEDDING_LENGTH),\n","        np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n","\n","embeddings3 = predict(images, CFG.model_path, CFG.model_name, CFG.input_size, CFG.batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:51:04.317424Z","iopub.status.busy":"2023-04-17T10:51:04.317029Z","iopub.status.idle":"2023-04-17T10:51:04.322635Z","shell.execute_reply":"2023-04-17T10:51:04.321309Z","shell.execute_reply.started":"2023-04-17T10:51:04.317386Z"},"trusted":true},"outputs":[],"source":["embeddings = embeddings12 + ratio_ViT_B_16 * embeddings3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:51:04.325408Z","iopub.status.busy":"2023-04-17T10:51:04.324520Z","iopub.status.idle":"2023-04-17T10:51:04.339589Z","shell.execute_reply":"2023-04-17T10:51:04.338546Z","shell.execute_reply.started":"2023-04-17T10:51:04.325380Z"},"trusted":true},"outputs":[],"source":["submission = pd.DataFrame(\n","    index=imgId_eId,\n","    data=embeddings,\n","    columns=['val']\n",").rename_axis('imgId_eId')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T10:51:04.341538Z","iopub.status.busy":"2023-04-17T10:51:04.341021Z","iopub.status.idle":"2023-04-17T10:51:04.360314Z","shell.execute_reply":"2023-04-17T10:51:04.359419Z","shell.execute_reply.started":"2023-04-17T10:51:04.341435Z"},"trusted":true},"outputs":[],"source":["submission.to_csv('submission.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
