{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install jax jaxlib flax optax mtranslate sentencepiece datasets transformers accelerate scikit-learn ipywidgets datasets nltk importlib-metadata\n!pip install transformers --upgrade","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-23T09:58:42.433366Z","iopub.execute_input":"2023-04-23T09:58:42.434126Z","iopub.status.idle":"2023-04-23T09:58:54.487568Z","shell.execute_reply.started":"2023-04-23T09:58:42.434084Z","shell.execute_reply":"2023-04-23T09:58:54.486325Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/huggingface/transformers.git\n","metadata":{"execution":{"iopub.status.busy":"2023-04-20T00:14:44.532708Z","iopub.execute_input":"2023-04-20T00:14:44.534013Z","iopub.status.idle":"2023-04-20T00:15:00.813921Z","shell.execute_reply.started":"2023-04-20T00:14:44.533947Z","shell.execute_reply":"2023-04-20T00:15:00.812679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/transformers/examples/flax/question-answering/","metadata":{"execution":{"iopub.status.busy":"2023-04-20T00:16:31.479150Z","iopub.execute_input":"2023-04-20T00:16:31.479762Z","iopub.status.idle":"2023-04-20T00:16:31.488498Z","shell.execute_reply.started":"2023-04-20T00:16:31.479719Z","shell.execute_reply":"2023-04-20T00:16:31.487043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, HTML\n# from huggingface_hub import notebook_login\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForQuestionAnswering, AdamW, Trainer, TrainingArguments, default_data_collator, FlaxAutoModelForQuestionAnswering\nimport time\nimport nltk\nimport math\nimport torch\nfrom datasets import Dataset, DatasetDict, load_dataset, load_metric\nimport tensorflow as tf\nimport re\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport transformers\n# from accelerate import Accelerator\nimport datasets\nimport sentencepiece\nfrom time import sleep\nfrom time import time\nfrom random import randint\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T09:59:08.051950Z","iopub.execute_input":"2023-04-23T09:59:08.052711Z","iopub.status.idle":"2023-04-23T09:59:49.486059Z","shell.execute_reply.started":"2023-04-23T09:59:08.052669Z","shell.execute_reply":"2023-04-23T09:59:49.484957Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Вход в huggingface\n# notebook_login()\n!huggingface-cli login --token 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","metadata":{"execution":{"iopub.status.busy":"2023-04-23T10:00:36.842863Z","iopub.execute_input":"2023-04-23T10:00:36.844179Z","iopub.status.idle":"2023-04-23T10:00:37.948073Z","shell.execute_reply.started":"2023-04-23T10:00:36.844137Z","shell.execute_reply":"2023-04-23T10:00:37.946927Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"def translate_sentence(text):\n    input_ids = t_tokenizer(text, return_tensors=\"jax\").input_ids\n    sequences = t_model.generate(input_ids, num_beams=2).sequences\n    outputs = t_tokenizer.batch_decode(sequences, skip_special_tokens=True)\n    return outputs\n\nfrom jax import vmap, jit\n\n@jit\ndef translate_sentence_batch(words):\n    return vmap(translate_sentence)(words)\n\ndef translate_sentences(sentences):\n    words = sentence.split(' ')\n    outputs = translate_sentence_batch(words)\n    return [' '.join(output) for output in outputs]\n","metadata":{"execution":{"iopub.status.busy":"2023-04-21T00:53:28.991408Z","iopub.execute_input":"2023-04-21T00:53:28.991950Z","iopub.status.idle":"2023-04-21T00:53:29.002669Z","shell.execute_reply.started":"2023-04-21T00:53:28.991904Z","shell.execute_reply":"2023-04-21T00:53:29.001290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"УТВЕРЖДАЮ: Председатель закупочной комиссии, заместитель генерального директора - по логистике и МТО АО «АТХ» ____________________ Т.Ю. Шустова «01» сентября 2022 г. ДОКУМЕНТАЦИЯ О КОНКУРЕНТНОЙ ЗАКУПКЕ ЗАПРОС ПРЕДЛОЖЕНИЙ В ЭЛЕКТРОННОЙ ФОРМЕ, УЧАСТНИКАМИ КОТОРОГО МОГУТ БЫТЬ ТОЛЬКО СУБЪЕКТЫ МАЛОГО И СРЕДНЕГО ПРЕДПРИНИМАТЕЛЬСТВА на право заключения Договора на выполнение работ по ремонту зданий и сооружений г. Киров 2022 год. Стр.2 СОДЕРЖАНИЕ СОДЕРЖАНИЕ 2 I. ОБЩИЕ УСЛОВИЯ ПРОВЕДЕНИЯ закупки 3 1. ОБЩИЕ ПОЛОЖЕНИЯ 3 1.1. Правовой статус документов 3 1.2. Заказчик, предмет и условия проведения закупки. 3 1.3. Начальная (максимальная) цена договора 4 1.4. Требования к участникам закупки 4 1.5. Участие в закупке коллективных участников (группы лиц) 5 1.6. Привлечение соисполнителей (субподрядчиков) к исполнению договора 6 1.7. Расходы на участие в закупке и при заключении договора 7 1.8. Предоставление приоритетов товаров российского происхождения, работ, услуг, выполняемых, оказываемых российс 3.5.2, 5.6.4 Закупка по единичным расценкам Нет.\"\n\ntranslate_sentence(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-20T23:50:05.382087Z","iopub.execute_input":"2023-04-20T23:50:05.382757Z","iopub.status.idle":"2023-04-20T23:50:05.389117Z","shell.execute_reply.started":"2023-04-20T23:50:05.382724Z","shell.execute_reply":"2023-04-20T23:50:05.387884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n\nfrom transformers import FlaxMarianModel, MarianTokenizer, AutoModelForSeq2SeqLM, FlaxMarianMTModel\n\nt_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")\nt_model = FlaxMarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\", from_pt=True)\n\nt_model.params = t_model.to_fp16(t_model.params)\n\nimport re\n\ndef translate_sentence(sentence):\n    words = sentence.split(' ')\n    translated_words = []\n    batch_size = 1  # Number of words to translate in one batch\n    for i in range(0, len(words), batch_size):\n        batch = words[i:i+batch_size]\n        input_ids = t_tokenizer(batch, return_tensors=\"jax\", max_length=64, padding=True, truncation=True).input_ids\n        outputs = t_model.generate(input_ids).sequences\n        translated_batch = t_tokenizer.batch_decode(outputs, max_length=64, skip_special_tokens=True)\n        translated_words.extend(translated_batch)\n    translated_sentence = ' '.join(translated_words)\n    return translated_sentence\n    \ndef clean_text(text):\n    # Удаление указанных символов из текста\n    text = re.sub(r'[\\\"\\#\\$\\;\\:\\^\\&\\№\\*\\-\\=\\+\\-\\,\\.\\@\\!\\?\\/\\]\\[\\}\\{\\|\\~\\«\\»\\`]', '', text)\n    text = re.sub(r'_', ' ', text)  # Замена _ на пробел\n    text = re.sub(r'\\s+', ' ', text)  # Удаление лишних пробелов\n    return text.strip()\n\ndef create_qa_dataframe(data):\n    examples = []\n    for row in tqdm(data, total = len(data)):\n        cleaned_text = clean_text(row['text'])\n        question = row['label']\n        extracted_part = row.get('extracted_part', {})\n        if extracted_part and extracted_part['text'] is not None:\n            answer = clean_text(extracted_part['text'][0].strip())\n            answer_start = extracted_part['answer_start'][0]\n            answer_end = extracted_part['answer_end'][0]\n            if answer:\n                answer_words = answer.split()\n                match_found = False\n                for i in range(len(answer_words)):\n                    answer_start_new = cleaned_text.find(answer_words[i])\n                    if answer_start_new != -1:\n                        match_found = True\n                        for j in range(i+1, len(answer_words)):\n                            next_word_start = answer_start_new + len(answer_words[i-1])\n                            next_word_end = next_word_start + len(answer_words[j])\n                            next_word = cleaned_text[next_word_start:next_word_end]\n                            if answer_words[j] != next_word:\n                                match_found = False\n                                break\n                        if match_found:\n                            answer_start = answer_start_new\n                            answer_end = answer_start_new + len(answer_words) - 1\n                            break\n                if not match_found:\n                    answer_start = answer_end = 0\n            else:\n                answer_start = answer_end = 0\n        else:\n            answer_start = answer_end = 0\n            answer = None\n            \n#         translated_text = translate_sentence(cleaned_text)\n#         translated_question = translate_sentence(question)\n#         translated_answer = translate_sentence(answer) if answer else None\n\n        example = {'context': cleaned_text, 'question': question, 'answer': answer, 'answer_start': answer_start, 'answer_end': answer_end}\n        examples.append(example)\n    df = pd.DataFrame(examples)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-21T00:30:03.434227Z","iopub.execute_input":"2023-04-21T00:30:03.434638Z","iopub.status.idle":"2023-04-21T00:30:06.357018Z","shell.execute_reply.started":"2023-04-21T00:30:03.434608Z","shell.execute_reply":"2023-04-21T00:30:06.355554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = create_qa_dataframe(train_data)\ndisplay(HTML(train_df[6:12].to_html()))","metadata":{"execution":{"iopub.status.busy":"2023-04-21T00:30:12.222891Z","iopub.execute_input":"2023-04-21T00:30:12.224073Z","iopub.status.idle":"2023-04-21T00:30:12.659435Z","shell.execute_reply.started":"2023-04-21T00:30:12.224030Z","shell.execute_reply":"2023-04-21T00:30:12.658084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# определите столбцы, которые нужно перевести\ncols_to_translate = ['context', 'question', 'answer']\n\n# создайте функцию, которая будет переводить строки\ndef translate_df_row(row):\n    for col in cols_to_translate:\n        row[col] = translate_sentences([row[col]])[0]\n    return row\n\n# примените функцию к каждой строке датафрейма\ntranslated_df = train_df[cols_to_translate].apply(translate_df_row, axis=1)\n\n# объедините оригинальный датафрейм с переведенными столбцами\ntrain_df[cols_to_translate] = translated_df[cols_to_translate]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create dataset","metadata":{}},{"cell_type":"code","source":"import mtranslate\n\nclass QADataset:\n    def __init__(self, train_data, val_data, test_data=None):\n        self.train_examples = self.create_qa_example(train_data)\n        self.val_examples = self.create_qa_example(val_data)\n        self.test_examples = self.create_qa_example(test_data) if test_data else []\n        self.train_dataset = datasets.Dataset.from_pandas(pd.DataFrame(self.train_examples))\n        self.val_dataset = datasets.Dataset.from_pandas(pd.DataFrame(self.val_examples))\n        self.test_dataset = datasets.Dataset.from_pandas(pd.DataFrame(self.test_examples)) if test_data else None\n        if test_data:\n            self.dataset_dict = DatasetDict({\n                'train': self.train_dataset,\n                'validation': self.val_dataset,\n                'test': self.test_dataset\n            })\n        else:\n            self.dataset_dict = DatasetDict({\n                'train': self.train_dataset,\n                'validation': self.val_dataset\n            })\n\n    def clean_text(self, text):\n        # Удаление указанных символов из текста\n        text = re.sub(r'[\\\"\\#\\$\\;\\:\\^\\&\\№\\*\\-\\=\\+\\-\\,\\.\\@\\!\\?\\/\\]\\[\\}\\{\\|\\~\\«\\»\\`]', '', text)\n        text = re.sub(r'_', ' ', text)  # Замена _ на пробел\n        text = re.sub(r'\\s+', ' ', text)  # Удаление лишних пробелов\n        return text.strip()\n\n    def create_qa_example(self, data):\n        examples = []\n        for row in tqdm(data, total = len(data)):\n            cleaned_text = self.clean_text(row['text'])\n            question = row['label']\n            extracted_part = row.get('extracted_part', {})\n            if extracted_part and extracted_part['text'] is not None:\n                answer = self.clean_text(extracted_part['text'][0].strip())\n                answer_start = extracted_part['answer_start'][0]\n                answer_end = extracted_part['answer_end'][0]\n                if answer:\n                    answer_words = answer.split()\n                    match_found = False\n                    for i in range(len(answer_words)):\n                        answer_start_new = cleaned_text.find(answer_words[i])\n                        if answer_start_new != -1:\n                            match_found = True\n                            for j in range(i+1, len(answer_words)):\n                                next_word_start = answer_start_new + len(answer_words[i-1])\n                                next_word_end = next_word_start + len(answer_words[j])\n                                next_word = cleaned_text[next_word_start:next_word_end]\n                                if answer_words[j] != next_word:\n                                    match_found = False\n                                    break\n                            if match_found:\n                                answer_start = answer_start_new\n                                answer_end = answer_start_new + len(answer_words) - 1\n                                break\n                    if not match_found:\n                        answer_start = answer_end = 0\n                else:\n                    answer_start = answer_end = 0\n            else:\n                answer_start = answer_end = 0\n                answer = None\n\n#             translated_text = translate_sentence(cleaned_text)\n    #         translated_question = translate_sentence(question)\n    #         translated_answer = translate_sentence(answer) if answer else None\n\n            # Append example only if answer is not None\n            if answer_start != 0 :\n                example = {'context': cleaned_text, 'question': question, 'answer': answer, 'answer_start': answer_start, 'answer_end': answer_end}\n                examples.append(example)\n            \n        return examples\n    \ndef prepare_train_features(examples):\n#     examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=150,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        return_tensors=\"jax\",\n    )\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = tokenized_examples[\"input_ids\"][i]\n#         cls_index = input_ids.index(tokenizer.cls_token_id)\n        cls_index = jnp.where(input_ids == tokenizer.cls_token_id)[0][0]\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answer_start = examples[\"answer_start\"][sample_index]\n        answer_end = examples[\"answer_end\"][sample_index]\n        if answer_start == 0 or answer_start == None:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            tokenized_examples[\"start_positions\"].append(answer_start)\n            tokenized_examples[\"end_positions\"].append(answer_end)\n#             token_start_index = 0\n#             while sequence_ids[token_start_index] != (0):\n#                 token_start_index += 1\n#             token_end_index = len(input_ids) - 1\n#             while sequence_ids[token_end_index] != (0):\n#                 token_end_index -= 1\n                \n#             if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n#                 tokenized_examples[\"start_positions\"].append(cls_index)\n#                 tokenized_examples[\"end_positions\"].append(cls_index)\n#             else:\n#                 while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n#                     token_start_index += 1\n#                 tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n#                 while offsets[token_end_index][1] >= end_char:\n#                     token_end_index -= 1\n#                 tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples\n\nclass QATrainer:\n    def __init__(self, model_name, train_dataset, val_dataset, batch_size=1, epochs=3):\n        self.model_name = model_name\n#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name, config=self.config)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n        self.batch_size = batch_size\n        self.epochs = epochs\n    \n    def training(self):\n        model_name = self.model_name.split(\"/\")[-1]\n        args = TrainingArguments(\n            model_name,\n            evaluation_strategy = \"epoch\",\n            learning_rate=2e-5,\n            per_device_train_batch_size=self.batch_size,\n            per_device_eval_batch_size=self.batch_size,\n            num_train_epochs=self.epochs,\n            report_to = 'none', \n            weight_decay=0.01,\n            push_to_hub=False,\n        )\n\n        trainer = Trainer(\n            model=self.model,\n            args=args,\n            train_dataset=self.train_dataset,\n            eval_dataset=self.val_dataset,\n            data_collator=default_data_collator,\n            tokenizer=self.tokenizer,\n        )\n        \n        trainer.train()\n        trainer.save_model(\"QA-trained\")\n        \n        return self.model","metadata":{"execution":{"iopub.status.busy":"2023-04-23T11:15:46.763783Z","iopub.execute_input":"2023-04-23T11:15:46.764221Z","iopub.status.idle":"2023-04-23T11:15:46.799386Z","shell.execute_reply.started":"2023-04-23T11:15:46.764189Z","shell.execute_reply":"2023-04-23T11:15:46.798407Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"# Used git finetune script","metadata":{}},{"cell_type":"code","source":"# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n\n# Args\n# num_labels = 1\n# seed = 60\n# num_train_epochs = 10\n# learning_rate = 2e-5\nper_device_batch_size = 3\ntotal_batch_size = per_device_batch_size * jax.local_device_count()\nmodel_name = \"google/bigbird-roberta-base\"\nmax_length = 1600\n\n# model elements\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = AutoConfig.from_pretrained(model_name)\nmodel = FlaxAutoModelForQuestionAnswering.from_pretrained(model_name, config=config)\npad_on_right = tokenizer.padding_side == \"right\"\n\n# dataset\nqa_dataset = QADataset(train_data, val_data, test_data=test_data)\ntokenized_dataset = qa_dataset.dataset_dict.map(prepare_train_features, batched=True, \n                                                 remove_columns=qa_dataset.dataset_dict[\"train\"].column_names)\ntrain_dataset = tokenized_dataset[\"train\"]\neval_dataset = tokenized_dataset[\"validation\"]\ntest_dataset = tokenized_dataset[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T01:03:26.286757Z","iopub.execute_input":"2023-04-23T01:03:26.287678Z","iopub.status.idle":"2023-04-23T01:03:33.798040Z","shell.execute_reply.started":"2023-04-23T01:03:26.287639Z","shell.execute_reply":"2023-04-23T01:03:33.796892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TPU used try 1","metadata":{}},{"cell_type":"code","source":"import jax\nimport flax\nimport optax\nfrom itertools import chain\n# from tqdm.notebook import tqdm\nfrom typing import Callable\n\nimport jax.numpy as jnp\n\nfrom flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\nfrom flax.training import train_state\nfrom flax import traverse_util\n# from torch.utils.data import DataLoader\n# import datasets","metadata":{"execution":{"iopub.status.busy":"2023-04-23T10:45:18.029243Z","iopub.execute_input":"2023-04-23T10:45:18.030277Z","iopub.status.idle":"2023-04-23T10:45:18.035684Z","shell.execute_reply.started":"2023-04-23T10:45:18.030235Z","shell.execute_reply":"2023-04-23T10:45:18.034668Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"print(flax.__version__)\njax.local_devices()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T10:45:19.862862Z","iopub.execute_input":"2023-04-23T10:45:19.863867Z","iopub.status.idle":"2023-04-23T10:45:19.870966Z","shell.execute_reply.started":"2023-04-23T10:45:19.863827Z","shell.execute_reply":"2023-04-23T10:45:19.869954Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"0.6.8\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"},"metadata":{}}]},{"cell_type":"code","source":"# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T11:09:05.257346Z","iopub.execute_input":"2023-04-23T11:09:05.258371Z","iopub.status.idle":"2023-04-23T11:09:05.317581Z","shell.execute_reply.started":"2023-04-23T11:09:05.258331Z","shell.execute_reply":"2023-04-23T11:09:05.316302Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"qa_dataset = QADataset(train_data, val_data, test_data=test_data)\ntokenized_dataset = qa_dataset.dataset_dict.map(prepare_train_features, batched=True, \n                                                 remove_columns=qa_dataset.dataset_dict[\"train\"].column_names)\ntrain_dataset = tokenized_dataset[\"train\"]\neval_dataset = tokenized_dataset[\"validation\"]\ntest_dataset = tokenized_dataset[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T02:04:00.351641Z","iopub.execute_input":"2023-04-23T02:04:00.352462Z","iopub.status.idle":"2023-04-23T02:06:10.144836Z","shell.execute_reply.started":"2023-04-23T02:04:00.352423Z","shell.execute_reply":"2023-04-23T02:06:10.143589Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"100%|██████████| 1439/1439 [00:00<00:00, 3162.33it/s]\n100%|██████████| 360/360 [00:00<00:00, 3400.04it/s]\n100%|██████████| 318/318 [00:00<00:00, 4229.43it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1439 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e95b0e392d8641629b8e3791b347d9ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/360 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc5ea3d3fa454615ba3481c61ac2643f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/318 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b306906a14a14ad8ba188751b19f187d"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-04-23T02:06:45.640620Z","iopub.execute_input":"2023-04-23T02:06:45.641724Z","iopub.status.idle":"2023-04-23T02:06:45.647337Z","shell.execute_reply.started":"2023-04-23T02:06:45.641689Z","shell.execute_reply":"2023-04-23T02:06:45.646191Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n    num_rows: 1439\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset['start_positions'][6]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T02:06:49.139443Z","iopub.execute_input":"2023-04-23T02:06:49.140646Z","iopub.status.idle":"2023-04-23T02:06:49.148792Z","shell.execute_reply.started":"2023-04-23T02:06:49.140607Z","shell.execute_reply":"2023-04-23T02:06:49.147591Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"1008"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(train_dataset['input_ids'][6])","metadata":{"execution":{"iopub.status.busy":"2023-04-23T02:06:52.027971Z","iopub.execute_input":"2023-04-23T02:06:52.029194Z","iopub.status.idle":"2023-04-23T02:06:53.039652Z","shell.execute_reply.started":"2023-04-23T02:06:52.029158Z","shell.execute_reply":"2023-04-23T02:06:53.038392Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"\"<s> обеспечение гарантийных обязательств</s></s> 4 II ИНФОРМАЦИОННАЯ КАРТА пп Наименование Содержание 1 Требования к участникам закупок в соответствии с ч 11 ст 31 Федерального закона 44ФЗ Установлено 11 Требования предъявляемые к участникам закупки в соответствии с частями 2 и 21 статьи 31 Федерального закона 44ФЗ Не установлены 12 Размер обеспечения заявки на участие 000 руб 121 Реквизиты счета для перечисления денежных средств в случае предусмотренном ч 13 ст 44 Федерального закона 44ФЗ УФК по Хабаровскому краю (Краевое государственное казенное учреждение ''Оператор систем электронного правительства Хабаровского края многофункциональный центр предоставления государственных и муниципальных услуг'' ЛС 05222206690) ИНН 2721187743 КПП 272201001 Банк получателя платежа ОТДЕЛЕНИЕ ХАБАРОВСК БАНКА РОССИИУФК по Хабаровскому краю г Хабаровск БИК 010813050 расчетный счет 03222643080000002200 122 Порядок внесения денежных средств в качестве обеспечения заявок условия независимой гарантии Не предусмотрен 13 Размер обеспечения исполнения контракта 500 % 131 Размер обеспечения гарантийных обязательств 000 % 132 Реквизиты счета на котором в соответствии с законодательством Российской Федерации учитываются операции со средствами поступающими заказчику УФК по Хабаровскому краю (Краевое государственное казенное учреждение ''Оператор систем электронного правительства Хабаровского края многофункциональный центр предоставления государственных и муниципальных услуг'' ЛС 05222206690) ИНН 2721187743 КПП 272201001 Банк получателя платежа ОТДЕЛЕНИЕ ХАБАРОВСК БАНКА РОССИИУФК по Хабаровскому краю г Хабаровск БИК 010813050 расчетный счет 03222643080000002200 133 Порядок предоставления обеспечения исполнения контракта гарантийных обязательств требования к такому обеспечению Порядок предоставления обеспечения исполнения контракта (договора) требования к обеспечению в соответствии с частью 3 частью 4 статьи 96 Федерального закона от 05042013 44ФЗ ''О контрактной системе в сфере закупок товаров работ услуг для обеспечения государственных и муниципальных нужд'' и разделами 7 и 8 ча Наименование место нахождения почтовый адрес адрес электронной почты номер контактного телефона ответственное должностное лицо заказчика Краевое государственное казенное учреждение ''Оператор систем электронного правительства Хабаровского края многофункциональный центр предоставления государственных и</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\""},"metadata":{}}]},{"cell_type":"code","source":"qa_dataset.dataset_dict","metadata":{"execution":{"iopub.status.busy":"2023-04-23T11:07:12.085848Z","iopub.execute_input":"2023-04-23T11:07:12.086650Z","iopub.status.idle":"2023-04-23T11:07:12.092742Z","shell.execute_reply.started":"2023-04-23T11:07:12.086613Z","shell.execute_reply":"2023-04-23T11:07:12.091708Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['context', 'question', 'answer', 'answer_start', 'answer_end'],\n        num_rows: 1439\n    })\n    validation: Dataset({\n        features: ['context', 'question', 'answer', 'answer_start', 'answer_end'],\n        num_rows: 360\n    })\n    test: None\n})"},"metadata":{}}]},{"cell_type":"code","source":"num_train_epochs = 50","metadata":{"execution":{"iopub.status.busy":"2023-04-23T14:11:01.069590Z","iopub.execute_input":"2023-04-23T14:11:01.070582Z","iopub.status.idle":"2023-04-23T14:11:01.075797Z","shell.execute_reply.started":"2023-04-23T14:11:01.070539Z","shell.execute_reply":"2023-04-23T14:11:01.074455Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n# Args\nnum_labels = 1\nseed = 60\nnum_train_epochs = 50\nlearning_rate = 2e-5\nper_device_batch_size = 1\ntotal_batch_size = per_device_batch_size * jax.local_device_count()\nmodel_name = 'xlm-roberta-base'\n# \"distilbert-base-uncased\"\nmax_length = 1440\n\n# model elements\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = AutoConfig.from_pretrained(model_name)\nmodel = FlaxAutoModelForQuestionAnswering.from_pretrained(model_name, config=config)\npad_on_right = tokenizer.padding_side == \"right\"\n\n# dataset\nqa_dataset = QADataset(train_data, val_data)\n# test_data=test_data\ntokenized_dataset = qa_dataset.dataset_dict.map(prepare_train_features, batched=True, \n                                                 remove_columns=qa_dataset.dataset_dict[\"train\"].column_names)\ntrain_dataset = tokenized_dataset[\"train\"]\neval_dataset = tokenized_dataset[\"validation\"]\n# test_dataset = tokenized_dataset[\"test\"]\n\n#add args\nnum_train_steps = len(train_dataset) // total_batch_size * num_train_epochs\nlearning_rate_function = optax.cosine_onecycle_schedule(transition_steps=num_train_steps, peak_value=learning_rate, pct_start=0.1, )\n\nprint(\"The overall batch size (both for training and eval) is\", total_batch_size)\nprint(\"The number of train steps (all the epochs) is\", num_train_steps)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T14:11:19.506429Z","iopub.execute_input":"2023-04-23T14:11:19.507435Z","iopub.status.idle":"2023-04-23T14:12:56.637663Z","shell.execute_reply.started":"2023-04-23T14:11:19.507392Z","shell.execute_reply":"2023-04-23T14:12:56.636243Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing FlaxXLMRobertaForQuestionAnswering: {('lm_head', 'layer_norm', 'scale'), ('lm_head', 'layer_norm', 'bias'), ('lm_head', 'dense', 'bias'), ('lm_head', 'bias'), ('lm_head', 'dense', 'kernel')}\n- This IS expected if you are initializing FlaxXLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxXLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxXLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: {('qa_outputs', 'kernel'), ('qa_outputs', 'bias')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 1439/1439 [00:00<00:00, 3374.85it/s]\n100%|██████████| 360/360 [00:00<00:00, 3544.25it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1189 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25972fce4f644658bd88f0b54ac68976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/303 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"140dbebd110948149ff00704a40abb5f"}},"metadata":{}},{"name":"stdout","text":"The overall batch size (both for training and eval) is 8\nThe number of train steps (all the epochs) is 7400\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_dataset = qa_dataset.dataset_dict.map(prepare_train_features, batched=True, \n                                                 remove_columns=qa_dataset.dataset_dict[\"train\"].column_names)\ntrain_dataset = tokenized_dataset[\"train\"]\neval_dataset = tokenized_dataset[\"validation\"]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T12:18:28.228360Z","iopub.execute_input":"2023-04-23T12:18:28.229223Z","iopub.status.idle":"2023-04-23T12:19:57.524603Z","shell.execute_reply.started":"2023-04-23T12:18:28.229192Z","shell.execute_reply":"2023-04-23T12:19:57.523456Z"},"trusted":true},"execution_count":93,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1189 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"203bd80dbece40a992d902b078ab729b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/303 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c4a27d6858498cbde418ddda4ba2df"}},"metadata":{}}]},{"cell_type":"code","source":"num_train_steps = len(train_dataset) // total_batch_size * num_train_epochs\nlearning_rate_function = optax.cosine_onecycle_schedule(transition_steps=num_train_steps, peak_value=learning_rate, pct_start=0.1, )","metadata":{"execution":{"iopub.status.busy":"2023-04-23T12:22:10.668094Z","iopub.execute_input":"2023-04-23T12:22:10.668520Z","iopub.status.idle":"2023-04-23T12:22:10.682382Z","shell.execute_reply.started":"2023-04-23T12:22:10.668490Z","shell.execute_reply":"2023-04-23T12:22:10.681056Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-04-23T11:17:59.202165Z","iopub.execute_input":"2023-04-23T11:17:59.203160Z","iopub.status.idle":"2023-04-23T11:17:59.210143Z","shell.execute_reply.started":"2023-04-23T11:17:59.203121Z","shell.execute_reply":"2023-04-23T11:17:59.208940Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n        num_rows: 1189\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n        num_rows: 303\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_dataset['train']['start_positions'][0]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T11:18:04.112018Z","iopub.execute_input":"2023-04-23T11:18:04.112423Z","iopub.status.idle":"2023-04-23T11:18:04.120455Z","shell.execute_reply.started":"2023-04-23T11:18:04.112392Z","shell.execute_reply":"2023-04-23T11:18:04.119361Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"1049"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenized_dataset['train']['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2023-04-23T13:47:41.870816Z","iopub.execute_input":"2023-04-23T13:47:41.871707Z","iopub.status.idle":"2023-04-23T13:47:42.707613Z","shell.execute_reply.started":"2023-04-23T13:47:41.871666Z","shell.execute_reply":"2023-04-23T13:47:42.706424Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"[0, 112203, 133744, 58514, 59, 2, 2, 208831, 106, 718, 22263, 1269, 46, 75420, 105, 183, 72392, 476, 424, 9888, 380, 162655, 35, 234665, 244, 225191, 244, 40188, 559, 49, 20960, 135, 128815, 130, 117551, 16753, 16286, 183, 496, 1662, 5357, 476, 427, 18949, 5188, 123954, 244, 35, 40238, 85212, 591, 89, 72499, 124234, 424, 142972, 43475, 225191, 244, 40188, 559, 65153, 1196, 49, 166153, 88952, 380, 162655, 35, 234665, 244, 225191, 244, 40188, 559, 518, 121918, 149514, 40378, 2297, 518, 66917, 96515, 35, 147055, 85374, 33473, 543, 963, 447, 12758, 27183, 119807, 59, 29, 255, 64743, 56600, 26124, 312, 167137, 591, 89, 72499, 124234, 424, 142972, 43475, 35, 15, 3107, 16, 25440, 129, 591, 415, 1882, 13345, 56600, 49, 20960, 135, 128815, 130, 117551, 16753, 16286, 183, 534, 8318, 7360, 476, 361, 12183, 5188, 29514, 1993, 119807, 59, 29, 255, 64743, 56600, 26124, 312, 167137, 591, 89, 72499, 124234, 424, 142972, 43475, 35, 15, 3107, 16, 25440, 129, 591, 415, 1882, 13345, 56600, 49, 104939, 149514, 40378, 2297, 518, 66917, 236921, 85374, 85374, 178301, 16753, 16286, 35, 147055, 85374, 33473, 953, 140782, 66917, 173604, 29, 19149, 49, 6, 155962, 2172, 151075, 103, 6171, 387, 140782, 66917, 133744, 58514, 59, 21280, 183, 79225, 58514, 59, 952, 1089, 225697, 2715, 133744, 58514, 59, 46983, 49, 199826, 158108, 18102, 29, 36858, 2192, 117318, 419, 25862, 5560, 698, 128898, 1584, 154693, 518, 145562, 66917, 133744, 58514, 59, 672, 153009, 2715, 72767, 17845, 4301, 1078, 117199, 59156, 142806, 59, 130914, 12358, 5509, 213857, 135, 226551, 59156, 142806, 59, 78835, 21116, 135, 3167, 1551, 12657, 14758, 84965, 34710, 31420, 24998, 12635, 16229, 58263, 96480, 10190, 328, 5573, 56110, 8871, 49517, 22043, 2678, 110144, 363, 108428, 96480, 10190, 328, 5573, 31420, 24998, 12635, 16229, 135, 3167, 1551, 12657, 14758, 19771, 36627, 18454, 9694, 387, 804, 164532, 757, 2588, 10700, 10057, 4633, 1382, 66035, 4648, 12338, 6746, 34479, 5729, 650, 1112, 49150, 13556, 135401, 179973, 181155, 18197, 78745, 212, 52181, 12635, 25862, 5098, 3742, 160775, 84701, 161025, 28568, 3559, 672, 153009, 2715, 27617, 3858, 76159, 3580, 2632, 108428, 96480, 10190, 328, 5573, 1537, 52619, 3413, 4836, 189982, 6746, 6, 67090, 1532, 1197, 1549, 120377, 104101, 2052, 15279, 17845, 4301, 1078, 117199, 59156, 142806, 59, 130914, 12358, 5509, 213857, 135, 226551, 59156, 142806, 59, 78835, 21116, 135, 3167, 1551, 12657, 14758, 84965, 34710, 31420, 24998, 12635, 16229, 58263, 96480, 10190, 328, 5573, 102021, 49, 199826, 158108, 18102, 49, 22326, 66917, 133744, 58514, 59, 220893, 989, 48895, 2151, 63088, 91188, 135, 183, 7588, 7613, 27617, 1086, 407, 68628, 52125, 20765, 1874, 11875, 11125, 652, 49817, 8941, 50316, 4485, 928, 135, 901, 147186, 4256, 135, 117318, 419, 129, 84609, 6934, 559, 3150, 3505, 165200, 59, 49, 155980, 51384, 103, 49, 5138, 25516, 6103, 121690, 84708, 130, 2101, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.decode(tokenized_dataset['train']['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2023-04-23T11:18:20.258129Z","iopub.execute_input":"2023-04-23T11:18:20.258499Z","iopub.status.idle":"2023-04-23T11:18:21.105816Z","shell.execute_reply.started":"2023-04-23T11:18:20.258472Z","shell.execute_reply":"2023-04-23T11:18:21.104717Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"'<s> обеспечение исполнения контракта</s></s> Приложение 1 к заявке – заказу от 2022г изненно необходимых и важнейших лекарственных препаратов в соответствии с постановлением Правительства Российской Федерации от 30112015г 1289 Об ограничениях и условиях допуска происходящих из иностранных государств лекарственных препаратов включенных в перечень жизненно необходимых и важнейших лекарственных препаратов для целей осуществления закупок для обеспечения государственных и муниципальных нужд Нет 1810 Установление запрета на допуск товаров легкой промышленности происходящих из иностранных государств и (или) услуг по прокату таких товаров в соответствии с постановлением Правительства Российской Федерации от 11082014г 791 Об установлении запрета на допуск товаров легкой промышленности происходящих из иностранных государств и (или) услуг по прокату таких товаров в целях осуществления закупок для обеспечения федеральных нужд нужд субъектов Российской Федерации и муниципальных нужд Нет 19 Размер обеспечения заявки на участие в электронном аукционе нет 20 Размер обеспечения исполнения контракта 5% от цены контракта 21 Обеспечение исполнения контракта путем внесения денежных средств на указанный заказчиком счет Реквизиты счета для предоставления обеспечения исполнения контракта Наименование учреждения Муниципальное общеобразовательное бюджетное учреждение средняя общеобразовательная школа сТемясово муниципального района Баймакский район Республики Башкортостан Юридический адрес 453663 Республика Башкортостан Баймакский район сТемясово улСоветская 20 ИНН 0254005845 КПП 025401001 Рс 40102810045370000067 Казначейский счет 03234643806060000100 Наименование банка Отделение НБ Республика Башкортостан БИК 018073401 лс 20103020420 Получатель Муниципальное общеобразовательное бюджетное учреждение средняя общеобразовательная школа сТемясово муниципального района Баймакский район Республики Башкортостан Факт внесения денежных средств в качестве обеспечения исполнения контракта подтверждается платежным поручением с отметкой банка об оплате Денежные средства возвращаются поставщику с По согласованию с заказчиком поставка дров может быть осуществлена в полном объеме в более ранние сроки Кубм 500</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"},"metadata":{}}]},{"cell_type":"code","source":"model = FlaxAutoModelForQuestionAnswering.from_pretrained(model_name, config=config)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T14:54:48.780782Z","iopub.execute_input":"2023-04-23T14:54:48.781726Z","iopub.status.idle":"2023-04-23T14:54:51.357517Z","shell.execute_reply.started":"2023-04-23T14:54:48.781682Z","shell.execute_reply":"2023-04-23T14:54:51.356362Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing FlaxXLMRobertaForQuestionAnswering: {('lm_head', 'layer_norm', 'scale'), ('lm_head', 'layer_norm', 'bias'), ('lm_head', 'dense', 'bias'), ('lm_head', 'bias'), ('lm_head', 'dense', 'kernel')}\n- This IS expected if you are initializing FlaxXLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxXLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxXLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: {('qa_outputs', 'kernel'), ('qa_outputs', 'bias')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\nclass RMSE(datasets.Metric):\n    def _info(self):\n        return datasets.MetricInfo(\n            description=\"Calculates Root Mean Squared Error (RMSE) metric.\",\n            citation=\"TODO: _CITATION\",\n            inputs_description=\"_KWARGS_DESCRIPTION\",\n            features=datasets.Features({\n                'predictions': datasets.Value('float32'),\n                'references': datasets.Value('float32'),\n            }),\n            codebase_urls=[],\n            reference_urls=[],\n            format='numpy'\n        )\n\n    def _compute(self, predictions, references):\n        rmse = np.sqrt(np.sum(np.square(predictions - references)) / predictions.shape[0])\n        return {\"RMSE\": rmse}\n\nclass TrainState(train_state.TrainState):\n    logits_function: Callable = flax.struct.field(pytree_node=False)\n    loss_function: Callable = flax.struct.field(pytree_node=False)\n        \ndef decay_mask_fn(params):\n    flat_params = traverse_util.flatten_dict(params)\n    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n    return traverse_util.unflatten_dict(flat_mask)\n\ndef adamw(weight_decay):\n    return optax.adamw(learning_rate=learning_rate_function, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay, mask=decay_mask_fn)\n\nadamw = adamw(1e-2)\n\nmomentum = 0.9\nbeta2 = 0.999\neps = 1e-6\n\n# tx = optax.chain(\n#     optax.clip_by_global_norm(1.0),\n#     optax.scale_by_adam(b1=0.0, b2=beta2),\n#     optax.additive_weight_decay(1e-2),\n#     optax.scale(-learning_rate),\n#     optax.trace(decay=0.9, nesterov=True),\n# )\n\n# tx = optax.chain(\n#     optax.trace(decay=0.9, nesterov=True),\n#     optax.scale_by_rms(),\n#     optax.scale(-learning_rate),\n# )\n\n# @jax.jit\n# def loss_function(logits, labels):\n#     return jnp.mean((logits[..., 0] - labels) ** 2)\n\n@jax.jit\ndef loss_function(logits, positions):\n#     logits = logits[..., 0] # получение массива start_logits\n    predicted_positions = jnp.argmax(logits, axis=-1) # получение индекса с наибольшим значением в каждом батче\n    loss = jnp.mean((predicted_positions - positions) ** 2) # вычисление средней ошибки\n    return loss\n\n\n# @jax.jit    \n# def eval_function(logits):\n#     return logits[..., 0]\n\n@jax.jit    \ndef eval_function(logits):\n#     logits = logits[..., 0]\n    l_index = jnp.argmax(logits, axis=-1)\n#     start_values = jnp.take_along_axis(logits, axis=-1)\n    return l_index\n\nstate = TrainState.create(\n    apply_fn=model.__call__,\n    params=model.params,\n    tx=adamw,\n    logits_function=eval_function,\n    loss_function=loss_function,\n)\n\ndef train_step(state, batch, dropout_rng):\n    start_positions = batch.pop(\"start_positions\")\n    end_positions = batch.pop(\"end_positions\")\n    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n#     print(batch)\n\n    def loss_fn(params):\n        outputs = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True, return_dict=True)\n#         start_logits, end_logits = jnp.squeeze(outputs, axis=-1).tolist()\n#         start_logits = np.array(start_logits)\n#         end_logits = np.array(end_logits)\n#         start_logits = jnp.asarray(start_logits)\n#         start_logits = jax.device_get(start_logits)\n        end_logits = outputs.end_logits\n        start_logits = outputs.start_logits\n#         print(outputs.start_logits)\n#         print(end_logits)\n        start_loss = state.loss_function(start_logits, start_positions)\n#         print(start_loss)\n        end_loss = state.loss_function(end_logits, end_positions)\n#         print(end_loss)\n        loss = (start_loss + end_loss) / 2.0\n#         print(loss)\n        return loss\n\n    grad_function = jax.value_and_grad(loss_fn)\n    loss, grad = grad_function(state.params)\n    grad = jax.lax.pmean(grad, \"batch\")\n    new_state = state.apply_gradients(grads=grad)\n    new_params = new_state.params\n    new_params = jax.tree_map(lambda p, g: p - learning_rate_function(state.step) * g, new_state.params, grad)\n    new_state = new_state.replace(params=new_params)\n    metrics = {\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}\n    return new_state, metrics, new_dropout_rng\n\nparallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n\ndef eval_step(state, batch):\n    outputs = state.apply_fn(**batch, params=state.params, train=False)\n    start_logits = outputs.start_logits\n    end_logits = outputs.end_logits\n#     start_logits = jax.lax.pmean(state.logits_function(start_logits), \"batch\")\n#     end_logits = jax.lax.pmean(state.logits_function(end_logits), \"batch\")\n    return state.logits_function(start_logits), state.logits_function(end_logits)\n\nparallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n\ndef train_data_loader(rng, dataset, batch_size):\n    steps_per_epoch = len(dataset) // batch_size\n    perms = jax.random.permutation(rng, len(dataset))\n    perms = perms[: steps_per_epoch * batch_size]\n    perms = perms.reshape((steps_per_epoch, batch_size))\n\n    for perm in perms:\n        batch = dataset[perm]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch\n        \ndef eval_data_loader(dataset, batch_size):\n    for i in range(len(dataset) // batch_size):\n        batch = dataset[i * batch_size : (i + 1) * batch_size]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch\n        \nstate = flax.jax_utils.replicate(state)\n\nrng = jax.random.PRNGKey(seed)\ndropout_rngs = jax.random.split(rng, jax.local_device_count())\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T14:54:53.622367Z","iopub.execute_input":"2023-04-23T14:54:53.622854Z","iopub.status.idle":"2023-04-23T14:54:54.452390Z","shell.execute_reply.started":"2023-04-23T14:54:53.622815Z","shell.execute_reply":"2023-04-23T14:54:54.451035Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nmetric_start = RMSE()\nmetric_end = RMSE()\n    \nfor i, epoch in enumerate(tqdm(range(1, num_train_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n    rng, input_rng = jax.random.split(rng)\n    \n    metric_start = RMSE()\n    metric_end = RMSE()\n    \n    for batch in train_data_loader(input_rng, train_dataset, total_batch_size):\n        state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n\n    # evaluate\n    for batch in eval_data_loader(eval_dataset, total_batch_size):\n        start_positions = batch[\"start_positions\"]\n        end_positions = batch[\"end_positions\"]\n        inputs = {k: v for k, v in batch.items() if k not in [\"start_positions\", \"end_positions\"]}\n        start_logits, end_logits = parallel_eval_step(state, inputs)\n        predictions_start = start_logits\n        predictions_end = end_logits\n        references_start = start_positions\n        references_end = end_positions\n        metric_start.add_batch(predictions=chain(*predictions_start), references=chain(*references_start))\n        metric_end.add_batch(predictions=chain(*predictions_end), references=chain(*references_end))\n\n    start_rmse = round(metric_start.compute()['RMSE'], 3)\n    end_rmse = round(metric_end.compute()['RMSE'], 3)\n\n    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n    metric_name = \"RMSE\"\n    print(f\"{i+1}/{num_train_epochs} | Train loss: {loss} | Eval {metric_name}: start={start_rmse}, end={end_rmse}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T14:54:56.927226Z","iopub.execute_input":"2023-04-23T14:54:56.928059Z","iopub.status.idle":"2023-04-23T15:38:58.118449Z","shell.execute_reply.started":"2023-04-23T14:54:56.928017Z","shell.execute_reply":"2023-04-23T15:38:58.116912Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stderr","text":"Epoch ...:   2%|▏         | 1/50 [01:47<1:27:27, 107.10s/it]","output_type":"stream"},{"name":"stdout","text":"1/50 | Train loss: 464000.0 | Eval RMSE: start=971.408, end=985.404\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:   4%|▍         | 2/50 [02:39<59:48, 74.76s/it]   ","output_type":"stream"},{"name":"stdout","text":"2/50 | Train loss: 138866.5 | Eval RMSE: start=969.896, end=985.383\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:   6%|▌         | 3/50 [03:29<49:58, 63.79s/it]","output_type":"stream"},{"name":"stdout","text":"3/50 | Train loss: 508565.0 | Eval RMSE: start=969.372, end=986.188\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:   8%|▊         | 4/50 [04:22<45:30, 59.36s/it]","output_type":"stream"},{"name":"stdout","text":"4/50 | Train loss: 1037620.0 | Eval RMSE: start=969.914, end=985.99\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  10%|█         | 5/50 [05:13<42:10, 56.23s/it]","output_type":"stream"},{"name":"stdout","text":"5/50 | Train loss: 375266.0 | Eval RMSE: start=969.516, end=985.925\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  12%|█▏        | 6/50 [06:04<40:03, 54.62s/it]","output_type":"stream"},{"name":"stdout","text":"6/50 | Train loss: 1822509.0 | Eval RMSE: start=971.707, end=984.65\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  14%|█▍        | 7/50 [06:55<38:12, 53.31s/it]","output_type":"stream"},{"name":"stdout","text":"7/50 | Train loss: 710720.0 | Eval RMSE: start=970.375, end=985.708\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  16%|█▌        | 8/50 [07:45<36:42, 52.45s/it]","output_type":"stream"},{"name":"stdout","text":"8/50 | Train loss: 569413.0 | Eval RMSE: start=971.704, end=985.22\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  18%|█▊        | 9/50 [08:36<35:26, 51.87s/it]","output_type":"stream"},{"name":"stdout","text":"9/50 | Train loss: 747185.0 | Eval RMSE: start=969.941, end=985.253\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  20%|██        | 10/50 [09:28<34:35, 51.88s/it]","output_type":"stream"},{"name":"stdout","text":"10/50 | Train loss: 240985.0 | Eval RMSE: start=967.777, end=986.525\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  22%|██▏       | 11/50 [10:21<33:52, 52.13s/it]","output_type":"stream"},{"name":"stdout","text":"11/50 | Train loss: 1750338.0 | Eval RMSE: start=971.445, end=985.627\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  24%|██▍       | 12/50 [11:13<33:08, 52.32s/it]","output_type":"stream"},{"name":"stdout","text":"12/50 | Train loss: 1838745.0 | Eval RMSE: start=971.451, end=986.903\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  26%|██▌       | 13/50 [12:07<32:27, 52.65s/it]","output_type":"stream"},{"name":"stdout","text":"13/50 | Train loss: 561980.5 | Eval RMSE: start=969.448, end=986.202\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  28%|██▊       | 14/50 [12:59<31:33, 52.58s/it]","output_type":"stream"},{"name":"stdout","text":"14/50 | Train loss: 1819810.0 | Eval RMSE: start=970.375, end=985.727\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  30%|███       | 15/50 [13:52<30:38, 52.52s/it]","output_type":"stream"},{"name":"stdout","text":"15/50 | Train loss: 883876.5 | Eval RMSE: start=971.527, end=986.107\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  32%|███▏      | 16/50 [14:44<29:45, 52.51s/it]","output_type":"stream"},{"name":"stdout","text":"16/50 | Train loss: 1679625.0 | Eval RMSE: start=970.664, end=985.031\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  34%|███▍      | 17/50 [15:37<28:52, 52.49s/it]","output_type":"stream"},{"name":"stdout","text":"17/50 | Train loss: 146450.0 | Eval RMSE: start=970.368, end=986.359\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  36%|███▌      | 18/50 [16:30<28:07, 52.74s/it]","output_type":"stream"},{"name":"stdout","text":"18/50 | Train loss: 1731865.0 | Eval RMSE: start=971.549, end=985.335\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  38%|███▊      | 19/50 [17:21<27:00, 52.27s/it]","output_type":"stream"},{"name":"stdout","text":"19/50 | Train loss: 1560005.0 | Eval RMSE: start=969.484, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  40%|████      | 20/50 [18:11<25:51, 51.73s/it]","output_type":"stream"},{"name":"stdout","text":"20/50 | Train loss: 384840.0 | Eval RMSE: start=970.522, end=986.185\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  42%|████▏     | 21/50 [19:03<24:55, 51.58s/it]","output_type":"stream"},{"name":"stdout","text":"21/50 | Train loss: 578148.5 | Eval RMSE: start=970.645, end=985.215\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  44%|████▍     | 22/50 [19:53<23:57, 51.33s/it]","output_type":"stream"},{"name":"stdout","text":"22/50 | Train loss: 232082.5 | Eval RMSE: start=971.298, end=984.732\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  46%|████▌     | 23/50 [20:44<23:03, 51.25s/it]","output_type":"stream"},{"name":"stdout","text":"23/50 | Train loss: 1451434.0 | Eval RMSE: start=971.44, end=985.794\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  48%|████▊     | 24/50 [21:38<22:31, 51.97s/it]","output_type":"stream"},{"name":"stdout","text":"24/50 | Train loss: 536485.0 | Eval RMSE: start=969.304, end=985.701\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  50%|█████     | 25/50 [22:30<21:41, 52.07s/it]","output_type":"stream"},{"name":"stdout","text":"25/50 | Train loss: 333230.5 | Eval RMSE: start=968.668, end=986.13\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  52%|█████▏    | 26/50 [23:22<20:44, 51.84s/it]","output_type":"stream"},{"name":"stdout","text":"26/50 | Train loss: 616274.0 | Eval RMSE: start=970.392, end=985.929\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  54%|█████▍    | 27/50 [24:12<19:44, 51.51s/it]","output_type":"stream"},{"name":"stdout","text":"27/50 | Train loss: 726608.5 | Eval RMSE: start=971.632, end=985.424\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  56%|█████▌    | 28/50 [25:05<18:57, 51.71s/it]","output_type":"stream"},{"name":"stdout","text":"28/50 | Train loss: 399325.0 | Eval RMSE: start=971.41, end=985.195\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  58%|█████▊    | 29/50 [25:55<17:55, 51.22s/it]","output_type":"stream"},{"name":"stdout","text":"29/50 | Train loss: 1734498.0 | Eval RMSE: start=971.577, end=985.804\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  60%|██████    | 30/50 [26:47<17:09, 51.47s/it]","output_type":"stream"},{"name":"stdout","text":"30/50 | Train loss: 1697813.0 | Eval RMSE: start=971.447, end=985.03\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  62%|██████▏   | 31/50 [27:39<16:23, 51.79s/it]","output_type":"stream"},{"name":"stdout","text":"31/50 | Train loss: 1962850.0 | Eval RMSE: start=971.435, end=984.748\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  64%|██████▍   | 32/50 [28:31<15:34, 51.89s/it]","output_type":"stream"},{"name":"stdout","text":"32/50 | Train loss: 1763588.0 | Eval RMSE: start=970.248, end=985.0\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  66%|██████▌   | 33/50 [29:22<14:36, 51.53s/it]","output_type":"stream"},{"name":"stdout","text":"33/50 | Train loss: 1897562.5 | Eval RMSE: start=970.361, end=986.487\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  68%|██████▊   | 34/50 [30:14<13:45, 51.57s/it]","output_type":"stream"},{"name":"stdout","text":"34/50 | Train loss: 1671858.0 | Eval RMSE: start=971.579, end=986.943\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  70%|███████   | 35/50 [31:05<12:53, 51.58s/it]","output_type":"stream"},{"name":"stdout","text":"35/50 | Train loss: 930102.5 | Eval RMSE: start=971.539, end=985.637\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  72%|███████▏  | 36/50 [31:58<12:04, 51.74s/it]","output_type":"stream"},{"name":"stdout","text":"36/50 | Train loss: 1133990.5 | Eval RMSE: start=969.183, end=986.17\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  74%|███████▍  | 37/50 [32:48<11:08, 51.46s/it]","output_type":"stream"},{"name":"stdout","text":"37/50 | Train loss: 1729229.0 | Eval RMSE: start=971.417, end=984.924\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  76%|███████▌  | 38/50 [33:41<10:20, 51.72s/it]","output_type":"stream"},{"name":"stdout","text":"38/50 | Train loss: 685916.5 | Eval RMSE: start=970.323, end=984.468\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  78%|███████▊  | 39/50 [34:32<09:29, 51.73s/it]","output_type":"stream"},{"name":"stdout","text":"39/50 | Train loss: 35345.0 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  80%|████████  | 40/50 [35:23<08:35, 51.52s/it]","output_type":"stream"},{"name":"stdout","text":"40/50 | Train loss: 1981238.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  82%|████████▏ | 41/50 [36:15<07:43, 51.55s/it]","output_type":"stream"},{"name":"stdout","text":"41/50 | Train loss: 1763593.0 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  84%|████████▍ | 42/50 [37:06<06:51, 51.44s/it]","output_type":"stream"},{"name":"stdout","text":"42/50 | Train loss: 492554.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  86%|████████▌ | 43/50 [37:57<05:58, 51.25s/it]","output_type":"stream"},{"name":"stdout","text":"43/50 | Train loss: 1771565.0 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  88%|████████▊ | 44/50 [38:47<05:05, 50.97s/it]","output_type":"stream"},{"name":"stdout","text":"44/50 | Train loss: 1596444.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  90%|█████████ | 45/50 [39:39<04:15, 51.08s/it]","output_type":"stream"},{"name":"stdout","text":"45/50 | Train loss: 1916896.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  92%|█████████▏| 46/50 [40:30<03:24, 51.21s/it]","output_type":"stream"},{"name":"stdout","text":"46/50 | Train loss: 143666.0 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  94%|█████████▍| 47/50 [41:22<02:34, 51.45s/it]","output_type":"stream"},{"name":"stdout","text":"47/50 | Train loss: 1032362.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  96%|█████████▌| 48/50 [42:16<01:44, 52.04s/it]","output_type":"stream"},{"name":"stdout","text":"48/50 | Train loss: 618842.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  98%|█████████▊| 49/50 [43:08<00:52, 52.21s/it]","output_type":"stream"},{"name":"stdout","text":"49/50 | Train loss: 1670576.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...: 100%|██████████| 50/50 [44:01<00:00, 52.82s/it]","output_type":"stream"},{"name":"stdout","text":"50/50 | Train loss: 1581336.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"### import os\nimport jax\nfrom flax.serialization import to_bytes, from_bytes\n\n# Определяем путь к файлу, в который мы хотим сохранить модель.\nmodel_path = 'my_model.jax'\n\n# Получаем параметры модели, которые мы хотим сохранить.\nmodel_params = jax.tree_map(lambda x: x.block_until_ready(), state.params)\n\n# Преобразуем параметры в байтовую строку.\nmodel_bytes = to_bytes(model_params)\n\n# Сохраняем модель в файл.\nwith open(model_path, 'wb') as f:\n    f.write(model_bytes)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:50:40.565056Z","iopub.execute_input":"2023-04-23T15:50:40.565474Z","iopub.status.idle":"2023-04-23T15:51:25.801271Z","shell.execute_reply.started":"2023-04-23T15:50:40.565445Z","shell.execute_reply":"2023-04-23T15:51:25.799958Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"# import jax\n# from flax.serialization import from_bytes\n\n# # Определяем путь к файлу, из которого мы хотим загрузить модель.\n# model_path = 'my_model.jax'\n\n# # Загружаем модель из файла.\n# with open(model_path, 'rb') as f:\n#     model_bytes = f.read()\n\n# # Преобразуем байтовую строку в параметры модели.\n# model_params = from_bytes(model_bytes)\n\n# # Создаем новый экземпляр модели, используя загруженные параметры.\n# model = FlaxAutoModelForQuestionAnswering(**model_params)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_data_loader(dataset, batch_size):\n    if len(dataset)<batch_size:\n        batch = dataset[:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        yield batch\n    else:\n        for i in range(len(dataset) // batch_size):\n            batch = dataset[i * batch_size : (i + 1) * batch_size]\n            batch = {k: jnp.array(v) for k, v in batch.items()}\n\n            yield batch\n        batch = dataset[(i+1) * batch_size:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        yield batch\n        \nfrom flax.jax_utils import unreplicate\n\nunrep_state = unreplicate(state)\n\n\ndef test_step(unrep_state, batch):\n    start_logits, end_logits = unrep_state.apply_fn(**batch, params=unrep_state.params, train=False)[0:2]\n    return state.logits_function(start_logits), state.logits_function(end_logits)\n\nparallel_test_step = jax.pmap(test_step, axis_name=\"batch\")\n\ndef generate_results():\n    preds = []\n    for batch in test_data_loader(test_dataset, total_batch_size):\n\n        if jax.process_index() == 0:\n            inputs = {k: v for k, v in batch.items()}\n            start_logits, end_logits = parallel_test_step(state, inputs)\n            preds.append((start_logits, end_logits))\n    return preds\n","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:23:47.226124Z","iopub.execute_input":"2023-04-19T06:23:47.227060Z","iopub.status.idle":"2023-04-19T06:23:48.475211Z","shell.execute_reply.started":"2023-04-19T06:23:47.227018Z","shell.execute_reply":"2023-04-19T06:23:48.473911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = generate_results()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:23:50.086713Z","iopub.execute_input":"2023-04-19T06:23:50.087644Z","iopub.status.idle":"2023-04-19T06:23:53.131172Z","shell.execute_reply.started":"2023-04-19T06:23:50.087602Z","shell.execute_reply":"2023-04-19T06:23:53.129742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# os.environ[\"WANDB_MODE\"] = \"disabled\"\nfrom datasets import Dataset, DatasetDict\nimport datasets\nmodel_name = \"valhalla/longformer-base-4096-finetuned-squadv1\" \"AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru\"\nmax_length = 4000\ntokenizer = AutoTokenizer.from_pretrained(model_name)\npad_on_right = tokenizer.padding_side == \"right\"\nqa_dataset = QADataset(train_data, val_data)\ntokenized_dataset = qa_dataset.dataset_dict.map(prepare_train_features, batched=True, \n                                                 remove_columns=qa_dataset.dataset_dict[\"train\"].column_names)\n\nQAtrainer = QATrainer(\n    model_name=model_name,\n    train_dataset=tokenized_dataset['train'],\n    val_dataset=tokenized_dataset['validation']\n)\n\nQAtrainer.training()\n\n# use_tpu = True  # Change to True if using TPU\n\n# if use_tpu:\n#     # Create distribution strategy\n#     print('TPU used')\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n#     strategy = tf.distribute.TPUStrategy(tpu)\n\n#     with strategy.scope():\n#         # Create model\n#         print('TPU used')\n# #         model = create_model(model_name)\n#         # Create trainer\n#         QAtrainer = QATrainer(\n#             model_name=model_name,\n#             train_dataset=tokenized_dataset['train'],\n#             val_dataset=tokenized_dataset['validation']\n#         )\n# else:\n#     if tf.config.list_physical_devices('GPU'):\n# #         gpus = tf.config.experimental.list_physical_devices('GPU')\n# #         for gpu in gpus:\n# #             tf.config.experimental.set_virtual_device_configuration(\n# #                 gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n# #         tf.config.optimizer.set_jit(True)\n# #         policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n# #         tf.keras.mixed_precision.experimental.set_policy(policy)\n#         strategy = tf.distribute.MirroredStrategy()\n#     else:\n#         strategy = tf.distribute.OneDeviceStrategy(device=\"/CPU:0\")\n\n#     with strategy.scope():\n# #         model = create_model(model_name)\n#         # Create trainer\n#         QAtrainer = QATrainer(\n#             model_name=model_name,\n#             train_dataset=tokenized_dataset['train'],\n#             val_dataset=tokenized_dataset['validation']\n#         )\n\n# # Train the model\n# QAtrainer.training()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T13:06:28.834300Z","iopub.execute_input":"2023-04-18T13:06:28.834993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tokenized_dataset['train']['attention_mask'][6])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets['validation']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qa_dataset = QADataset(train_data, val_data)\nqa_dataset.dataset_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip list | grep transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataframe(data, fields, subfields):\n    main_df = pd.DataFrame(data)[fields]\n    sub_df_list = []\n    for subfield in subfields:\n        sub_df = pd.DataFrame(list(main_df[subfield]))\n        sub_df.columns = [f\"{subfield}_{col}\" for col in sub_df.columns]\n        sub_df_list.append(sub_df)\n    main_df = main_df.drop(columns=['extracted_part'])\n    return pd.concat([main_df] + sub_df_list, axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_qa_examples(train_data)[0]['context']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmax_seq_length = 4000\nmodel_name = \"valhalla/longformer-base-4096-finetuned-squadv1\"\n# LongformerTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = LongformerConfig.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')\nconfig.attention_mode = 'sliding_chunks'\n\nnum_epochs = 3\nbatch_size = 16\npad_on_right = tokenizer.padding_side == \"right\"\ntrain_dataset = QADataset(train_data, model_name, max_seq_length)\nval_dataset = QADataset(val_data, model_name, max_seq_length)\n# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\nassert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer('<s>', '<s>')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"QAtrainer = QATrainer(\n    model_name=model_name,\n    train_dataset=tokenized_dataset['train'],\n    val_dataset=tokenized_dataset['validation']\n)\n\nQAtrainer.training()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TPU used try 2","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import default_data_collator\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata_collator = default_data_collator\nargs = TrainingArguments(\n    model_name,\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    push_to_hub=True,\n)\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['validation'],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfrom transformers import BertConfig, BertModel\n\n# bert_config = BertConfig(\n#     vocab_size=32000,\n#     hidden_size=768,\n#     num_hidden_layers=12,\n#     num_attention_heads=12,\n#     intermediate_size=3072,\n#     hidden_dropout_prob=0.1,\n#     attention_probs_dropout_prob=0.1,\n#     max_position_embeddings=512,\n#     type_vocab_size=2,\n#     initializer_range=0.02,\n#     layer_norm_eps=1e-12,\n#     gradient_checkpointing=False,\n#     position_embedding_type=\"absolute\",\n#     use_cache=True,\n#     is_decoder=False,\n#     pad_token_id=0,\n#     bos_token_id=1,\n#     eos_token_id=2\n# )\n\ntokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\nmax_seq_length = 4000\nbatch_size = 16\nepochs = eps = 1\n     \ntrain_dataset = QADataset(train_data, tokenizer, max_seq_length)\nval_dataset = QADataset(val_data, tokenizer, max_seq_length)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n\n# qa_model = QAModel(bert_config)\n# qa_trainer = QATrainer(qa_model, train_dataloader, val_dataloader, lr=1e-12, eps=eps)\n# train_losses, val_losses = qa_trainer.train(epochs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, val_loader):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for step, batch in enumerate(val_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            segment_ids = batch['segment_ids'].to(device)\n            start_positions = batch['start_positions'].to(device)\n            end_positions = batch['end_positions'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=segment_ids, start_positions=start_positions, end_positions=end_positions)\n            loss = outputs.loss\n            total_loss += loss.item()\n        avg_loss = total_loss / len(val_loader)\n        return avg_loss\n\ndef predict(model, test_loader):\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for step, batch in enumerate(test_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            segment_ids = batch['segment_ids'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=segment_ids)\n            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n            start_preds = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n            end_preds = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n            for i in range(len(start_preds)):\n                start_pred = np.argmax(start_preds[i])\n                end_pred = np.argmax(end_preds[i])\n                if start_pred > end_pred:\n                    answer = \"\"\n                else:\n                    answer = tokenizer.decode(input_ids[i][start_pred:end_pred+1], skip_special_tokens=True)\n                predictions.append({\n                    \"context\": batch['context'][i],\n                    \"question\": batch['question'][i],\n                    \"extracted_part\": answer\n                })\n    with open('predictions.json', 'w', encoding='utf-8') as f:\n        json.dump(predictions, f, ensure_ascii=False, indent=4)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class QADataset(Dataset):\n    def __init__(self, data, tokenizer, max_seq_length):\n        self.examples = self.create_qa_examples(data)\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n        self.skip = False\n\n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        context = example['context']\n        question = example['question']\n        answer = example['answer']\n        answer_start = example['answer_start']\n        answer_end = example['answer_end']\n        assert answer_end <= len(example['context'])\n        is_char_in_ans = [0] * len(context)\n        for i in range(answer_start, answer_end):\n            is_char_in_ans[i] = 1\n        tokenized_context = self.tokenizer.encode_plus(context, add_special_tokens=False, return_offsets_mapping=True, return_tensors=\"tf\")\n        ans_token_idx = []\n        is_ans_token = [0] * len(tokenized_context)\n\n        for idx, token in enumerate(tokenized_context):\n            token_start = tokenized_context.token_to_chars(idx)[0]\n            token_end = tokenized_context.token_to_chars(idx)[1]\n            if sum(is_char_in_ans[token_start:token_end]) > 0:\n                ans_token_idx.append(idx)\n                for i in range(token_start, token_end):\n                    is_ans_token[i] = 1\n        if sum(is_ans_token) == 0:\n            start_token_idx, end_token_idx = 0, 0\n        else:\n            start_token_idx = ans_token_idx[0]\n            end_token_idx = ans_token_idx[-1]\n            while start_token_idx > 0 and is_ans_token[tokenized_context.token_to_chars(start_token_idx-1)[0]]:\n                start_token_idx -= 1\n            while end_token_idx < len(tokenized_context)-1 and is_ans_token[tokenized_context.token_to_chars(end_token_idx+1)[1]-1]:\n                end_token_idx += 1\n            \n\n        tokenized_question = self.tokenizer.encode_plus(question, return_offsets_mapping=True, return_tensors=\"tf\")\n        tokens = ['<s>'] + tokenized_context.tokens() + ['</s>']+ ['</s>'] + tokenized_question.tokens() + ['</s>']\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        token_type_ids = [0] * (len(tokenized_context.tokens())+2) + [1] * (len(\n            tokenized_question.tokens())+2)\n        attention_mask = [1] * len(input_ids)\n        padding_length = self.max_seq_length - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n        features = []\n#         encoded_dict = self.tokenizer.encode_plus(\n#             question,\n#             context,\n#             add_special_tokens=True,\n#             truncation='longest_first',\n#             max_length=self.max_seq_length,\n#             return_tensors='pt'\n#         )\n#         input_ids = encoded_dict['input_ids'].squeeze()\n#         attention_mask = encoded_dict['attention_mask'].squeeze()\n#         input_ids = torch.nn.functional.pad(encoded_dict['input_ids'], (0, self.max_seq_length - encoded_dict['input_ids'].shape[1]), mode='constant', value=0)\n#         attention_mask = torch.nn.functional.pad(encoded_dict['attention_mask'], (0, self.max_seq_length - encoded_dict['attention_mask'].shape[1]), mode='constant', value=0)\n        \n        features = {'input_ids': input_ids, 'attention_mask': attention_mask, \n                    'token_type_ids': token_type_ids, 'start_token_idx': start_token_idx, 'end_token_idx': end_token_idx}\n#         max_len_dict = {}\n#         for key, value in features.items():\n#             if isinstance(value, (list, tuple)):\n#                 max_len_dict[key] = max(len(seq) for seq in value)\n#         for key, value in features.items():\n#             if isinstance(value, (list, tuple)):\n#                 max_len = max_len_dict[key]\n#                 for i in range(len(value)):\n#                     pad_len = max_len - len(value[i])\n#                     value[i] = torch.cat([value[i], torch.zeros(pad_len, dtype=torch.long)])\n#                 features[key] = torch.stack(value)\n\n        return features\n    \n    def create_qa_examples(self, data):\n        examples = []\n        for row in data:\n            text = row['text']\n            question = row['label']\n            extracted_part = row.get('extracted_part', {})\n            if extracted_part and 'text' in extracted_part:\n                answer = extracted_part['text'][0].strip()\n                answer_start = extracted_part['answer_start'][0]\n                answer_end = extracted_part['answer_end'][0]\n            else:\n                answer = answer_start = answer_end = None\n\n            example = {'context': text, 'question': question, 'answer': answer, 'answer_start': answer_start, 'answer_end': answer_end}\n            examples.append(example)\n        return examples\n    \n    \n    @staticmethod\n    def prepare_test_data(data):\n        examples = []\n        for row in data:\n            text = row['text']\n            question = row['label']\n            example = {'context': text, 'question': question}\n            examples.append(example)\n        return examples\n\ndef collate_fn(batch, device):\n    input_ids = pad_sequence([torch.tensor(example['input_ids']) for example in batch], batch_first=True, padding_value=0).to(device)\n    attention_mask = pad_sequence([torch.tensor(example['attention_mask']) for example in batch], batch_first=True, padding_value=0).to(device)\n    token_type_ids = pad_sequence([torch.tensor(example['token_type_ids']) for example in batch], batch_first=True, padding_value=0).to(device)\n    start_positions = torch.tensor([example['answer_start'] for example in batch]).to(device)\n    end_positions = torch.tensor([example['answer_end'] for example in batch]).to(device)\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'token_type_ids': token_type_ids,\n        'start_positions': start_positions,\n        'end_positions': end_positions\n    }\n\n\ndef create_inputs_targets(dataset):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n        \"start_token_idx\": [],\n        \"end_token_idx\": [],\n    }\n    for idx in range(len(dataset)):\n        example = dataset[idx]\n        for key in dataset_dict:\n            if isinstance(example[key], torch.Tensor):\n                value = example[key].numpy().tolist()\n            else:\n                value = example[key]\n            dataset_dict[key].append(value)\n\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n    return x, y\n\ndef x_y_split(model_name, train_data, validation_data, batch_size):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)\n#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    train_dataset = QADataset(train_data, tokenizer, max_seq_length)\n#     train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True,\n#                                    collate_fn=lambda batch: collate_fn(batch, device))\n    x_train, y_train = create_inputs_targets(train_dataset)\n    \n    validation_dataset = QADataset(validation_data, tokenizer, max_seq_length)\n#     validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, drop_last=True,\n#                                         collate_fn=lambda batch: collate_fn(batch, device))\n    x_val, y_val = create_inputs_targets(validation_dataset)\n\n    return x_train, y_train, x_val, y_val\n\n\n\ndef create_model(model_name):\n    ## BERT encoder\n    encoder = TFLongformerForQuestionAnswering.from_pretrained(model_name)\n\n    ## QA Model\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model\n\nclass ExactMatch(keras.callbacks.Callback):\n    def __init__(self, x_eval, y_eval):\n        super().__init__()\n        self.x_eval = x_eval\n        self.y_eval = y_eval\n\n    def on_epoch_end(self, epoch, logs=None):\n        pred_start, pred_end = self.model.predict(self.x_eval)\n        count = 0\n        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n            offsets = squad_eg.context_token_to_char\n            start = np.argmax(start)\n            end = np.argmax(end)\n            if start >= len(offsets):\n                continue\n            pred_char_start = offsets[start][0]\n            if end < len(offsets):\n                pred_char_end = offsets[end][1]\n                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n            else:\n                pred_ans = squad_eg.context[pred_char_start:]\n\n            if pred_ans in squad_eg.all_answers:\n                count += 1\n        acc = count / len(self.y_eval[0])\n        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\nexamp = QADataset(train_data, tokenizer, max_seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"QADataset(train_data, tokenizer, max_seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, validation_data = train_test_split(train_data[:50], test_size=0.2, random_state=42)\nmax_seq_length = 4000\nmodel_name = \"valhalla/longformer-base-4096-finetuned-squadv1\"\nconfiguration = LongformerConfig()\nnum_epochs = 3\n\nx_train, y_train, x_val, y_val = x_y_split(model_name = model_name, train_data = train_data, validation_data = validation_data, batch_size = 16)\nmax_len = len(x_train[0][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y_train[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train имеет структуру словаря, в котором есть 3 подсловаря - признака, в каждом из них набор примеров n-го количества, в каждом примере уже непосредственно находятся данные\nв y_train 2 словаря, которые содержат n примеров, в каждом из которых находится таргет. \nкакие модели обучения можно написать на таких данных, не пользуясь предобученными моделями и их ограничениями","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntf.debugging.set_log_device_placement(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_tpu = False  # Change to True if using TPU\n\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    strategy = tf.distribute.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        print('TPU used')\n        model = create_model(model_name)\nelse:\n#     # Use GPU if TPU is not available\n#     if tf.config.list_physical_devices('GPU'):\n#         strategy = tf.distribute.MirroredStrategy()\n        \n#     else:\n    strategy = tf.distribute.OneDeviceStrategy(device=\"/CPU:0\")\n\n    with strategy.scope():\n        model = create_model(model_name)\n\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nprint(tf.test.is_built_with_cuda())\nprint(tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx_train_np = [np.array(x_train[0]), np.array(x_train[1]), np.array(x_train[2])]\ny_train_np = [np.array(y_train[0]), np.array(y_train[1])]\nx_val_np = [np.array(x_val[0]), np.array(x_val[1]), np.array(x_val[2])]\ny_val_np = [np.array(y_val[0]), np.array(y_val[1])]\nexact_match_callback = ExactMatch(x_val_np, y_val_np)\nmodel.fit(\n    x_train_np,\n    y_train_np,\n    validation_data=(x_val_np, y_val_np),\n    epochs=1,\n    verbose=2,\n    batch_size=64,\n#     callbacks=[exact_match_callback],\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = QADataset(train_data, tokenizer, max_seq_length)\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True,\n                               collate_fn=lambda batch: collate_fn(batch, device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader.dataset[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained('cointegrated/LaBSE-en-ru')\n# examples = QADataset.create_qa_examples(train_data, train_data)\n# questions = [example['context'] for example in examples]\n# question_tokens = [tokenizer.tokenize(question) for question in questions]\n# import matplotlib.pyplot as plt\n\n# question_lengths = [len(tokens) for tokens in question_tokens]\n# plt.hist(question_lengths, bins=50)\n# plt.xlabel('Length of question tokens')\n# plt.ylabel('Frequency')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Разбиваем данные на обучающую и валидационную выборки\n# train_data, validation_data = train_test_split(train_data, test_size=0.2, random_state=42)\n# max_seq_length = 3072\n# model_name = \"allenai/longformer-large-4096-finetuned-triviaqa\"\n\n# # Число эпох обучения\n# num_epochs = 3\n# output_dir = 'my_model'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'cointegrated/LaBSE-en-ru'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import BigBirdTokenizer, BigBirdForQuestionAnswering\n\n# tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n# model = BigBirdForQuestionAnswering.from_pretrained('google/bigbird-roberta-base')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# import tensorflow as tf\n\n# train(model_name, train_data, validation_data, output_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}