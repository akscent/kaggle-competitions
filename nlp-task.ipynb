{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install jax jaxlib flax optax mtranslate sentencepiece datasets transformers accelerate scikit-learn ipywidgets datasets nltk importlib-metadata\n!pip install transformers --upgrade","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-24T00:00:40.758280Z","iopub.execute_input":"2023-04-24T00:00:40.758808Z","iopub.status.idle":"2023-04-24T00:01:05.288378Z","shell.execute_reply.started":"2023-04-24T00:00:40.758765Z","shell.execute_reply":"2023-04-24T00:01:05.286596Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/huggingface/transformers.git\n","metadata":{"execution":{"iopub.status.busy":"2023-04-20T00:14:44.532708Z","iopub.execute_input":"2023-04-20T00:14:44.534013Z","iopub.status.idle":"2023-04-20T00:15:00.813921Z","shell.execute_reply.started":"2023-04-20T00:14:44.533947Z","shell.execute_reply":"2023-04-20T00:15:00.812679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/transformers/examples/flax/question-answering/","metadata":{"execution":{"iopub.status.busy":"2023-04-20T00:16:31.479150Z","iopub.execute_input":"2023-04-20T00:16:31.479762Z","iopub.status.idle":"2023-04-20T00:16:31.488498Z","shell.execute_reply.started":"2023-04-20T00:16:31.479719Z","shell.execute_reply":"2023-04-20T00:16:31.487043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, HTML\n# from huggingface_hub import notebook_login\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForQuestionAnswering, AdamW, Trainer, TrainingArguments, default_data_collator, FlaxAutoModelForQuestionAnswering\nimport time\nimport nltk\nimport math\nimport torch\nfrom datasets import Dataset, DatasetDict, load_dataset, load_metric\nimport tensorflow as tf\nimport re\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport transformers\n# from accelerate import Accelerator\nimport datasets\nimport sentencepiece\nfrom time import sleep\nfrom time import time\nfrom random import randint\n","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:02:31.459767Z","iopub.execute_input":"2023-04-24T00:02:31.460303Z","iopub.status.idle":"2023-04-24T00:02:46.431177Z","shell.execute_reply.started":"2023-04-24T00:02:31.460257Z","shell.execute_reply":"2023-04-24T00:02:46.429623Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Вход в huggingface\n# notebook_login()\n!huggingface-cli login --token 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","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:02:51.291612Z","iopub.execute_input":"2023-04-24T00:02:51.292514Z","iopub.status.idle":"2023-04-24T00:02:52.851198Z","shell.execute_reply.started":"2023-04-24T00:02:51.292469Z","shell.execute_reply":"2023-04-24T00:02:52.849648Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"def translate_sentence(text):\n    input_ids = t_tokenizer(text, return_tensors=\"jax\").input_ids\n    sequences = t_model.generate(input_ids, num_beams=2).sequences\n    outputs = t_tokenizer.batch_decode(sequences, skip_special_tokens=True)\n    return outputs\n\nfrom jax import vmap, jit\n\n@jit\ndef translate_sentence_batch(words):\n    return vmap(translate_sentence)(words)\n\ndef translate_sentences(sentences):\n    words = sentence.split(' ')\n    outputs = translate_sentence_batch(words)\n    return [' '.join(output) for output in outputs]\n","metadata":{"execution":{"iopub.status.busy":"2023-04-21T00:53:28.991408Z","iopub.execute_input":"2023-04-21T00:53:28.991950Z","iopub.status.idle":"2023-04-21T00:53:29.002669Z","shell.execute_reply.started":"2023-04-21T00:53:28.991904Z","shell.execute_reply":"2023-04-21T00:53:29.001290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"УТВЕРЖДАЮ: Председатель закупочной комиссии, заместитель генерального директора - по логистике и МТО АО «АТХ» ____________________ Т.Ю. Шустова «01» сентября 2022 г. ДОКУМЕНТАЦИЯ О КОНКУРЕНТНОЙ ЗАКУПКЕ ЗАПРОС ПРЕДЛОЖЕНИЙ В ЭЛЕКТРОННОЙ ФОРМЕ, УЧАСТНИКАМИ КОТОРОГО МОГУТ БЫТЬ ТОЛЬКО СУБЪЕКТЫ МАЛОГО И СРЕДНЕГО ПРЕДПРИНИМАТЕЛЬСТВА на право заключения Договора на выполнение работ по ремонту зданий и сооружений г. Киров 2022 год. Стр.2 СОДЕРЖАНИЕ СОДЕРЖАНИЕ 2 I. ОБЩИЕ УСЛОВИЯ ПРОВЕДЕНИЯ закупки 3 1. ОБЩИЕ ПОЛОЖЕНИЯ 3 1.1. Правовой статус документов 3 1.2. Заказчик, предмет и условия проведения закупки. 3 1.3. Начальная (максимальная) цена договора 4 1.4. Требования к участникам закупки 4 1.5. Участие в закупке коллективных участников (группы лиц) 5 1.6. Привлечение соисполнителей (субподрядчиков) к исполнению договора 6 1.7. Расходы на участие в закупке и при заключении договора 7 1.8. Предоставление приоритетов товаров российского происхождения, работ, услуг, выполняемых, оказываемых российс 3.5.2, 5.6.4 Закупка по единичным расценкам Нет.\"\n\ntranslate_sentence(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-20T23:50:05.382087Z","iopub.execute_input":"2023-04-20T23:50:05.382757Z","iopub.status.idle":"2023-04-20T23:50:05.389117Z","shell.execute_reply.started":"2023-04-20T23:50:05.382724Z","shell.execute_reply":"2023-04-20T23:50:05.387884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n\nfrom transformers import FlaxMarianModel, MarianTokenizer, AutoModelForSeq2SeqLM, FlaxMarianMTModel\n\nt_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")\nt_model = FlaxMarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\", from_pt=True)\n\nt_model.params = t_model.to_fp16(t_model.params)\n\nimport re\n\ndef translate_sentence(sentence):\n    words = sentence.split(' ')\n    translated_words = []\n    batch_size = 1  # Number of words to translate in one batch\n    for i in range(0, len(words), batch_size):\n        batch = words[i:i+batch_size]\n        input_ids = t_tokenizer(batch, return_tensors=\"jax\", max_length=64, padding=True, truncation=True).input_ids\n        outputs = t_model.generate(input_ids).sequences\n        translated_batch = t_tokenizer.batch_decode(outputs, max_length=64, skip_special_tokens=True)\n        translated_words.extend(translated_batch)\n    translated_sentence = ' '.join(translated_words)\n    return translated_sentence\n    \ndef clean_text(text):\n    # Удаление указанных символов из текста\n    text = re.sub(r'[\\\"\\#\\$\\;\\:\\^\\&\\№\\*\\-\\=\\+\\-\\,\\.\\@\\!\\?\\/\\]\\[\\}\\{\\|\\~\\«\\»\\`]', '', text)\n    text = re.sub(r'_', ' ', text)  # Замена _ на пробел\n    text = re.sub(r'\\s+', ' ', text)  # Удаление лишних пробелов\n    return text.strip()\n\ndef create_qa_dataframe(data):\n    examples = []\n    for row in tqdm(data, total = len(data)):\n        cleaned_text = clean_text(row['text'])\n        question = row['label']\n        extracted_part = row.get('extracted_part', {})\n        if extracted_part and extracted_part['text'] is not None:\n            answer = clean_text(extracted_part['text'][0].strip())\n            answer_start = extracted_part['answer_start'][0]\n            answer_end = extracted_part['answer_end'][0]\n            if answer:\n                answer_words = answer.split()\n                match_found = False\n                for i in range(len(answer_words)):\n                    answer_start_new = cleaned_text.find(answer_words[i])\n                    if answer_start_new != -1:\n                        match_found = True\n                        for j in range(i+1, len(answer_words)):\n                            next_word_start = answer_start_new + len(answer_words[i-1])\n                            next_word_end = next_word_start + len(answer_words[j])\n                            next_word = cleaned_text[next_word_start:next_word_end]\n                            if answer_words[j] != next_word:\n                                match_found = False\n                                break\n                        if match_found:\n                            answer_start = answer_start_new\n                            answer_end = answer_start_new + len(answer_words) - 1\n                            break\n                if not match_found:\n                    answer_start = answer_end = 0\n            else:\n                answer_start = answer_end = 0\n        else:\n            answer_start = answer_end = 0\n            answer = None\n            \n#         translated_text = translate_sentence(cleaned_text)\n#         translated_question = translate_sentence(question)\n#         translated_answer = translate_sentence(answer) if answer else None\n\n        example = {'context': cleaned_text, 'question': question, 'answer': answer, 'answer_start': answer_start, 'answer_end': answer_end}\n        examples.append(example)\n    df = pd.DataFrame(examples)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-21T00:30:03.434227Z","iopub.execute_input":"2023-04-21T00:30:03.434638Z","iopub.status.idle":"2023-04-21T00:30:06.357018Z","shell.execute_reply.started":"2023-04-21T00:30:03.434608Z","shell.execute_reply":"2023-04-21T00:30:06.355554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = create_qa_dataframe(train_data)\ndisplay(HTML(train_df[6:12].to_html()))","metadata":{"execution":{"iopub.status.busy":"2023-04-21T00:30:12.222891Z","iopub.execute_input":"2023-04-21T00:30:12.224073Z","iopub.status.idle":"2023-04-21T00:30:12.659435Z","shell.execute_reply.started":"2023-04-21T00:30:12.224030Z","shell.execute_reply":"2023-04-21T00:30:12.658084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# определите столбцы, которые нужно перевести\ncols_to_translate = ['context', 'question', 'answer']\n\n# создайте функцию, которая будет переводить строки\ndef translate_df_row(row):\n    for col in cols_to_translate:\n        row[col] = translate_sentences([row[col]])[0]\n    return row\n\n# примените функцию к каждой строке датафрейма\ntranslated_df = train_df[cols_to_translate].apply(translate_df_row, axis=1)\n\n# объедините оригинальный датафрейм с переведенными столбцами\ntrain_df[cols_to_translate] = translated_df[cols_to_translate]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create dataset","metadata":{}},{"cell_type":"code","source":"import mtranslate\n\nclass QADataset:\n    def __init__(self, train_data, val_data, test_data=None):\n        self.train_examples = self.create_qa_example(train_data)\n        self.val_examples = self.create_qa_example(val_data)\n        self.test_examples = self.create_qa_example(test_data) if test_data else []\n        self.train_dataset = datasets.Dataset.from_pandas(pd.DataFrame(self.train_examples))\n        self.val_dataset = datasets.Dataset.from_pandas(pd.DataFrame(self.val_examples))\n        self.test_dataset = datasets.Dataset.from_pandas(pd.DataFrame(self.test_examples)) if test_data else None\n        if test_data:\n            self.dataset_dict = DatasetDict({\n                'train': self.train_dataset,\n                'validation': self.val_dataset,\n                'test': self.test_dataset\n            })\n        else:\n            self.dataset_dict = DatasetDict({\n                'train': self.train_dataset,\n                'validation': self.val_dataset\n            })\n\n    def clean_text(self, text):\n        # Удаление указанных символов из текста\n        text = re.sub(r'[\\\"\\#\\$\\;\\:\\^\\&\\№\\*\\-\\=\\+\\-\\,\\.\\@\\!\\?\\/\\]\\[\\}\\{\\|\\~\\«\\»\\`]', '', text)\n        text = re.sub(r'_', ' ', text)  # Замена _ на пробел\n        text = re.sub(r'\\s+', ' ', text)  # Удаление лишних пробелов\n        return text.strip()\n\n    def create_qa_example(self, data):\n        examples = []\n        for row in tqdm(data, total = len(data)):\n            cleaned_text = self.clean_text(row['text'])\n            question = row['label']\n            extracted_part = row.get('extracted_part', {})\n            if extracted_part and extracted_part['text'] is not None:\n                answer = self.clean_text(extracted_part['text'][0].strip())\n                answer_start = extracted_part['answer_start'][0]\n                answer_end = extracted_part['answer_end'][0]\n                if answer:\n                    answer_words = answer.split()\n                    match_found = False\n                    for i in range(len(answer_words)):\n                        answer_start_new = cleaned_text.find(answer_words[i])\n                        if answer_start_new != -1:\n                            match_found = True\n                            for j in range(i+1, len(answer_words)):\n                                next_word_start = answer_start_new + len(answer_words[i-1])\n                                next_word_end = next_word_start + len(answer_words[j])\n                                next_word = cleaned_text[next_word_start:next_word_end]\n                                if answer_words[j] != next_word:\n                                    match_found = False\n                                    break\n                            if match_found:\n                                answer_start = answer_start_new\n                                answer_end = answer_start_new + len(answer_words) - 1\n                                break\n                    if not match_found:\n                        answer_start = answer_end = 0\n                else:\n                    answer_start = answer_end = 0\n            else:\n                answer_start = answer_end = 0\n                answer = None\n\n#             translated_text = translate_sentence(cleaned_text)\n    #         translated_question = translate_sentence(question)\n    #         translated_answer = translate_sentence(answer) if answer else None\n\n            # Append example only if answer is not None\n            if answer_start != 0 :\n                example = {'context': cleaned_text, 'question': question, 'answer': answer, 'answer_start': answer_start, 'answer_end': answer_end}\n                examples.append(example)\n            \n        return examples\n    \ndef prepare_train_features(examples):\n#     examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=150,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        return_tensors=\"jax\",\n    )\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = tokenized_examples[\"input_ids\"][i]\n#         cls_index = input_ids.index(tokenizer.cls_token_id)\n        cls_index = jnp.where(input_ids == tokenizer.cls_token_id)[0][0]\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answer_start = examples[\"answer_start\"][sample_index]\n        answer_end = examples[\"answer_end\"][sample_index]\n        if answer_start == 0 or answer_start == None:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            tokenized_examples[\"start_positions\"].append(answer_start)\n            tokenized_examples[\"end_positions\"].append(answer_end)\n\n    return tokenized_examples\n\nclass QATrainer:\n    def __init__(self, model_name, train_dataset, val_dataset, batch_size=1, epochs=3):\n        self.model_name = model_name\n#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name, config=self.config)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n        self.batch_size = batch_size\n        self.epochs = epochs\n    \n    def training(self):\n        model_name = self.model_name.split(\"/\")[-1]\n        args = TrainingArguments(\n            model_name,\n            evaluation_strategy = \"epoch\",\n            learning_rate=2e-5,\n            per_device_train_batch_size=self.batch_size,\n            per_device_eval_batch_size=self.batch_size,\n            num_train_epochs=self.epochs,\n            report_to = 'none', \n            weight_decay=0.01,\n            push_to_hub=False,\n        )\n\n        trainer = Trainer(\n            model=self.model,\n            args=args,\n            train_dataset=self.train_dataset,\n            eval_dataset=self.val_dataset,\n            data_collator=default_data_collator,\n            tokenizer=self.tokenizer,\n        )\n        \n        trainer.train()\n        trainer.save_model(\"QA-trained\")\n        \n        return self.model","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:47:49.098214Z","iopub.execute_input":"2023-04-24T01:47:49.098977Z","iopub.status.idle":"2023-04-24T01:47:49.136275Z","shell.execute_reply.started":"2023-04-24T01:47:49.098925Z","shell.execute_reply":"2023-04-24T01:47:49.135042Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"markdown","source":"# Used git finetune script","metadata":{}},{"cell_type":"code","source":"# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n\n# Args\nnum_labels = 1\nseed = 60\nnum_train_epochs = 10\nlearning_rate = 2e-5\nper_device_batch_size = 3\ntotal_batch_size = per_device_batch_size * jax.local_device_count()\nmodel_name = \"google/bigbird-roberta-base\"\nmax_length = 1600\n\n# model elements\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = AutoConfig.from_pretrained(model_name)\nmodel = FlaxAutoModelForQuestionAnswering.from_pretrained(model_name, config=config)\npad_on_right = tokenizer.padding_side == \"right\"\n\n# dataset\nqa_dataset = QADataset(train_data, val_data, test_data=test_data)\ntokenized_dataset = qa_dataset.dataset_dict.map(prepare_train_features, batched=True, \n                                                 remove_columns=qa_dataset.dataset_dict[\"train\"].column_names)\ntrain_dataset = tokenized_dataset[\"train\"]\neval_dataset = tokenized_dataset[\"validation\"]\ntest_dataset = tokenized_dataset[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T01:03:26.286757Z","iopub.execute_input":"2023-04-23T01:03:26.287678Z","iopub.status.idle":"2023-04-23T01:03:33.798040Z","shell.execute_reply.started":"2023-04-23T01:03:26.287639Z","shell.execute_reply":"2023-04-23T01:03:33.796892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_labels = 1\nseed = 60\nnum_train_epochs = 50\nlearning_rate = 2e-5\nper_device_batch_size = 1\ntotal_batch_size = per_device_batch_size * jax.local_device_count()\nmodel_name = \"xlm-roberta-base\"\nmax_length = 400\n\n# model elements\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = AutoConfig.from_pretrained(model_name)\nmodel = FlaxAutoModelForQuestionAnswering.from_pretrained(model_name, config=config, from_pt=True)\npad_on_right = tokenizer.padding_side == \"right\"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T02:19:12.472359Z","iopub.execute_input":"2023-04-24T02:19:12.472831Z","iopub.status.idle":"2023-04-24T02:19:20.992089Z","shell.execute_reply.started":"2023-04-24T02:19:12.472793Z","shell.execute_reply":"2023-04-24T02:19:20.991040Z"},"trusted":true},"execution_count":216,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing FlaxXLMRobertaForQuestionAnswering: {('roberta', 'pooler', 'dense', 'bias'), ('lm_head', 'dense', 'kernel'), ('lm_head', 'layer_norm', 'kernel'), ('roberta', 'pooler', 'dense', 'kernel'), ('lm_head', 'bias'), ('lm_head', 'layer_norm', 'bias'), ('lm_head', 'dense', 'bias'), ('lm_head', 'decoder', 'kernel')}\n- This IS expected if you are initializing FlaxXLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxXLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxXLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: {('qa_outputs', 'bias'), ('qa_outputs', 'kernel')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Prepare the input text\ncontext = \"The quick brown fox jumps over the lazy dog.\"\nquestion = \"What does the fox jump over?\"\n\n# Tokenize the input text\ninputs = tokenizer(question, context, truncation=\"only_second\",\n        max_length=max_length,\n        padding=\"max_length\",\n        return_tensors=\"jax\",)\n\n# Get the start and end logits from the model outputs\noutputs = model(**inputs)\n# start_logits, end_logits = outputs.start_logits, outputs.end_logits\nstart_logits = outputs.start_logits[0][len(tokenizer.encode(question))+1:(len(tokenizer.encode(question))+1)+(len(tokenizer.encode(context)))-2]\nend_logits = outputs.end_logits[0][len(tokenizer.encode(question))+1:(len(tokenizer.encode(question))+1)+(len(tokenizer.encode(context)))-2]\n\n# Softmax the logits to get probabilities\nstart_probs = jax.nn.softmax(start_logits, axis=-1)\nend_probs = jax.nn.softmax(end_logits, axis=-1)\n\n# Find the pair with the highest probability\nmax_prob = 0\nbest_pair = None\nfor start_idx in range(start_probs.shape[0]):\n    for end_idx in range(start_idx, end_probs.shape[0]):\n        prob = (start_probs[start_idx] * end_probs[end_idx]).max()\n        if prob > max_prob:\n            max_prob = prob\n            best_pair = (start_idx, end_idx)\n\n# Get the answer text\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][len(tokenizer.encode(question))+1:(len(tokenizer.encode(question))+1)+(len(tokenizer.encode(context)))-2])\nanswer_tokens = tokens[best_pair[0]:best_pair[1]]\nanswer = tokenizer.convert_tokens_to_string(answer_tokens)\nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:21:11.275415Z","iopub.execute_input":"2023-04-24T01:21:11.276583Z","iopub.status.idle":"2023-04-24T01:21:13.922430Z","shell.execute_reply.started":"2023-04-24T01:21:11.276526Z","shell.execute_reply":"2023-04-24T01:21:13.921157Z"},"trusted":true},"execution_count":177,"outputs":[{"name":"stdout","text":"dog\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.decode(inputs[\"input_ids\"][0][len(tokenizer.encode(question))+1:(len(tokenizer.encode(question))+1)+(len(tokenizer.encode(context)))-2])","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:20:09.546817Z","iopub.execute_input":"2023-04-24T01:20:09.547219Z","iopub.status.idle":"2023-04-24T01:20:09.580985Z","shell.execute_reply.started":"2023-04-24T01:20:09.547185Z","shell.execute_reply":"2023-04-24T01:20:09.579663Z"},"trusted":true},"execution_count":176,"outputs":[{"execution_count":176,"output_type":"execute_result","data":{"text/plain":"'The quick brown fox jumps over the lazy dog.'"},"metadata":{}}]},{"cell_type":"code","source":"start_probs","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:21:27.671894Z","iopub.execute_input":"2023-04-24T01:21:27.673080Z","iopub.status.idle":"2023-04-24T01:21:27.681293Z","shell.execute_reply.started":"2023-04-24T01:21:27.673032Z","shell.execute_reply":"2023-04-24T01:21:27.680007Z"},"trusted":true},"execution_count":178,"outputs":[{"execution_count":178,"output_type":"execute_result","data":{"text/plain":"DeviceArray([0.07289212, 0.074141  , 0.06796393, 0.07254633, 0.071644  ,\n             0.07240427, 0.07998803, 0.08557792, 0.08254207, 0.08150709,\n             0.07228388, 0.08729137, 0.0792181 ], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"inputs[\"input_ids\"][0][len(tokenizer.encode(question))+1:-1]","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:01:50.807127Z","iopub.execute_input":"2023-04-24T01:01:50.807613Z","iopub.status.idle":"2023-04-24T01:01:50.820471Z","shell.execute_reply.started":"2023-04-24T01:01:50.807561Z","shell.execute_reply":"2023-04-24T01:01:50.819062Z"},"trusted":true},"execution_count":144,"outputs":[{"execution_count":144,"output_type":"execute_result","data":{"text/plain":"DeviceArray([ 4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n              1012], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"start_logits","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:42:38.199474Z","iopub.execute_input":"2023-04-24T00:42:38.200839Z","iopub.status.idle":"2023-04-24T00:42:38.210209Z","shell.execute_reply.started":"2023-04-24T00:42:38.200766Z","shell.execute_reply":"2023-04-24T00:42:38.208710Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"DeviceArray([[0.6122262 , 0.07552624, 0.09251422, 0.00552287, 0.07077111,\n              0.05825523, 0.06881095, 0.16842231, 0.23597269, 0.19985367,\n              0.18723549, 0.06714658, 0.25579754, 0.15875067, 0.59082323]],            dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(inputs['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:02:15.083844Z","iopub.execute_input":"2023-04-24T01:02:15.084295Z","iopub.status.idle":"2023-04-24T01:02:15.094106Z","shell.execute_reply.started":"2023-04-24T01:02:15.084255Z","shell.execute_reply":"2023-04-24T01:02:15.092845Z"},"trusted":true},"execution_count":145,"outputs":[{"execution_count":145,"output_type":"execute_result","data":{"text/plain":"'[CLS] what does the fox jump over? [SEP] the quick brown fox jumps over the lazy dog. [SEP]'"},"metadata":{}}]},{"cell_type":"code","source":"outputs.start_logits[0][len(tokenizer.encode(question))+1:-1]","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:03:03.633233Z","iopub.execute_input":"2023-04-24T01:03:03.633730Z","iopub.status.idle":"2023-04-24T01:03:03.644034Z","shell.execute_reply.started":"2023-04-24T01:03:03.633689Z","shell.execute_reply":"2023-04-24T01:03:03.643080Z"},"trusted":true},"execution_count":148,"outputs":[{"execution_count":148,"output_type":"execute_result","data":{"text/plain":"DeviceArray([-0.00570387, -0.18271723, -0.50954324,  0.2117034 ,\n             -0.26089227, -0.1121341 , -0.22503148, -0.43832636,\n              0.6577821 ], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(inputs[\"input_ids\"][0][len(tokenizer.encode(question))+1:-1])","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:03:09.714541Z","iopub.execute_input":"2023-04-24T01:03:09.714980Z","iopub.status.idle":"2023-04-24T01:03:09.725862Z","shell.execute_reply.started":"2023-04-24T01:03:09.714944Z","shell.execute_reply":"2023-04-24T01:03:09.724746Z"},"trusted":true},"execution_count":149,"outputs":[{"execution_count":149,"output_type":"execute_result","data":{"text/plain":"'quick brown fox jumps over the lazy dog.'"},"metadata":{}}]},{"cell_type":"code","source":"(start_probs * end_probs)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:52:43.623829Z","iopub.execute_input":"2023-04-24T00:52:43.624244Z","iopub.status.idle":"2023-04-24T00:52:43.632239Z","shell.execute_reply.started":"2023-04-24T00:52:43.624207Z","shell.execute_reply":"2023-04-24T00:52:43.631119Z"},"trusted":true},"execution_count":115,"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"DeviceArray([0.00575675, 0.00560929, 0.0050902 , 0.00606117, 0.00520201,\n             0.00452383, 0.00632051, 0.00569825, 0.00642842, 0.00580962,\n             0.00567145, 0.00713313, 0.00767574], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"end_probs","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:53:19.484183Z","iopub.execute_input":"2023-04-24T00:53:19.484622Z","iopub.status.idle":"2023-04-24T00:53:19.492084Z","shell.execute_reply.started":"2023-04-24T00:53:19.484562Z","shell.execute_reply":"2023-04-24T00:53:19.491044Z"},"trusted":true},"execution_count":117,"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"DeviceArray([0.07897632, 0.07565713, 0.07489557, 0.0835489 , 0.07260911,\n             0.06248023, 0.07901829, 0.06658554, 0.0778806 , 0.07127752,\n             0.07846078, 0.08171634, 0.09689372], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"start_probs","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:53:12.225808Z","iopub.execute_input":"2023-04-24T00:53:12.226246Z","iopub.status.idle":"2023-04-24T00:53:12.236279Z","shell.execute_reply.started":"2023-04-24T00:53:12.226212Z","shell.execute_reply":"2023-04-24T00:53:12.234839Z"},"trusted":true},"execution_count":116,"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"DeviceArray([0.07289211, 0.07414098, 0.06796393, 0.07254633, 0.071644  ,\n             0.07240427, 0.07998799, 0.08557788, 0.08254205, 0.08150706,\n             0.07228386, 0.08729138, 0.0792181 ], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"start_probs[]","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:25:40.138772Z","iopub.execute_input":"2023-04-24T00:25:40.139241Z","iopub.status.idle":"2023-04-24T00:25:40.147384Z","shell.execute_reply.started":"2023-04-24T00:25:40.139204Z","shell.execute_reply":"2023-04-24T00:25:40.146045Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"DeviceArray([[0.05581303, 0.03478736, 0.03400503, 0.0360894 , 0.03413697,\n              0.03402139, 0.03558325, 0.04018251, 0.03448509, 0.05940889,\n              0.0601492 , 0.03516771, 0.03577024, 0.03279004, 0.03500088,\n              0.03456554, 0.03493234, 0.0385912 , 0.04128812, 0.03982344,\n              0.0393241 , 0.03487425, 0.04211481, 0.03821976, 0.05887551]],            dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"# получить start_logits и end_logits\nstart_logits, end_logits = model(**inputs)\n\n# преобразовать выходы в массив numpy\nstart_logits = np.array(start_logits)\nend_logits = np.array(end_logits)\n\n# выбрать наиболее вероятную подстроку из контекста\nstart_idx = np.argmax(start_logits)\nend_idx = np.argmax(end_logits[start_idx:]) + start_idx\nanswer = context[start_idx:end_idx]","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:15:38.130691Z","iopub.execute_input":"2023-04-24T00:15:38.131264Z","iopub.status.idle":"2023-04-24T00:15:39.790845Z","shell.execute_reply.started":"2023-04-24T00:15:38.131183Z","shell.execute_reply":"2023-04-24T00:15:39.789193Z"},"trusted":true},"execution_count":22,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/1316366747.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# выбрать наиболее вероятную подстроку из контекста\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mend_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 0-dimensional, but 1 were indexed"],"ename":"IndexError","evalue":"too many indices for array: array is 0-dimensional, but 1 were indexed","output_type":"error"}]},{"cell_type":"code","source":"end_probs","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:13:59.712882Z","iopub.execute_input":"2023-04-24T00:13:59.713354Z","iopub.status.idle":"2023-04-24T00:13:59.722903Z","shell.execute_reply.started":"2023-04-24T00:13:59.713304Z","shell.execute_reply":"2023-04-24T00:13:59.721290Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"DeviceArray([[0.02696974, 0.04158742, 0.04274609, 0.04604455, 0.04667987,\n              0.0421382 , 0.03589347, 0.03883765, 0.04524194, 0.02491666,\n              0.02451394, 0.04416884, 0.04231253, 0.04188661, 0.04672613,\n              0.04060787, 0.03494312, 0.04419231, 0.03723909, 0.04355604,\n              0.03986315, 0.04388052, 0.04570124, 0.05418945, 0.02516356]],            dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"start_probs","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:12:09.037812Z","iopub.execute_input":"2023-04-24T00:12:09.038376Z","iopub.status.idle":"2023-04-24T00:12:09.048209Z","shell.execute_reply.started":"2023-04-24T00:12:09.038323Z","shell.execute_reply":"2023-04-24T00:12:09.046890Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"DeviceArray([[0.05581303, 0.03478736, 0.03400503, 0.0360894 , 0.03413697,\n              0.03402139, 0.03558325, 0.04018251, 0.03448509, 0.05940889,\n              0.0601492 , 0.03516771, 0.03577024, 0.03279004, 0.03500088,\n              0.03456554, 0.03493234, 0.0385912 , 0.04128812, 0.03982344,\n              0.0393241 , 0.03487425, 0.04211481, 0.03821976, 0.05887551]],            dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"end_idx","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:14:03.934697Z","iopub.execute_input":"2023-04-24T00:14:03.936125Z","iopub.status.idle":"2023-04-24T00:14:03.946313Z","shell.execute_reply.started":"2023-04-24T00:14:03.936072Z","shell.execute_reply":"2023-04-24T00:14:03.944624Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"DeviceArray([[23]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# TPU used try 1","metadata":{}},{"cell_type":"code","source":"import jax\nimport flax\nimport optax\nfrom itertools import chain\n# from tqdm.notebook import tqdm\nfrom typing import Callable\n\nimport jax.numpy as jnp\n\nfrom flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\nfrom flax.training import train_state\nfrom flax import traverse_util\n# from torch.utils.data import DataLoader\n# import datasets","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:47:59.765509Z","iopub.execute_input":"2023-04-24T01:47:59.765983Z","iopub.status.idle":"2023-04-24T01:47:59.774002Z","shell.execute_reply.started":"2023-04-24T01:47:59.765947Z","shell.execute_reply":"2023-04-24T01:47:59.772575Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"print(flax.__version__)\njax.local_devices()","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:48:02.810533Z","iopub.execute_input":"2023-04-24T01:48:02.811054Z","iopub.status.idle":"2023-04-24T01:48:02.821870Z","shell.execute_reply.started":"2023-04-24T01:48:02.811012Z","shell.execute_reply":"2023-04-24T01:48:02.820524Z"},"trusted":true},"execution_count":181,"outputs":[{"name":"stdout","text":"0.6.4\n","output_type":"stream"},{"execution_count":181,"output_type":"execute_result","data":{"text/plain":"[CpuDevice(id=0)]"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\n\n# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n\n# Args\nnum_labels = 1\nseed = 60\nnum_train_epochs = 50\nlearning_rate = 2e-5\nper_device_batch_size = 1\ntotal_batch_size = per_device_batch_size * jax.local_device_count()\nmodel_name = 'xlm-roberta-base'\n# \"distilbert-base-uncased\"\nmax_length = 1440\n\n# model elements\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = AutoConfig.from_pretrained(model_name)\nmodel = FlaxAutoModelForQuestionAnswering.from_pretrained(model_name, config=config)\npad_on_right = tokenizer.padding_side == \"right\"\n\n# dataset\nqa_dataset = QADataset(train_data, val_data, test_data=test_data)\n# test_data=test_data\ntokenized_dataset = qa_dataset.dataset_dict.map(prepare_train_features, batched=True, \n                                                 remove_columns=qa_dataset.dataset_dict[\"train\"].column_names)\ntrain_dataset = tokenized_dataset[\"train\"]\neval_dataset = tokenized_dataset[\"validation\"]\ntest_dataset = tokenized_dataset[\"test\"]\n\n#add args\nnum_train_steps = len(train_dataset) // total_batch_size * num_train_epochs\n\nprint(\"The overall batch size (both for training and eval) is\", total_batch_size)\nprint(\"The number of train steps (all the epochs) is\", num_train_steps)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:51:45.448352Z","iopub.execute_input":"2023-04-24T01:51:45.448885Z","iopub.status.idle":"2023-04-24T01:52:51.280451Z","shell.execute_reply.started":"2023-04-24T01:51:45.448837Z","shell.execute_reply":"2023-04-24T01:52:51.278602Z"},"trusted":true},"execution_count":183,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing FlaxXLMRobertaForQuestionAnswering: {('lm_head', 'layer_norm', 'scale'), ('lm_head', 'dense', 'kernel'), ('lm_head', 'bias'), ('lm_head', 'layer_norm', 'bias'), ('lm_head', 'dense', 'bias')}\n- This IS expected if you are initializing FlaxXLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxXLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxXLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: {('qa_outputs', 'bias'), ('qa_outputs', 'kernel')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 1439/1439 [00:00<00:00, 4218.56it/s]\n100%|██████████| 360/360 [00:00<00:00, 4248.26it/s]\n100%|██████████| 318/318 [00:00<00:00, 4912.03it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"707b7114f3d442d2945ae957f29eae71"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/608547845.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# test_data=test_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m tokenized_dataset = qa_dataset.dataset_dict.map(prepare_train_features, batched=True, \n\u001b[0;32m---> 35\u001b[0;31m                                                  remove_columns=qa_dataset.dataset_dict[\"train\"].column_names)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0meval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 )\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             }\n\u001b[1;32m    460\u001b[0m         )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 )\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             }\n\u001b[1;32m    460\u001b[0m         )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m                 \u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m                 \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m             )\n\u001b[1;32m   1975\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m         }\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2341\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2342\u001b[0m                                 \u001b[0mcheck_same_num_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2343\u001b[0;31m                                 \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2344\u001b[0m                             )\n\u001b[1;32m   2345\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mNumExamplesMismatchError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                 \u001b[0;31m# Check if the function returns updated examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2223\u001b[0m                 \u001b[0mupdate_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                 \u001b[0mvalidate_function_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Nothing to update, let's move on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mvalidate_function_output\u001b[0;34m(processed_inputs, indices)\u001b[0m\n\u001b[1;32m   2202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall_dict_values_are_lists\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2203\u001b[0m                     raise TypeError(\n\u001b[0;32m-> 2204\u001b[0;31m                         \u001b[0;34mf\"Provided `function` which is applied to all elements of table returns a `dict` of types {[type(x) for x in processed_inputs.values()]}. When using `batched=True`, make sure provided `function` returns a `dict` of types like `{allowed_batch_return_types}`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2205\u001b[0m                     )\n\u001b[1;32m   2206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'jaxlib.xla_extension.DeviceArray'>, <class 'jaxlib.xla_extension.DeviceArray'>, <class 'list'>, <class 'list'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>)`."],"ename":"TypeError","evalue":"Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'jaxlib.xla_extension.DeviceArray'>, <class 'jaxlib.xla_extension.DeviceArray'>, <class 'list'>, <class 'list'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>)`.","output_type":"error"}]},{"cell_type":"code","source":"tokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-04-24T02:18:09.736722Z","iopub.execute_input":"2023-04-24T02:18:09.737220Z","iopub.status.idle":"2023-04-24T02:18:09.781963Z","shell.execute_reply.started":"2023-04-24T02:18:09.737183Z","shell.execute_reply":"2023-04-24T02:18:09.780053Z"},"trusted":true},"execution_count":214,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3954752550.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"],"ename":"NameError","evalue":"name 'tokenized_dataset' is not defined","output_type":"error"}]},{"cell_type":"code","source":"tokenized_dataset['train']['start_positions'][0]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T11:18:04.112018Z","iopub.execute_input":"2023-04-23T11:18:04.112423Z","iopub.status.idle":"2023-04-23T11:18:04.120455Z","shell.execute_reply.started":"2023-04-23T11:18:04.112392Z","shell.execute_reply":"2023-04-23T11:18:04.119361Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"1049"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenized_dataset['train']['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2023-04-23T13:47:41.870816Z","iopub.execute_input":"2023-04-23T13:47:41.871707Z","iopub.status.idle":"2023-04-23T13:47:42.707613Z","shell.execute_reply.started":"2023-04-23T13:47:41.871666Z","shell.execute_reply":"2023-04-23T13:47:42.706424Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"[0, 112203, 133744, 58514, 59, 2, 2, 208831, 106, 718, 22263, 1269, 46, 75420, 105, 183, 72392, 476, 424, 9888, 380, 162655, 35, 234665, 244, 225191, 244, 40188, 559, 49, 20960, 135, 128815, 130, 117551, 16753, 16286, 183, 496, 1662, 5357, 476, 427, 18949, 5188, 123954, 244, 35, 40238, 85212, 591, 89, 72499, 124234, 424, 142972, 43475, 225191, 244, 40188, 559, 65153, 1196, 49, 166153, 88952, 380, 162655, 35, 234665, 244, 225191, 244, 40188, 559, 518, 121918, 149514, 40378, 2297, 518, 66917, 96515, 35, 147055, 85374, 33473, 543, 963, 447, 12758, 27183, 119807, 59, 29, 255, 64743, 56600, 26124, 312, 167137, 591, 89, 72499, 124234, 424, 142972, 43475, 35, 15, 3107, 16, 25440, 129, 591, 415, 1882, 13345, 56600, 49, 20960, 135, 128815, 130, 117551, 16753, 16286, 183, 534, 8318, 7360, 476, 361, 12183, 5188, 29514, 1993, 119807, 59, 29, 255, 64743, 56600, 26124, 312, 167137, 591, 89, 72499, 124234, 424, 142972, 43475, 35, 15, 3107, 16, 25440, 129, 591, 415, 1882, 13345, 56600, 49, 104939, 149514, 40378, 2297, 518, 66917, 236921, 85374, 85374, 178301, 16753, 16286, 35, 147055, 85374, 33473, 953, 140782, 66917, 173604, 29, 19149, 49, 6, 155962, 2172, 151075, 103, 6171, 387, 140782, 66917, 133744, 58514, 59, 21280, 183, 79225, 58514, 59, 952, 1089, 225697, 2715, 133744, 58514, 59, 46983, 49, 199826, 158108, 18102, 29, 36858, 2192, 117318, 419, 25862, 5560, 698, 128898, 1584, 154693, 518, 145562, 66917, 133744, 58514, 59, 672, 153009, 2715, 72767, 17845, 4301, 1078, 117199, 59156, 142806, 59, 130914, 12358, 5509, 213857, 135, 226551, 59156, 142806, 59, 78835, 21116, 135, 3167, 1551, 12657, 14758, 84965, 34710, 31420, 24998, 12635, 16229, 58263, 96480, 10190, 328, 5573, 56110, 8871, 49517, 22043, 2678, 110144, 363, 108428, 96480, 10190, 328, 5573, 31420, 24998, 12635, 16229, 135, 3167, 1551, 12657, 14758, 19771, 36627, 18454, 9694, 387, 804, 164532, 757, 2588, 10700, 10057, 4633, 1382, 66035, 4648, 12338, 6746, 34479, 5729, 650, 1112, 49150, 13556, 135401, 179973, 181155, 18197, 78745, 212, 52181, 12635, 25862, 5098, 3742, 160775, 84701, 161025, 28568, 3559, 672, 153009, 2715, 27617, 3858, 76159, 3580, 2632, 108428, 96480, 10190, 328, 5573, 1537, 52619, 3413, 4836, 189982, 6746, 6, 67090, 1532, 1197, 1549, 120377, 104101, 2052, 15279, 17845, 4301, 1078, 117199, 59156, 142806, 59, 130914, 12358, 5509, 213857, 135, 226551, 59156, 142806, 59, 78835, 21116, 135, 3167, 1551, 12657, 14758, 84965, 34710, 31420, 24998, 12635, 16229, 58263, 96480, 10190, 328, 5573, 102021, 49, 199826, 158108, 18102, 49, 22326, 66917, 133744, 58514, 59, 220893, 989, 48895, 2151, 63088, 91188, 135, 183, 7588, 7613, 27617, 1086, 407, 68628, 52125, 20765, 1874, 11875, 11125, 652, 49817, 8941, 50316, 4485, 928, 135, 901, 147186, 4256, 135, 117318, 419, 129, 84609, 6934, 559, 3150, 3505, 165200, 59, 49, 155980, 51384, 103, 49, 5138, 25516, 6103, 121690, 84708, 130, 2101, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.decode(tokenized_dataset['train']['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2023-04-23T11:18:20.258129Z","iopub.execute_input":"2023-04-23T11:18:20.258499Z","iopub.status.idle":"2023-04-23T11:18:21.105816Z","shell.execute_reply.started":"2023-04-23T11:18:20.258472Z","shell.execute_reply":"2023-04-23T11:18:21.104717Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"'<s> обеспечение исполнения контракта</s></s> Приложение 1 к заявке – заказу от 2022г изненно необходимых и важнейших лекарственных препаратов в соответствии с постановлением Правительства Российской Федерации от 30112015г 1289 Об ограничениях и условиях допуска происходящих из иностранных государств лекарственных препаратов включенных в перечень жизненно необходимых и важнейших лекарственных препаратов для целей осуществления закупок для обеспечения государственных и муниципальных нужд Нет 1810 Установление запрета на допуск товаров легкой промышленности происходящих из иностранных государств и (или) услуг по прокату таких товаров в соответствии с постановлением Правительства Российской Федерации от 11082014г 791 Об установлении запрета на допуск товаров легкой промышленности происходящих из иностранных государств и (или) услуг по прокату таких товаров в целях осуществления закупок для обеспечения федеральных нужд нужд субъектов Российской Федерации и муниципальных нужд Нет 19 Размер обеспечения заявки на участие в электронном аукционе нет 20 Размер обеспечения исполнения контракта 5% от цены контракта 21 Обеспечение исполнения контракта путем внесения денежных средств на указанный заказчиком счет Реквизиты счета для предоставления обеспечения исполнения контракта Наименование учреждения Муниципальное общеобразовательное бюджетное учреждение средняя общеобразовательная школа сТемясово муниципального района Баймакский район Республики Башкортостан Юридический адрес 453663 Республика Башкортостан Баймакский район сТемясово улСоветская 20 ИНН 0254005845 КПП 025401001 Рс 40102810045370000067 Казначейский счет 03234643806060000100 Наименование банка Отделение НБ Республика Башкортостан БИК 018073401 лс 20103020420 Получатель Муниципальное общеобразовательное бюджетное учреждение средняя общеобразовательная школа сТемясово муниципального района Баймакский район Республики Башкортостан Факт внесения денежных средств в качестве обеспечения исполнения контракта подтверждается платежным поручением с отметкой банка об оплате Денежные средства возвращаются поставщику с По согласованию с заказчиком поставка дров может быть осуществлена в полном объеме в более ранние сроки Кубм 500</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"},"metadata":{}}]},{"cell_type":"code","source":"# model = FlaxAutoModelForQuestionAnswering.from_pretrained(model_name, config=config)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T02:18:23.850699Z","iopub.execute_input":"2023-04-24T02:18:23.851251Z","iopub.status.idle":"2023-04-24T02:18:34.665934Z","shell.execute_reply.started":"2023-04-24T02:18:23.851198Z","shell.execute_reply":"2023-04-24T02:18:34.664693Z"},"trusted":true},"execution_count":215,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing FlaxXLMRobertaForQuestionAnswering: {('lm_head', 'layer_norm', 'scale'), ('lm_head', 'dense', 'kernel'), ('lm_head', 'bias'), ('lm_head', 'layer_norm', 'bias'), ('lm_head', 'dense', 'bias')}\n- This IS expected if you are initializing FlaxXLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxXLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxXLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: {('qa_outputs', 'bias'), ('qa_outputs', 'kernel')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs['input_ids'][0]","metadata":{"execution":{"iopub.status.busy":"2023-04-24T02:10:26.247514Z","iopub.execute_input":"2023-04-24T02:10:26.248831Z","iopub.status.idle":"2023-04-24T02:10:26.261333Z","shell.execute_reply.started":"2023-04-24T02:10:26.248763Z","shell.execute_reply":"2023-04-24T02:10:26.259867Z"},"trusted":true},"execution_count":196,"outputs":[{"execution_count":196,"output_type":"execute_result","data":{"text/plain":"DeviceArray([     0,   4865,  14602,     70,      6, 147797,  88203,\n                645,     32,      2,      2,    581,  63773, 119455,\n                  6, 147797,  88203,      7,    645,     70,     21,\n               3285,  10269,      5,      2,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1,      1,      1,      1,      1,      1,      1,\n                  1], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"jnp.where(inputs['input_ids'][0] == 2)[0][-2]","metadata":{"execution":{"iopub.status.busy":"2023-04-24T02:11:58.327302Z","iopub.execute_input":"2023-04-24T02:11:58.327882Z","iopub.status.idle":"2023-04-24T02:11:58.346247Z","shell.execute_reply.started":"2023-04-24T02:11:58.327827Z","shell.execute_reply":"2023-04-24T02:11:58.345028Z"},"trusted":true},"execution_count":202,"outputs":[{"execution_count":202,"output_type":"execute_result","data":{"text/plain":"DeviceArray(10, dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenize the input text\ninputs = tokenizer(question, context, truncation=\"only_second\",\n        max_length=max_length,\n        padding=\"max_length\",\n        return_tensors=\"jax\",)\n\n# Get the start and end logits from the model outputs\noutputs = model(**inputs)\n# start_logits, end_logits = outputs.start_logits, outputs.end_logits\n# start_logits = outputs.start_logits[0][len(tokenizer.encode(question))+1:(len(tokenizer.encode(question))+1)+(len(tokenizer.encode(context)))-2]\n# end_logits = outputs.end_logits[0][len(tokenizer.encode(question))+1:(len(tokenizer.encode(question))+1)+(len(tokenizer.encode(context)))-2]\n\nmask_indices = np.where(inputs['input_ids'][0] == 2)[0]\nlast_mask_index = mask_indices[-1]-1\nsecond_last_mask_index = mask_indices[-2]+1\nstart_logits = outputs.start_logits[0][second_last_mask_index:last_mask_index]\nend_logits = outputs.end_logits[0][second_last_mask_index:last_mask_index]\n\n# Softmax the logits to get probabilities\nstart_probs = jax.nn.softmax(start_logits, axis=-1)\nend_probs = jax.nn.softmax(end_logits, axis=-1)\n\n# Find the pair with the highest probability\nmax_prob = 0\nbest_pair = None\nfor start_idx in range(start_probs.shape[0]):\n    for end_idx in range(start_idx, end_probs.shape[0]):\n        prob = (start_probs[start_idx] * end_probs[end_idx]).max()\n        if prob > max_prob:\n            max_prob = prob\n            best_pair = (start_idx, end_idx)\n\n# Get the answer text\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][second_last_mask_index:last_mask_index])\nanswer_tokens = tokens[best_pair[0]:best_pair[1]+1]\nanswer = tokenizer.convert_tokens_to_string(answer_tokens)\nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T02:23:06.962159Z","iopub.execute_input":"2023-04-24T02:23:06.962664Z","iopub.status.idle":"2023-04-24T02:23:09.703801Z","shell.execute_reply.started":"2023-04-24T02:23:06.962621Z","shell.execute_reply":"2023-04-24T02:23:09.702733Z"},"trusted":true},"execution_count":230,"outputs":[{"name":"stdout","text":"dog\n","output_type":"stream"}]},{"cell_type":"code","source":"# from tqdm import tqdm\n\nclass RMSE(datasets.Metric):\n    def _info(self):\n        return datasets.MetricInfo(\n            description=\"Calculates Root Mean Squared Error (RMSE) metric.\",\n            citation=\"TODO: _CITATION\",\n            inputs_description=\"_KWARGS_DESCRIPTION\",\n            features=datasets.Features({\n                'predictions': datasets.Value('float32'),\n                'references': datasets.Value('float32'),\n            }),\n            codebase_urls=[],\n            reference_urls=[],\n            format='numpy'\n        )\n\n    def _compute(self, predictions, references):\n        rmse = np.sqrt(np.sum(np.square(predictions - references)) / predictions.shape[0])\n        return {\"RMSE\": rmse}\n\nclass TrainState(train_state.TrainState):\n    logits_function: Callable = flax.struct.field(pytree_node=False)\n    loss_function: Callable = flax.struct.field(pytree_node=False)\n\nlearning_rate_function = optax.cosine_onecycle_schedule(transition_steps=num_train_steps, \n                                                        peak_value=learning_rate, pct_start=0.1, \n                                                       )\ndef decay_mask_fn(params):\n    flat_params = traverse_util.flatten_dict(params)\n    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n    return traverse_util.unflatten_dict(flat_mask)\n\n# def adamw(weight_decay):\n#     return optax.adamw(learning_rate=learning_rate_function, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay, mask=decay_mask_fn)\n\n# adamw = adamw(1e-2)\n\ndef adamw(weight_decay, schedule_fn):\n    return optax.chain(\n        optax.scale_by_adam(b1=0.9, b2=0.999, eps=1e-6),\n        optax.scale_by_schedule(schedule_fn),\n        optax.scale(-1.0),\n        optax.additive_weight_decay(weight_decay, mask=decay_mask_fn)\n    )\n\n# Пример функции для создания расписания скорости обучения\ndef learning_rate_schedule(max_lr, warmup_steps, total_steps):\n    step_fn = lambda step: np.minimum((step + 1) / warmup_steps, 1.0)\n    lr_fn = lambda step: max_lr * step_fn(step) * (total_steps - step) / np.maximum(total_steps - warmup_steps, 1.0)\n    return lr_fn\n\n# Пример использования нового оптимизатора с расписанием скорости обучения\nlearning_rate_fn = learning_rate_schedule(max_lr=1e-3, warmup_steps=(num_train_steps/10), total_steps=num_train_steps)\nadamw = adamw(weight_decay=1e-2, schedule_fn=learning_rate_fn)\n\n# @jax.jit\n# def loss_function(logits, labels):\n#     return jnp.mean((logits[..., 0] - labels) ** 2)\n\n@jax.jit\ndef loss_function(predicted_positions, positions):\n#     logits = logits[..., 0] # получение массива start_logits\n#     predicted_positions = jnp.argmax(logits, axis=-1) # получение индекса с наибольшим значением в каждом батче\n    loss = jnp.mean((predicted_positions - positions) ** 2) # вычисление средней ошибки\n    return loss\n\n\n# @jax.jit    \n# def eval_function(logits):\n#     return logits[..., 0]\n\n@jax.jit    \ndef eval_function(logits):\n#     logits = logits[..., 0]\n#     l_index = jnp.argmax(logits, axis=-1)\n#     start_values = jnp.take_along_axis(logits, axis=-1)\n    return logits\n\n@jax.jit\ndef get_best_indexes(batch, outputs):\n    \n    mask_indices = np.where(batch['input_ids'][0] == 2)[0] # 2 - разделитель </sep>\n    last_mask_index = mask_indices[-1]-1\n    second_last_mask_index = mask_indices[-2]+1\n    start_logits = outputs.start_logits[0][second_last_mask_index:last_mask_index]\n    end_logits = outputs.end_logits[0][second_last_mask_index:last_mask_index]\n\n    # Softmax\n    start_probs = jax.nn.softmax(start_logits, axis=-1)\n    end_probs = jax.nn.softmax(end_logits, axis=-1)\n\n    # Поиск индексов с наибольшим произведением вероятностей\n    max_prob = 0\n    best_pair = None\n    for start_idx in range(start_probs.shape[0]):\n        for end_idx in range(start_idx, end_probs.shape[0]):\n            prob = (start_probs[start_idx] * end_probs[end_idx]).max()\n            if prob > max_prob:\n                max_prob = prob\n                best_pair = (start_idx, end_idx)\n\n    return best_pair[0], best_pair[1]\n\nstate = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw,\n                          logits_function=eval_function, loss_function=loss_function,\n                         )\n\ndef train_step(state, batch, dropout_rng):\n    start_positions = batch.pop(\"start_positions\")\n    end_positions = batch.pop(\"end_positions\")\n    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n\n    def loss_fn(params):\n        outputs = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True, return_dict=True)\n        start_pred, end_pred = get_best_indexes(batch, outputs)\n        start_loss = state.loss_function(start_pred, start_positions)\n        end_loss = state.loss_function(end_pred, end_positions)\n        loss = (start_loss + end_loss) / 2.0\n        return loss\n\n    grad_function = jax.value_and_grad(loss_fn)\n    loss, grad = grad_function(state.params)\n    grad = jax.lax.pmean(grad, \"batch\")\n    new_state = state.apply_gradients(grads=grad)\n    new_params = new_state.params\n    new_params = jax.tree_map(lambda p, g: p - learning_rate_fn(state.step) * g, new_state.params, grad)\n    new_state = new_state.replace(params=new_params)\n    metrics = {\"loss\": loss, \"learning_rate\": learning_rate_fn(state.step)}\n    print(metrics)\n    return new_state, metrics, new_dropout_rng\n\nparallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n\ndef eval_step(state, batch):\n    outputs = state.apply_fn(**batch, params=state.params, train=False)\n#     start_logits = outputs.start_logits\n#     end_logits = outputs.end_logits\n#     start_logits = jax.lax.pmean(state.logits_function(start_logits), \"batch\")\n#     end_logits = jax.lax.pmean(state.logits_function(end_logits), \"batch\")\n    start_pred, end_pred = get_best_indexes(batch, outputs)\n    return state.logits_function(start_pred), state.logits_function(end_pred)\n\nparallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n\ndef train_data_loader(rng, dataset, batch_size):\n    steps_per_epoch = len(dataset) // batch_size\n    perms = jax.random.permutation(rng, len(dataset))\n    perms = perms[: steps_per_epoch * batch_size]\n    perms = perms.reshape((steps_per_epoch, batch_size))\n\n    for perm in perms:\n        batch = dataset[perm]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch\n        \ndef eval_data_loader(dataset, batch_size):\n    for i in range(len(dataset) // batch_size):\n        batch = dataset[i * batch_size : (i + 1) * batch_size]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch\n        \nstate = flax.jax_utils.replicate(state)\n\nrng = jax.random.PRNGKey(seed)\ndropout_rngs = jax.random.split(rng, jax.local_device_count())\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T14:54:53.622367Z","iopub.execute_input":"2023-04-23T14:54:53.622854Z","iopub.status.idle":"2023-04-23T14:54:54.452390Z","shell.execute_reply.started":"2023-04-23T14:54:53.622815Z","shell.execute_reply":"2023-04-23T14:54:54.451035Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nmetric_start = RMSE()\nmetric_end = RMSE()\n    \nfor i, epoch in enumerate(tqdm(range(1, num_train_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n    rng, input_rng = jax.random.split(rng)\n    \n    metric_start = RMSE()\n    metric_end = RMSE()\n    \n    for batch in train_data_loader(input_rng, train_dataset, total_batch_size):\n        state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n\n    # evaluate\n    for batch in eval_data_loader(eval_dataset, total_batch_size):\n        start_positions = batch[\"start_positions\"]\n        end_positions = batch[\"end_positions\"]\n        inputs = {k: v for k, v in batch.items() if k not in [\"start_positions\", \"end_positions\"]}\n        start_logits, end_logits = parallel_eval_step(state, inputs)\n        predictions_start = start_logits\n        predictions_end = end_logits\n        references_start = start_positions\n        references_end = end_positions\n        print(predictions_start)\n        print(references_start)\n        metric_start.add_batch(predictions=chain(*predictions_start), references=chain(*references_start))\n        metric_end.add_batch(predictions=chain(*predictions_end), references=chain(*references_end))\n\n    start_rmse = round(metric_start.compute()['RMSE'], 3)\n    end_rmse = round(metric_end.compute()['RMSE'], 3)\n\n    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n    metric_name = \"RMSE\"\n    print(f\"{i+1}/{num_train_epochs} | Train loss: {loss} | Eval {metric_name}: start={start_rmse}, end={end_rmse}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T14:54:56.927226Z","iopub.execute_input":"2023-04-23T14:54:56.928059Z","iopub.status.idle":"2023-04-23T15:38:58.118449Z","shell.execute_reply.started":"2023-04-23T14:54:56.928017Z","shell.execute_reply":"2023-04-23T15:38:58.116912Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stderr","text":"Epoch ...:   2%|▏         | 1/50 [01:47<1:27:27, 107.10s/it]","output_type":"stream"},{"name":"stdout","text":"1/50 | Train loss: 464000.0 | Eval RMSE: start=971.408, end=985.404\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:   4%|▍         | 2/50 [02:39<59:48, 74.76s/it]   ","output_type":"stream"},{"name":"stdout","text":"2/50 | Train loss: 138866.5 | Eval RMSE: start=969.896, end=985.383\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:   6%|▌         | 3/50 [03:29<49:58, 63.79s/it]","output_type":"stream"},{"name":"stdout","text":"3/50 | Train loss: 508565.0 | Eval RMSE: start=969.372, end=986.188\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:   8%|▊         | 4/50 [04:22<45:30, 59.36s/it]","output_type":"stream"},{"name":"stdout","text":"4/50 | Train loss: 1037620.0 | Eval RMSE: start=969.914, end=985.99\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  10%|█         | 5/50 [05:13<42:10, 56.23s/it]","output_type":"stream"},{"name":"stdout","text":"5/50 | Train loss: 375266.0 | Eval RMSE: start=969.516, end=985.925\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  12%|█▏        | 6/50 [06:04<40:03, 54.62s/it]","output_type":"stream"},{"name":"stdout","text":"6/50 | Train loss: 1822509.0 | Eval RMSE: start=971.707, end=984.65\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  14%|█▍        | 7/50 [06:55<38:12, 53.31s/it]","output_type":"stream"},{"name":"stdout","text":"7/50 | Train loss: 710720.0 | Eval RMSE: start=970.375, end=985.708\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  16%|█▌        | 8/50 [07:45<36:42, 52.45s/it]","output_type":"stream"},{"name":"stdout","text":"8/50 | Train loss: 569413.0 | Eval RMSE: start=971.704, end=985.22\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  18%|█▊        | 9/50 [08:36<35:26, 51.87s/it]","output_type":"stream"},{"name":"stdout","text":"9/50 | Train loss: 747185.0 | Eval RMSE: start=969.941, end=985.253\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  20%|██        | 10/50 [09:28<34:35, 51.88s/it]","output_type":"stream"},{"name":"stdout","text":"10/50 | Train loss: 240985.0 | Eval RMSE: start=967.777, end=986.525\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  22%|██▏       | 11/50 [10:21<33:52, 52.13s/it]","output_type":"stream"},{"name":"stdout","text":"11/50 | Train loss: 1750338.0 | Eval RMSE: start=971.445, end=985.627\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  24%|██▍       | 12/50 [11:13<33:08, 52.32s/it]","output_type":"stream"},{"name":"stdout","text":"12/50 | Train loss: 1838745.0 | Eval RMSE: start=971.451, end=986.903\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  26%|██▌       | 13/50 [12:07<32:27, 52.65s/it]","output_type":"stream"},{"name":"stdout","text":"13/50 | Train loss: 561980.5 | Eval RMSE: start=969.448, end=986.202\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  28%|██▊       | 14/50 [12:59<31:33, 52.58s/it]","output_type":"stream"},{"name":"stdout","text":"14/50 | Train loss: 1819810.0 | Eval RMSE: start=970.375, end=985.727\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  30%|███       | 15/50 [13:52<30:38, 52.52s/it]","output_type":"stream"},{"name":"stdout","text":"15/50 | Train loss: 883876.5 | Eval RMSE: start=971.527, end=986.107\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  32%|███▏      | 16/50 [14:44<29:45, 52.51s/it]","output_type":"stream"},{"name":"stdout","text":"16/50 | Train loss: 1679625.0 | Eval RMSE: start=970.664, end=985.031\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  34%|███▍      | 17/50 [15:37<28:52, 52.49s/it]","output_type":"stream"},{"name":"stdout","text":"17/50 | Train loss: 146450.0 | Eval RMSE: start=970.368, end=986.359\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  36%|███▌      | 18/50 [16:30<28:07, 52.74s/it]","output_type":"stream"},{"name":"stdout","text":"18/50 | Train loss: 1731865.0 | Eval RMSE: start=971.549, end=985.335\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  38%|███▊      | 19/50 [17:21<27:00, 52.27s/it]","output_type":"stream"},{"name":"stdout","text":"19/50 | Train loss: 1560005.0 | Eval RMSE: start=969.484, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  40%|████      | 20/50 [18:11<25:51, 51.73s/it]","output_type":"stream"},{"name":"stdout","text":"20/50 | Train loss: 384840.0 | Eval RMSE: start=970.522, end=986.185\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  42%|████▏     | 21/50 [19:03<24:55, 51.58s/it]","output_type":"stream"},{"name":"stdout","text":"21/50 | Train loss: 578148.5 | Eval RMSE: start=970.645, end=985.215\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  44%|████▍     | 22/50 [19:53<23:57, 51.33s/it]","output_type":"stream"},{"name":"stdout","text":"22/50 | Train loss: 232082.5 | Eval RMSE: start=971.298, end=984.732\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  46%|████▌     | 23/50 [20:44<23:03, 51.25s/it]","output_type":"stream"},{"name":"stdout","text":"23/50 | Train loss: 1451434.0 | Eval RMSE: start=971.44, end=985.794\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  48%|████▊     | 24/50 [21:38<22:31, 51.97s/it]","output_type":"stream"},{"name":"stdout","text":"24/50 | Train loss: 536485.0 | Eval RMSE: start=969.304, end=985.701\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  50%|█████     | 25/50 [22:30<21:41, 52.07s/it]","output_type":"stream"},{"name":"stdout","text":"25/50 | Train loss: 333230.5 | Eval RMSE: start=968.668, end=986.13\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  52%|█████▏    | 26/50 [23:22<20:44, 51.84s/it]","output_type":"stream"},{"name":"stdout","text":"26/50 | Train loss: 616274.0 | Eval RMSE: start=970.392, end=985.929\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  54%|█████▍    | 27/50 [24:12<19:44, 51.51s/it]","output_type":"stream"},{"name":"stdout","text":"27/50 | Train loss: 726608.5 | Eval RMSE: start=971.632, end=985.424\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  56%|█████▌    | 28/50 [25:05<18:57, 51.71s/it]","output_type":"stream"},{"name":"stdout","text":"28/50 | Train loss: 399325.0 | Eval RMSE: start=971.41, end=985.195\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  58%|█████▊    | 29/50 [25:55<17:55, 51.22s/it]","output_type":"stream"},{"name":"stdout","text":"29/50 | Train loss: 1734498.0 | Eval RMSE: start=971.577, end=985.804\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  60%|██████    | 30/50 [26:47<17:09, 51.47s/it]","output_type":"stream"},{"name":"stdout","text":"30/50 | Train loss: 1697813.0 | Eval RMSE: start=971.447, end=985.03\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  62%|██████▏   | 31/50 [27:39<16:23, 51.79s/it]","output_type":"stream"},{"name":"stdout","text":"31/50 | Train loss: 1962850.0 | Eval RMSE: start=971.435, end=984.748\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  64%|██████▍   | 32/50 [28:31<15:34, 51.89s/it]","output_type":"stream"},{"name":"stdout","text":"32/50 | Train loss: 1763588.0 | Eval RMSE: start=970.248, end=985.0\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  66%|██████▌   | 33/50 [29:22<14:36, 51.53s/it]","output_type":"stream"},{"name":"stdout","text":"33/50 | Train loss: 1897562.5 | Eval RMSE: start=970.361, end=986.487\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  68%|██████▊   | 34/50 [30:14<13:45, 51.57s/it]","output_type":"stream"},{"name":"stdout","text":"34/50 | Train loss: 1671858.0 | Eval RMSE: start=971.579, end=986.943\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  70%|███████   | 35/50 [31:05<12:53, 51.58s/it]","output_type":"stream"},{"name":"stdout","text":"35/50 | Train loss: 930102.5 | Eval RMSE: start=971.539, end=985.637\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  72%|███████▏  | 36/50 [31:58<12:04, 51.74s/it]","output_type":"stream"},{"name":"stdout","text":"36/50 | Train loss: 1133990.5 | Eval RMSE: start=969.183, end=986.17\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  74%|███████▍  | 37/50 [32:48<11:08, 51.46s/it]","output_type":"stream"},{"name":"stdout","text":"37/50 | Train loss: 1729229.0 | Eval RMSE: start=971.417, end=984.924\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  76%|███████▌  | 38/50 [33:41<10:20, 51.72s/it]","output_type":"stream"},{"name":"stdout","text":"38/50 | Train loss: 685916.5 | Eval RMSE: start=970.323, end=984.468\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  78%|███████▊  | 39/50 [34:32<09:29, 51.73s/it]","output_type":"stream"},{"name":"stdout","text":"39/50 | Train loss: 35345.0 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  80%|████████  | 40/50 [35:23<08:35, 51.52s/it]","output_type":"stream"},{"name":"stdout","text":"40/50 | Train loss: 1981238.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  82%|████████▏ | 41/50 [36:15<07:43, 51.55s/it]","output_type":"stream"},{"name":"stdout","text":"41/50 | Train loss: 1763593.0 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  84%|████████▍ | 42/50 [37:06<06:51, 51.44s/it]","output_type":"stream"},{"name":"stdout","text":"42/50 | Train loss: 492554.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  86%|████████▌ | 43/50 [37:57<05:58, 51.25s/it]","output_type":"stream"},{"name":"stdout","text":"43/50 | Train loss: 1771565.0 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  88%|████████▊ | 44/50 [38:47<05:05, 50.97s/it]","output_type":"stream"},{"name":"stdout","text":"44/50 | Train loss: 1596444.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  90%|█████████ | 45/50 [39:39<04:15, 51.08s/it]","output_type":"stream"},{"name":"stdout","text":"45/50 | Train loss: 1916896.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  92%|█████████▏| 46/50 [40:30<03:24, 51.21s/it]","output_type":"stream"},{"name":"stdout","text":"46/50 | Train loss: 143666.0 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  94%|█████████▍| 47/50 [41:22<02:34, 51.45s/it]","output_type":"stream"},{"name":"stdout","text":"47/50 | Train loss: 1032362.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  96%|█████████▌| 48/50 [42:16<01:44, 52.04s/it]","output_type":"stream"},{"name":"stdout","text":"48/50 | Train loss: 618842.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...:  98%|█████████▊| 49/50 [43:08<00:52, 52.21s/it]","output_type":"stream"},{"name":"stdout","text":"49/50 | Train loss: 1670576.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"Epoch ...: 100%|██████████| 50/50 [44:01<00:00, 52.82s/it]","output_type":"stream"},{"name":"stdout","text":"50/50 | Train loss: 1581336.5 | Eval RMSE: start=971.401, end=984.706\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"question, text = \"обеспечение гарантийных обязательств\", \"Приложение 1 к заявке – заказу от 2022г изненно необходимых и важнейших лекарственных препаратов в соответствии с постановлением Правительства Российской Федерации от 30112015г 1289 Об ограничениях и условиях допуска происходящих из иностранных государств лекарственных препаратов включенных в перечень жизненно необходимых и важнейших лекарственных препаратов для целей осуществления закупок для обеспечения государственных и муниципальных нужд Нет 1810 Установление запрета на допуск товаров легкой промышленности происходящих из иностранных государств и (или) услуг по прокату таких товаров в соответствии с постановлением Правительства Российской Федерации от 11082014г 791 Об установлении запрета на допуск товаров легкой промышленности происходящих из иностранных государств и (или) услуг по прокату таких товаров в целях осуществления закупок для обеспечения федеральных нужд нужд субъектов Российской Федерации и муниципальных нужд Нет 19 Размер обеспечения заявки на участие в электронном аукционе нет 20 Размер обеспечения исполнения контракта 5% от цены контракта 21 Обеспечение исполнения контракта путем внесения денежных средств на указанный заказчиком счет Реквизиты счета для предоставления обеспечения исполнения контракта Наименование учреждения Муниципальное общеобразовательное бюджетное учреждение средняя общеобразовательная школа сТемясово муниципального района Баймакский район Республики Башкортостан Юридический адрес 453663 Республика Башкортостан Баймакский район сТемясово улСоветская 20 ИНН 0254005845 КПП 025401001 Рс 40102810045370000067 Казначейский счет 03234643806060000100 Наименование банка Отделение НБ Республика Башкортостан БИК 018073401 лс 20103020420 Получатель Муниципальное общеобразовательное бюджетное учреждение средняя общеобразовательная школа сТемясово муниципального района Баймакский район Республики Башкортостан Факт внесения денежных средств в качестве обеспечения исполнения контракта подтверждается платежным поручением с отметкой банка об оплате Денежные средства возвращаются поставщику с По согласованию с заказчиком поставка дров может быть осуществлена в полном объеме в более ранние сроки Кубм 500\"\ninputs = tokenizer(question, text, \n                   truncation=\"only_second\",\n                   max_length=max_length,\n                   stride=150,\n                   return_overflowing_tokens=True,\n                   return_offsets_mapping=True,\n                   padding=\"max_length\",\n                   return_tensors=\"jax\",)\n\noutputs = state.apply_fn(**inputs)\nstart_scores = outputs.start_logits\nend_scores = outputs.end_logits\nstart_scores","metadata":{"execution":{"iopub.status.busy":"2023-04-23T23:59:47.501113Z","iopub.execute_input":"2023-04-23T23:59:47.501830Z","iopub.status.idle":"2023-04-23T23:59:47.595007Z","shell.execute_reply.started":"2023-04-23T23:59:47.501770Z","shell.execute_reply":"2023-04-23T23:59:47.592886Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3097171052.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"обеспечение гарантийных обязательств\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Приложение 1 к заявке – заказу от 2022г изненно необходимых и важнейших лекарственных препаратов в соответствии с постановлением Правительства Российской Федерации от 30112015г 1289 Об ограничениях и условиях допуска происходящих из иностранных государств лекарственных препаратов включенных в перечень жизненно необходимых и важнейших лекарственных препаратов для целей осуществления закупок для обеспечения государственных и муниципальных нужд Нет 1810 Установление запрета на допуск товаров легкой промышленности происходящих из иностранных государств и (или) услуг по прокату таких товаров в соответствии с постановлением Правительства Российской Федерации от 11082014г 791 Об установлении запрета на допуск товаров легкой промышленности происходящих из иностранных государств и (или) услуг по прокату таких товаров в целях осуществления закупок для обеспечения федеральных нужд нужд субъектов Российской Федерации и муниципальных нужд Нет 19 Размер обеспечения заявки на участие в электронном аукционе нет 20 Размер обеспечения исполнения контракта 5% от цены контракта 21 Обеспечение исполнения контракта путем внесения денежных средств на указанный заказчиком счет Реквизиты счета для предоставления обеспечения исполнения контракта Наименование учреждения Муниципальное общеобразовательное бюджетное учреждение средняя общеобразовательная школа сТемясово муниципального района Баймакский район Республики Башкортостан Юридический адрес 453663 Республика Башкортостан Баймакский район сТемясово улСоветская 20 ИНН 0254005845 КПП 025401001 Рс 40102810045370000067 Казначейский счет 03234643806060000100 Наименование банка Отделение НБ Республика Башкортостан БИК 018073401 лс 20103020420 Получатель Муниципальное общеобразовательное бюджетное учреждение средняя общеобразовательная школа сТемясово муниципального района Баймакский район Республики Башкортостан Факт внесения денежных средств в качестве обеспечения исполнения контракта подтверждается платежным поручением с отметкой банка об оплате Денежные средства возвращаются поставщику с По согласованию с заказчиком поставка дров может быть осуществлена в полном объеме в более ранние сроки Кубм 500\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m inputs = tokenizer(question, text, \n\u001b[0m\u001b[1;32m      3\u001b[0m                    \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"only_second\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"### import os\nimport jax\nfrom flax.serialization import to_bytes, from_bytes\n\n# Определяем путь к файлу, в который мы хотим сохранить модель.\nmodel_path = 'my_model.jax'\n\n# Получаем параметры модели, которые мы хотим сохранить.\nmodel_params = jax.tree_map(lambda x: x.block_until_ready(), state.params)\n\n# Преобразуем параметры в байтовую строку.\nmodel_bytes = to_bytes(model_params)\n\n# Сохраняем модель в файл.\nwith open(model_path, 'wb') as f:\n    f.write(model_bytes)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:50:40.565056Z","iopub.execute_input":"2023-04-23T15:50:40.565474Z","iopub.status.idle":"2023-04-23T15:51:25.801271Z","shell.execute_reply.started":"2023-04-23T15:50:40.565445Z","shell.execute_reply":"2023-04-23T15:51:25.799958Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import jax\n# from flax.serialization import from_bytes\n\n# # Определяем путь к файлу, из которого мы хотим загрузить модель.\n# model_path = 'my_model.jax'\n\n# # Загружаем модель из файла.\n# with open(model_path, 'rb') as f:\n#     model_bytes = f.read()\n\n# # Преобразуем байтовую строку в параметры модели.\n# model_params = from_bytes(model_bytes)\n\n# # Создаем новый экземпляр модели, используя загруженные параметры.\n# model = FlaxAutoModelForQuestionAnswering(**model_params)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_data_loader(dataset, batch_size):\n    if len(dataset)<batch_size:\n        batch = dataset[:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        yield batch\n    else:\n        for i in range(len(dataset) // batch_size):\n            batch = dataset[i * batch_size : (i + 1) * batch_size]\n            batch = {k: jnp.array(v) for k, v in batch.items()}\n\n            yield batch\n        batch = dataset[(i+1) * batch_size:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        yield batch\n        \nfrom flax.jax_utils import unreplicate\n\nunrep_state = unreplicate(state)\n\n\ndef test_step(unrep_state, batch):\n    start_logits, end_logits = unrep_state.apply_fn(**batch, params=unrep_state.params, train=False)[0:2]\n    return state.logits_function(start_logits), state.logits_function(end_logits)\n\nparallel_test_step = jax.pmap(test_step, axis_name=\"batch\")\n\ndef generate_results():\n    preds = []\n    for batch in test_data_loader(test_dataset, total_batch_size):\n\n        if jax.process_index() == 0:\n            inputs = {k: v for k, v in batch.items()}\n            start_logits, end_logits = parallel_test_step(state, inputs)\n            preds.append((start_logits, end_logits))\n    return preds\n","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:23:47.226124Z","iopub.execute_input":"2023-04-19T06:23:47.227060Z","iopub.status.idle":"2023-04-19T06:23:48.475211Z","shell.execute_reply.started":"2023-04-19T06:23:47.227018Z","shell.execute_reply":"2023-04-19T06:23:48.473911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = generate_results()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:23:50.086713Z","iopub.execute_input":"2023-04-19T06:23:50.087644Z","iopub.status.idle":"2023-04-19T06:23:53.131172Z","shell.execute_reply.started":"2023-04-19T06:23:50.087602Z","shell.execute_reply":"2023-04-19T06:23:53.129742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# os.environ[\"WANDB_MODE\"] = \"disabled\"\nfrom datasets import Dataset, DatasetDict\nimport datasets\nmodel_name = \"valhalla/longformer-base-4096-finetuned-squadv1\" \"AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru\"\nmax_length = 4000\ntokenizer = AutoTokenizer.from_pretrained(model_name)\npad_on_right = tokenizer.padding_side == \"right\"\nqa_dataset = QADataset(train_data, val_data)\ntokenized_dataset = qa_dataset.dataset_dict.map(prepare_train_features, batched=True, \n                                                 remove_columns=qa_dataset.dataset_dict[\"train\"].column_names)\n\nQAtrainer = QATrainer(\n    model_name=model_name,\n    train_dataset=tokenized_dataset['train'],\n    val_dataset=tokenized_dataset['validation']\n)\n\nQAtrainer.training()\n\n# use_tpu = True  # Change to True if using TPU\n\n# if use_tpu:\n#     # Create distribution strategy\n#     print('TPU used')\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n#     strategy = tf.distribute.TPUStrategy(tpu)\n\n#     with strategy.scope():\n#         # Create model\n#         print('TPU used')\n# #         model = create_model(model_name)\n#         # Create trainer\n#         QAtrainer = QATrainer(\n#             model_name=model_name,\n#             train_dataset=tokenized_dataset['train'],\n#             val_dataset=tokenized_dataset['validation']\n#         )\n# else:\n#     if tf.config.list_physical_devices('GPU'):\n# #         gpus = tf.config.experimental.list_physical_devices('GPU')\n# #         for gpu in gpus:\n# #             tf.config.experimental.set_virtual_device_configuration(\n# #                 gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n# #         tf.config.optimizer.set_jit(True)\n# #         policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n# #         tf.keras.mixed_precision.experimental.set_policy(policy)\n#         strategy = tf.distribute.MirroredStrategy()\n#     else:\n#         strategy = tf.distribute.OneDeviceStrategy(device=\"/CPU:0\")\n\n#     with strategy.scope():\n# #         model = create_model(model_name)\n#         # Create trainer\n#         QAtrainer = QATrainer(\n#             model_name=model_name,\n#             train_dataset=tokenized_dataset['train'],\n#             val_dataset=tokenized_dataset['validation']\n#         )\n\n# # Train the model\n# QAtrainer.training()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T13:06:28.834300Z","iopub.execute_input":"2023-04-18T13:06:28.834993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tokenized_dataset['train']['attention_mask'][6])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets['validation']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qa_dataset = QADataset(train_data, val_data)\nqa_dataset.dataset_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip list | grep transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataframe(data, fields, subfields):\n    main_df = pd.DataFrame(data)[fields]\n    sub_df_list = []\n    for subfield in subfields:\n        sub_df = pd.DataFrame(list(main_df[subfield]))\n        sub_df.columns = [f\"{subfield}_{col}\" for col in sub_df.columns]\n        sub_df_list.append(sub_df)\n    main_df = main_df.drop(columns=['extracted_part'])\n    return pd.concat([main_df] + sub_df_list, axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_qa_examples(train_data)[0]['context']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmax_seq_length = 4000\nmodel_name = \"valhalla/longformer-base-4096-finetuned-squadv1\"\n# LongformerTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = LongformerConfig.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')\nconfig.attention_mode = 'sliding_chunks'\n\nnum_epochs = 3\nbatch_size = 16\npad_on_right = tokenizer.padding_side == \"right\"\ntrain_dataset = QADataset(train_data, model_name, max_seq_length)\nval_dataset = QADataset(val_data, model_name, max_seq_length)\n# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\nassert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer('<s>', '<s>')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"QAtrainer = QATrainer(\n    model_name=model_name,\n    train_dataset=tokenized_dataset['train'],\n    val_dataset=tokenized_dataset['validation']\n)\n\nQAtrainer.training()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TPU used try 2","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import default_data_collator\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata_collator = default_data_collator\nargs = TrainingArguments(\n    model_name,\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    push_to_hub=True,\n)\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['validation'],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfrom transformers import BertConfig, BertModel\n\n# bert_config = BertConfig(\n#     vocab_size=32000,\n#     hidden_size=768,\n#     num_hidden_layers=12,\n#     num_attention_heads=12,\n#     intermediate_size=3072,\n#     hidden_dropout_prob=0.1,\n#     attention_probs_dropout_prob=0.1,\n#     max_position_embeddings=512,\n#     type_vocab_size=2,\n#     initializer_range=0.02,\n#     layer_norm_eps=1e-12,\n#     gradient_checkpointing=False,\n#     position_embedding_type=\"absolute\",\n#     use_cache=True,\n#     is_decoder=False,\n#     pad_token_id=0,\n#     bos_token_id=1,\n#     eos_token_id=2\n# )\n\ntokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\nmax_seq_length = 4000\nbatch_size = 16\nepochs = eps = 1\n     \ntrain_dataset = QADataset(train_data, tokenizer, max_seq_length)\nval_dataset = QADataset(val_data, tokenizer, max_seq_length)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n\n# qa_model = QAModel(bert_config)\n# qa_trainer = QATrainer(qa_model, train_dataloader, val_dataloader, lr=1e-12, eps=eps)\n# train_losses, val_losses = qa_trainer.train(epochs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, val_loader):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for step, batch in enumerate(val_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            segment_ids = batch['segment_ids'].to(device)\n            start_positions = batch['start_positions'].to(device)\n            end_positions = batch['end_positions'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=segment_ids, start_positions=start_positions, end_positions=end_positions)\n            loss = outputs.loss\n            total_loss += loss.item()\n        avg_loss = total_loss / len(val_loader)\n        return avg_loss\n\ndef predict(model, test_loader):\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for step, batch in enumerate(test_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            segment_ids = batch['segment_ids'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=segment_ids)\n            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n            start_preds = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n            end_preds = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n            for i in range(len(start_preds)):\n                start_pred = np.argmax(start_preds[i])\n                end_pred = np.argmax(end_preds[i])\n                if start_pred > end_pred:\n                    answer = \"\"\n                else:\n                    answer = tokenizer.decode(input_ids[i][start_pred:end_pred+1], skip_special_tokens=True)\n                predictions.append({\n                    \"context\": batch['context'][i],\n                    \"question\": batch['question'][i],\n                    \"extracted_part\": answer\n                })\n    with open('predictions.json', 'w', encoding='utf-8') as f:\n        json.dump(predictions, f, ensure_ascii=False, indent=4)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class QADataset(Dataset):\n    def __init__(self, data, tokenizer, max_seq_length):\n        self.examples = self.create_qa_examples(data)\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n        self.skip = False\n\n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        context = example['context']\n        question = example['question']\n        answer = example['answer']\n        answer_start = example['answer_start']\n        answer_end = example['answer_end']\n        assert answer_end <= len(example['context'])\n        is_char_in_ans = [0] * len(context)\n        for i in range(answer_start, answer_end):\n            is_char_in_ans[i] = 1\n        tokenized_context = self.tokenizer.encode_plus(context, add_special_tokens=False, return_offsets_mapping=True, return_tensors=\"tf\")\n        ans_token_idx = []\n        is_ans_token = [0] * len(tokenized_context)\n\n        for idx, token in enumerate(tokenized_context):\n            token_start = tokenized_context.token_to_chars(idx)[0]\n            token_end = tokenized_context.token_to_chars(idx)[1]\n            if sum(is_char_in_ans[token_start:token_end]) > 0:\n                ans_token_idx.append(idx)\n                for i in range(token_start, token_end):\n                    is_ans_token[i] = 1\n        if sum(is_ans_token) == 0:\n            start_token_idx, end_token_idx = 0, 0\n        else:\n            start_token_idx = ans_token_idx[0]\n            end_token_idx = ans_token_idx[-1]\n            while start_token_idx > 0 and is_ans_token[tokenized_context.token_to_chars(start_token_idx-1)[0]]:\n                start_token_idx -= 1\n            while end_token_idx < len(tokenized_context)-1 and is_ans_token[tokenized_context.token_to_chars(end_token_idx+1)[1]-1]:\n                end_token_idx += 1\n            \n\n        tokenized_question = self.tokenizer.encode_plus(question, return_offsets_mapping=True, return_tensors=\"tf\")\n        tokens = ['<s>'] + tokenized_context.tokens() + ['</s>']+ ['</s>'] + tokenized_question.tokens() + ['</s>']\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        token_type_ids = [0] * (len(tokenized_context.tokens())+2) + [1] * (len(\n            tokenized_question.tokens())+2)\n        attention_mask = [1] * len(input_ids)\n        padding_length = self.max_seq_length - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n        features = []\n#         encoded_dict = self.tokenizer.encode_plus(\n#             question,\n#             context,\n#             add_special_tokens=True,\n#             truncation='longest_first',\n#             max_length=self.max_seq_length,\n#             return_tensors='pt'\n#         )\n#         input_ids = encoded_dict['input_ids'].squeeze()\n#         attention_mask = encoded_dict['attention_mask'].squeeze()\n#         input_ids = torch.nn.functional.pad(encoded_dict['input_ids'], (0, self.max_seq_length - encoded_dict['input_ids'].shape[1]), mode='constant', value=0)\n#         attention_mask = torch.nn.functional.pad(encoded_dict['attention_mask'], (0, self.max_seq_length - encoded_dict['attention_mask'].shape[1]), mode='constant', value=0)\n        \n        features = {'input_ids': input_ids, 'attention_mask': attention_mask, \n                    'token_type_ids': token_type_ids, 'start_token_idx': start_token_idx, 'end_token_idx': end_token_idx}\n#         max_len_dict = {}\n#         for key, value in features.items():\n#             if isinstance(value, (list, tuple)):\n#                 max_len_dict[key] = max(len(seq) for seq in value)\n#         for key, value in features.items():\n#             if isinstance(value, (list, tuple)):\n#                 max_len = max_len_dict[key]\n#                 for i in range(len(value)):\n#                     pad_len = max_len - len(value[i])\n#                     value[i] = torch.cat([value[i], torch.zeros(pad_len, dtype=torch.long)])\n#                 features[key] = torch.stack(value)\n\n        return features\n    \n    def create_qa_examples(self, data):\n        examples = []\n        for row in data:\n            text = row['text']\n            question = row['label']\n            extracted_part = row.get('extracted_part', {})\n            if extracted_part and 'text' in extracted_part:\n                answer = extracted_part['text'][0].strip()\n                answer_start = extracted_part['answer_start'][0]\n                answer_end = extracted_part['answer_end'][0]\n            else:\n                answer = answer_start = answer_end = None\n\n            example = {'context': text, 'question': question, 'answer': answer, 'answer_start': answer_start, 'answer_end': answer_end}\n            examples.append(example)\n        return examples\n    \n    \n    @staticmethod\n    def prepare_test_data(data):\n        examples = []\n        for row in data:\n            text = row['text']\n            question = row['label']\n            example = {'context': text, 'question': question}\n            examples.append(example)\n        return examples\n\ndef collate_fn(batch, device):\n    input_ids = pad_sequence([torch.tensor(example['input_ids']) for example in batch], batch_first=True, padding_value=0).to(device)\n    attention_mask = pad_sequence([torch.tensor(example['attention_mask']) for example in batch], batch_first=True, padding_value=0).to(device)\n    token_type_ids = pad_sequence([torch.tensor(example['token_type_ids']) for example in batch], batch_first=True, padding_value=0).to(device)\n    start_positions = torch.tensor([example['answer_start'] for example in batch]).to(device)\n    end_positions = torch.tensor([example['answer_end'] for example in batch]).to(device)\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'token_type_ids': token_type_ids,\n        'start_positions': start_positions,\n        'end_positions': end_positions\n    }\n\n\ndef create_inputs_targets(dataset):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n        \"start_token_idx\": [],\n        \"end_token_idx\": [],\n    }\n    for idx in range(len(dataset)):\n        example = dataset[idx]\n        for key in dataset_dict:\n            if isinstance(example[key], torch.Tensor):\n                value = example[key].numpy().tolist()\n            else:\n                value = example[key]\n            dataset_dict[key].append(value)\n\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n    return x, y\n\ndef x_y_split(model_name, train_data, validation_data, batch_size):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)\n#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    train_dataset = QADataset(train_data, tokenizer, max_seq_length)\n#     train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True,\n#                                    collate_fn=lambda batch: collate_fn(batch, device))\n    x_train, y_train = create_inputs_targets(train_dataset)\n    \n    validation_dataset = QADataset(validation_data, tokenizer, max_seq_length)\n#     validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, drop_last=True,\n#                                         collate_fn=lambda batch: collate_fn(batch, device))\n    x_val, y_val = create_inputs_targets(validation_dataset)\n\n    return x_train, y_train, x_val, y_val\n\n\n\ndef create_model(model_name):\n    ## BERT encoder\n    encoder = TFLongformerForQuestionAnswering.from_pretrained(model_name)\n\n    ## QA Model\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model\n\nclass ExactMatch(keras.callbacks.Callback):\n    def __init__(self, x_eval, y_eval):\n        super().__init__()\n        self.x_eval = x_eval\n        self.y_eval = y_eval\n\n    def on_epoch_end(self, epoch, logs=None):\n        pred_start, pred_end = self.model.predict(self.x_eval)\n        count = 0\n        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n            offsets = squad_eg.context_token_to_char\n            start = np.argmax(start)\n            end = np.argmax(end)\n            if start >= len(offsets):\n                continue\n            pred_char_start = offsets[start][0]\n            if end < len(offsets):\n                pred_char_end = offsets[end][1]\n                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n            else:\n                pred_ans = squad_eg.context[pred_char_start:]\n\n            if pred_ans in squad_eg.all_answers:\n                count += 1\n        acc = count / len(self.y_eval[0])\n        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\nexamp = QADataset(train_data, tokenizer, max_seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"QADataset(train_data, tokenizer, max_seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, validation_data = train_test_split(train_data[:50], test_size=0.2, random_state=42)\nmax_seq_length = 4000\nmodel_name = \"valhalla/longformer-base-4096-finetuned-squadv1\"\nconfiguration = LongformerConfig()\nnum_epochs = 3\n\nx_train, y_train, x_val, y_val = x_y_split(model_name = model_name, train_data = train_data, validation_data = validation_data, batch_size = 16)\nmax_len = len(x_train[0][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y_train[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train имеет структуру словаря, в котором есть 3 подсловаря - признака, в каждом из них набор примеров n-го количества, в каждом примере уже непосредственно находятся данные\nв y_train 2 словаря, которые содержат n примеров, в каждом из которых находится таргет. \nкакие модели обучения можно написать на таких данных, не пользуясь предобученными моделями и их ограничениями","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntf.debugging.set_log_device_placement(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_tpu = False  # Change to True if using TPU\n\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    strategy = tf.distribute.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        print('TPU used')\n        model = create_model(model_name)\nelse:\n#     # Use GPU if TPU is not available\n#     if tf.config.list_physical_devices('GPU'):\n#         strategy = tf.distribute.MirroredStrategy()\n        \n#     else:\n    strategy = tf.distribute.OneDeviceStrategy(device=\"/CPU:0\")\n\n    with strategy.scope():\n        model = create_model(model_name)\n\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nprint(tf.test.is_built_with_cuda())\nprint(tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx_train_np = [np.array(x_train[0]), np.array(x_train[1]), np.array(x_train[2])]\ny_train_np = [np.array(y_train[0]), np.array(y_train[1])]\nx_val_np = [np.array(x_val[0]), np.array(x_val[1]), np.array(x_val[2])]\ny_val_np = [np.array(y_val[0]), np.array(y_val[1])]\nexact_match_callback = ExactMatch(x_val_np, y_val_np)\nmodel.fit(\n    x_train_np,\n    y_train_np,\n    validation_data=(x_val_np, y_val_np),\n    epochs=1,\n    verbose=2,\n    batch_size=64,\n#     callbacks=[exact_match_callback],\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = QADataset(train_data, tokenizer, max_seq_length)\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True,\n                               collate_fn=lambda batch: collate_fn(batch, device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader.dataset[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained('cointegrated/LaBSE-en-ru')\n# examples = QADataset.create_qa_examples(train_data, train_data)\n# questions = [example['context'] for example in examples]\n# question_tokens = [tokenizer.tokenize(question) for question in questions]\n# import matplotlib.pyplot as plt\n\n# question_lengths = [len(tokens) for tokens in question_tokens]\n# plt.hist(question_lengths, bins=50)\n# plt.xlabel('Length of question tokens')\n# plt.ylabel('Frequency')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Разбиваем данные на обучающую и валидационную выборки\n# train_data, validation_data = train_test_split(train_data, test_size=0.2, random_state=42)\n# max_seq_length = 3072\n# model_name = \"allenai/longformer-large-4096-finetuned-triviaqa\"\n\n# # Число эпох обучения\n# num_epochs = 3\n# output_dir = 'my_model'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'cointegrated/LaBSE-en-ru'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import BigBirdTokenizer, BigBirdForQuestionAnswering\n\n# tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n# model = BigBirdForQuestionAnswering.from_pretrained('google/bigbird-roberta-base')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# import tensorflow as tf\n\n# train(model_name, train_data, validation_data, output_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}