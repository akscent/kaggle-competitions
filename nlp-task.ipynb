{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install jax jaxlib flax optax mtranslate sentencepiece datasets transformers accelerate scikit-learn ipywidgets datasets nltk importlib-metadata\n!pip install transformers --upgrade\n\nfrom IPython.display import display, HTML\n# from huggingface_hub import notebook_login\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForQuestionAnswering, AdamW, Trainer, TrainingArguments, default_data_collator, FlaxAutoModelForQuestionAnswering\nimport time\nimport nltk\nimport math\nimport torch\nfrom datasets import Dataset, DatasetDict, load_dataset, load_metric\nimport tensorflow as tf\nimport re\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport transformers\n# from accelerate import Accelerator\nimport datasets\nimport sentencepiece\nfrom time import sleep\nfrom time import time\nfrom random import randint\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Вход в huggingface\n# notebook_login()\n!huggingface-cli login --token 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","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:45:47.922076Z","iopub.execute_input":"2023-04-24T10:45:47.923153Z","iopub.status.idle":"2023-04-24T10:45:49.570737Z","shell.execute_reply.started":"2023-04-24T10:45:47.923112Z","shell.execute_reply":"2023-04-24T10:45:49.568856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"from transformers import FlaxMarianMTModel, AutoTokenizer\nimport jax\nfrom jax import vmap\nimport jax.numpy as jnp\nimport jax.numpy as jnp\nimport jax.lax as lax\n# from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-04-24T06:43:48.334863Z","iopub.execute_input":"2023-04-24T06:43:48.335650Z","iopub.status.idle":"2023-04-24T06:43:51.251421Z","shell.execute_reply.started":"2023-04-24T06:43:48.335616Z","shell.execute_reply":"2023-04-24T06:43:51.250122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T07:13:25.874324Z","iopub.execute_input":"2023-04-24T07:13:25.874747Z","iopub.status.idle":"2023-04-24T07:13:25.883278Z","shell.execute_reply.started":"2023-04-24T07:13:25.874718Z","shell.execute_reply":"2023-04-24T07:13:25.882133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n\ndef clean_text(text):\n    # Удаление указанных символов из текста\n    text = re.sub(r'[\\\"\\#\\$\\;\\:\\^\\&\\№\\*\\-\\=\\+\\-\\,\\.\\@\\!\\?\\/\\]\\[\\}\\{\\|\\~\\«\\»\\`]', '', text)\n    text = re.sub(r'_', ' ', text)  # Замена _ на пробел\n    text = re.sub(r'\\s+', ' ', text)  # Удаление лишних пробелов\n    return text.strip()\n\ndef create_qa_dataframe(data):\n    examples = []\n    for row in tqdm(data, total = len(data)):\n        cleaned_text = clean_text(row['text'])\n        question = row['label']\n        extracted_part = row.get('extracted_part', {})\n        if extracted_part and extracted_part['text'] is not None:\n            answer = clean_text(extracted_part['text'][0].strip())\n            answer_start = extracted_part['answer_start'][0]\n            answer_end = extracted_part['answer_end'][0]\n            if answer:\n                answer_words = answer.split()\n                match_found = False\n                for i in range(len(answer_words)):\n                    answer_start_new = cleaned_text.find(answer_words[i])\n                    if answer_start_new != -1:\n                        match_found = True\n                        for j in range(i+1, len(answer_words)):\n                            next_word_start = answer_start_new + len(answer_words[i-1])\n                            next_word_end = next_word_start + len(answer_words[j])\n                            next_word = cleaned_text[next_word_start:next_word_end]\n                            if answer_words[j] != next_word:\n                                match_found = False\n                                break\n                        if match_found:\n                            answer_start = answer_start_new\n                            answer_end = answer_start_new + len(answer_words) - 1\n                            break\n                if not match_found:\n                    answer_start = answer_end = 0\n            else:\n                answer_start = answer_end = 0\n        else:\n            answer_start = answer_end = 0\n            answer = None\n            \n#         translated_text = translate_sentence(cleaned_text)\n#         translated_question = translate_sentence(question)\n#         translated_answer = translate_sentence(answer) if answer else None\n\n        example = {'context': cleaned_text, 'question': question, 'answer': answer, 'answer_start': answer_start, 'answer_end': answer_end}\n        examples.append(example)\n    df = pd.DataFrame(examples)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-21T00:30:03.434227Z","iopub.execute_input":"2023-04-21T00:30:03.434638Z","iopub.status.idle":"2023-04-21T00:30:06.357018Z","shell.execute_reply.started":"2023-04-21T00:30:03.434608Z","shell.execute_reply":"2023-04-21T00:30:06.355554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = create_qa_dataframe(train_data)\ndisplay(HTML(train_df[6:12].to_html()))","metadata":{"execution":{"iopub.status.busy":"2023-04-21T00:30:12.222891Z","iopub.execute_input":"2023-04-21T00:30:12.224073Z","iopub.status.idle":"2023-04-21T00:30:12.659435Z","shell.execute_reply.started":"2023-04-21T00:30:12.224030Z","shell.execute_reply":"2023-04-21T00:30:12.658084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create dataset","metadata":{}},{"cell_type":"code","source":"# import mtranslate\n\nclass QADataset:\n    def __init__(self, train_data, val_data, test_data=None, zero_flag=False):\n        self.zero_flag = zero_flag\n        self.train_examples = self.create_qa_example(train_data)\n        self.val_examples = self.create_qa_example(val_data)\n        self.test_examples = self.create_qa_example(test_data) if test_data else []\n        self.train_dataset = datasets.Dataset.from_pandas(pd.DataFrame(self.train_examples))\n        self.val_dataset = datasets.Dataset.from_pandas(pd.DataFrame(self.val_examples))\n        self.test_dataset = datasets.Dataset.from_pandas(pd.DataFrame(self.test_examples)) if test_data else None\n        if test_data:\n            self.dataset_dict = DatasetDict({\n                'train': self.train_dataset,\n                'validation': self.val_dataset,\n                'test': self.test_dataset\n            })\n        else:\n            self.dataset_dict = DatasetDict({\n                'train': self.train_dataset,\n                'validation': self.val_dataset\n            })\n\n    def clean_text(self, text):\n        # Удаление указанных символов из текста\n        text = re.sub(r'[\\\"\\#\\$\\;\\:\\^\\&\\№\\*\\-\\=\\+\\-\\,\\.\\@\\!\\?\\/\\]\\[\\}\\{\\|\\~\\«\\»\\`]', '', text)\n        text = re.sub(r'_', ' ', text)  # Замена _ на пробел\n        text = re.sub(r'\\s+', ' ', text)  # Удаление лишних пробелов\n        return text.strip()\n\n    def create_qa_example(self, data):\n        examples = []\n        for row in tqdm(data, total = len(data)):\n            cleaned_text = self.clean_text(row['text'])\n            question = row['label']\n            extracted_part = row.get('extracted_part', {})\n            if extracted_part and extracted_part['text'] is not None:\n                answer = self.clean_text(extracted_part['text'][0].strip())\n                answer_start = extracted_part['answer_start'][0]\n                answer_end = extracted_part['answer_end'][0]\n                if answer:\n                    answer_words = answer.split()\n                    match_found = False\n                    for i in range(len(answer_words)):\n                        answer_start_new = cleaned_text.find(answer_words[i])\n                        if answer_start_new != -1:\n                            match_found = True\n                            for j in range(i+1, len(answer_words)):\n                                next_word_start = answer_start_new + len(answer_words[i-1])\n                                next_word_end = next_word_start + len(answer_words[j])\n                                next_word = cleaned_text[next_word_start:next_word_end]\n                                if answer_words[j] != next_word:\n                                    match_found = False\n                                    break\n                            if match_found:\n                                answer_start = answer_start_new\n                                answer_end = answer_start_new + len(answer_words) - 1\n                                break\n                    if not match_found:\n                        answer_start = answer_end = 0\n                else:\n                    answer_start = answer_end = 0\n            else:\n                answer_start = answer_end = 0\n                answer = None\n\n#             translated_text = translate_sentence(cleaned_text)\n    #         translated_question = translate_sentence(question)\n    #         translated_answer = translate_sentence(answer) if answer else None\n\n            # Append example only if answer is not None\n            if self.zero_flag:\n                if answer_start != 0 :\n                    example = {'context': cleaned_text, 'question': question, 'answer': answer, 'answer_start': answer_start, 'answer_end': answer_end}\n                    examples.append(example)\n            else:\n                example = {'context': cleaned_text, 'question': question, 'answer': answer, 'answer_start': answer_start, 'answer_end': answer_end}\n                examples.append(example)\n            \n        return examples\n    \ndef prepare_train_features(examples):\n#     examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=150,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        return_tensors=\"jax\",\n    )\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = tokenized_examples[\"input_ids\"][i]\n#         cls_index = input_ids.index(tokenizer.cls_token_id)\n        cls_index = jnp.where(input_ids == tokenizer.cls_token_id)[0][0]\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answer_start = examples[\"answer_start\"][sample_index]\n        answer_end = examples[\"answer_end\"][sample_index]\n        if answer_start == 0 or answer_start == None:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            tokenized_examples[\"start_positions\"].append(answer_start)\n            tokenized_examples[\"end_positions\"].append(answer_end)\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:46:52.426628Z","iopub.execute_input":"2023-04-24T10:46:52.427905Z","iopub.status.idle":"2023-04-24T10:46:52.454380Z","shell.execute_reply.started":"2023-04-24T10:46:52.427829Z","shell.execute_reply":"2023-04-24T10:46:52.452979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Used git finetune script","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-23T01:03:26.286757Z","iopub.execute_input":"2023-04-23T01:03:26.287678Z","iopub.status.idle":"2023-04-23T01:03:33.798040Z","shell.execute_reply.started":"2023-04-23T01:03:26.287639Z","shell.execute_reply":"2023-04-23T01:03:33.796892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T02:19:12.472359Z","iopub.execute_input":"2023-04-24T02:19:12.472831Z","iopub.status.idle":"2023-04-24T02:19:20.992089Z","shell.execute_reply.started":"2023-04-24T02:19:12.472793Z","shell.execute_reply":"2023-04-24T02:19:20.991040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Пример\ncontext = \"УТВЕРЖДАЮ Председатель закупочной комиссии заместитель генерального директора по логистике и МТО АО АТХ ТЮ Шустова 01 сентября 2022 г ДОКУМЕНТАЦИЯ О КОНКУРЕНТНОЙ ЗАКУПКЕ ЗАПРОС ПРЕДЛОЖЕНИЙ В ЭЛЕКТРОННОЙ ФОРМЕ УЧАСТНИКАМИ КОТОРОГО МОГУТ БЫТЬ ТОЛЬКО СУБЪЕКТЫ МАЛОГО И СРЕДНЕГО ПРЕДПРИНИМАТЕЛЬСТВА на право заключения Договора на выполнение работ по ремонту зданий и сооружений г Киров 2022 год Стр2 СОДЕРЖАНИЕ СОДЕРЖАНИЕ 2 I ОБЩИЕ УСЛОВИЯ ПРОВЕДЕНИЯ закупки 3 1 ОБЩИЕ ПОЛОЖЕНИЯ 3 11 Правовой статус документов 3 12 Заказчик предмет и условия проведения закупки 3 13 Начальная (максимальная) цена договора 4 14 Требования к участникам закупки 4 15 Участие в закупке коллективных участников (группы лиц) 5 16 Привлечение соисполнителей (субподрядчиков) к исполнению договора 6 17 Расходы на участие в закупке и при заключении договора 7 18 Предоставление приоритетов товаров российского происхождения работ услуг выполняемых оказываемых российс 352 564 Закупка по единичным расценкам\"\nquestion = \"обеспечение гарантийных обязательств\"\n\n# \ninputs = tokenizer(question, context, truncation=\"only_second\",\n        max_length=max_length,\n        padding=\"max_length\",\n        return_tensors=\"jax\",)\n\noutputs = model(**inputs)\nstart_logits = outputs.start_logits[0][len(tokenizer.encode(question))+1:(len(tokenizer.encode(question))+1)+(len(tokenizer.encode(context)))-2]\nend_logits = outputs.end_logits[0][len(tokenizer.encode(question))+1:(len(tokenizer.encode(question))+1)+(len(tokenizer.encode(context)))-2]\nstart_probs = jax.nn.softmax(start_logits, axis=-1)\nend_probs = jax.nn.softmax(end_logits, axis=-1)\nmax_prob = 0\nbest_pair = None\nfor start_idx in range(start_probs.shape[0]):\n    for end_idx in range(start_idx, end_probs.shape[0]):\n        prob = (start_probs[start_idx] * end_probs[end_idx]).max()\n        if prob > max_prob:\n            max_prob = prob\n            best_pair = (start_idx, end_idx)\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][len(tokenizer.encode(question))+1:(len(tokenizer.encode(question))+1)+(len(tokenizer.encode(context)))-2])\nanswer_tokens = tokens[best_pair[0]:best_pair[1]]\nanswer = tokenizer.convert_tokens_to_string(answer_tokens)\nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T08:58:05.698594Z","iopub.execute_input":"2023-04-24T08:58:05.699388Z","iopub.status.idle":"2023-04-24T08:59:44.028003Z","shell.execute_reply.started":"2023-04-24T08:58:05.699348Z","shell.execute_reply":"2023-04-24T08:59:44.026691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:20:09.546817Z","iopub.execute_input":"2023-04-24T01:20:09.547219Z","iopub.status.idle":"2023-04-24T01:20:09.580985Z","shell.execute_reply.started":"2023-04-24T01:20:09.547185Z","shell.execute_reply":"2023-04-24T01:20:09.579663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:21:27.671894Z","iopub.execute_input":"2023-04-24T01:21:27.673080Z","iopub.status.idle":"2023-04-24T01:21:27.681293Z","shell.execute_reply.started":"2023-04-24T01:21:27.673032Z","shell.execute_reply":"2023-04-24T01:21:27.680007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:01:50.807127Z","iopub.execute_input":"2023-04-24T01:01:50.807613Z","iopub.status.idle":"2023-04-24T01:01:50.820471Z","shell.execute_reply.started":"2023-04-24T01:01:50.807561Z","shell.execute_reply":"2023-04-24T01:01:50.819062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:42:38.199474Z","iopub.execute_input":"2023-04-24T00:42:38.200839Z","iopub.status.idle":"2023-04-24T00:42:38.210209Z","shell.execute_reply.started":"2023-04-24T00:42:38.200766Z","shell.execute_reply":"2023-04-24T00:42:38.208710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:02:15.083844Z","iopub.execute_input":"2023-04-24T01:02:15.084295Z","iopub.status.idle":"2023-04-24T01:02:15.094106Z","shell.execute_reply.started":"2023-04-24T01:02:15.084255Z","shell.execute_reply":"2023-04-24T01:02:15.092845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:03:03.633233Z","iopub.execute_input":"2023-04-24T01:03:03.633730Z","iopub.status.idle":"2023-04-24T01:03:03.644034Z","shell.execute_reply.started":"2023-04-24T01:03:03.633689Z","shell.execute_reply":"2023-04-24T01:03:03.643080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:03:09.714541Z","iopub.execute_input":"2023-04-24T01:03:09.714980Z","iopub.status.idle":"2023-04-24T01:03:09.725862Z","shell.execute_reply.started":"2023-04-24T01:03:09.714944Z","shell.execute_reply":"2023-04-24T01:03:09.724746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:52:43.623829Z","iopub.execute_input":"2023-04-24T00:52:43.624244Z","iopub.status.idle":"2023-04-24T00:52:43.632239Z","shell.execute_reply.started":"2023-04-24T00:52:43.624207Z","shell.execute_reply":"2023-04-24T00:52:43.631119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:53:19.484183Z","iopub.execute_input":"2023-04-24T00:53:19.484622Z","iopub.status.idle":"2023-04-24T00:53:19.492084Z","shell.execute_reply.started":"2023-04-24T00:53:19.484562Z","shell.execute_reply":"2023-04-24T00:53:19.491044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:53:12.225808Z","iopub.execute_input":"2023-04-24T00:53:12.226246Z","iopub.status.idle":"2023-04-24T00:53:12.236279Z","shell.execute_reply.started":"2023-04-24T00:53:12.226212Z","shell.execute_reply":"2023-04-24T00:53:12.234839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:25:40.138772Z","iopub.execute_input":"2023-04-24T00:25:40.139241Z","iopub.status.idle":"2023-04-24T00:25:40.147384Z","shell.execute_reply.started":"2023-04-24T00:25:40.139204Z","shell.execute_reply":"2023-04-24T00:25:40.146045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:15:38.130691Z","iopub.execute_input":"2023-04-24T00:15:38.131264Z","iopub.status.idle":"2023-04-24T00:15:39.790845Z","shell.execute_reply.started":"2023-04-24T00:15:38.131183Z","shell.execute_reply":"2023-04-24T00:15:39.789193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:13:59.712882Z","iopub.execute_input":"2023-04-24T00:13:59.713354Z","iopub.status.idle":"2023-04-24T00:13:59.722903Z","shell.execute_reply.started":"2023-04-24T00:13:59.713304Z","shell.execute_reply":"2023-04-24T00:13:59.721290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:12:09.037812Z","iopub.execute_input":"2023-04-24T00:12:09.038376Z","iopub.status.idle":"2023-04-24T00:12:09.048209Z","shell.execute_reply.started":"2023-04-24T00:12:09.038323Z","shell.execute_reply":"2023-04-24T00:12:09.046890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T00:14:03.934697Z","iopub.execute_input":"2023-04-24T00:14:03.936125Z","iopub.status.idle":"2023-04-24T00:14:03.946313Z","shell.execute_reply.started":"2023-04-24T00:14:03.936072Z","shell.execute_reply":"2023-04-24T00:14:03.944624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TPU used try 1","metadata":{}},{"cell_type":"code","source":"import jax\nimport flax\nimport optax\nfrom itertools import chain\nfrom typing import Callable\nimport jax.numpy as jnp\nfrom flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\nfrom flax.training import train_state\nfrom flax import traverse_util\n# from torch.utils.data import DataLoader\n# import datasets","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:45:55.704621Z","iopub.execute_input":"2023-04-24T10:45:55.705050Z","iopub.status.idle":"2023-04-24T10:45:56.120048Z","shell.execute_reply.started":"2023-04-24T10:45:55.705014Z","shell.execute_reply":"2023-04-24T10:45:56.118600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(flax.__version__)\njax.local_devices()","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:45:59.757968Z","iopub.execute_input":"2023-04-24T10:45:59.758396Z","iopub.status.idle":"2023-04-24T10:45:59.795844Z","shell.execute_reply.started":"2023-04-24T10:45:59.758359Z","shell.execute_reply":"2023-04-24T10:45:59.794461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\n# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n\n# Args\nnum_labels = 1\nseed = 60\nnum_train_epochs = 8\nlearning_rate = 2e-5\nper_device_batch_size = 1\ntotal_batch_size = per_device_batch_size * jax.local_device_count()\nmodel_name = 'xlm-roberta-base'\n# \"distilbert-base-uncased\"\nmax_length = 1440\n\n# model elements\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = AutoConfig.from_pretrained(model_name)\nmodel = FlaxAutoModelForQuestionAnswering.from_pretrained(model_name, config=config)\npad_on_right = tokenizer.padding_side == \"right\"\n\n# dataset\nqa_dataset = QADataset(train_data, val_data, test_data=test_data)\n# test_data=test_data\ntokenized_dataset = qa_dataset.dataset_dict.map(prepare_train_features, batched=True, \n                                                 remove_columns=qa_dataset.dataset_dict[\"train\"].column_names)\ntrain_dataset = tokenized_dataset[\"train\"]\neval_dataset = tokenized_dataset[\"validation\"]\ntest_dataset = tokenized_dataset[\"test\"]\n\n#add args\nnum_train_steps = len(train_dataset) // total_batch_size * num_train_epochs\n\nprint(\"The overall batch size (both for training and eval) is\", total_batch_size)\nprint(\"The number of train steps (all the epochs) is\", num_train_steps)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:47:01.609148Z","iopub.execute_input":"2023-04-24T10:47:01.609656Z","iopub.status.idle":"2023-04-24T10:47:51.062969Z","shell.execute_reply.started":"2023-04-24T10:47:01.609615Z","shell.execute_reply":"2023-04-24T10:47:51.060812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:38:47.498697Z","iopub.execute_input":"2023-04-24T10:38:47.499369Z","iopub.status.idle":"2023-04-24T10:38:47.514492Z","shell.execute_reply.started":"2023-04-24T10:38:47.499323Z","shell.execute_reply":"2023-04-24T10:38:47.513208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-04-24T07:38:12.530470Z","iopub.execute_input":"2023-04-24T07:38:12.531086Z","iopub.status.idle":"2023-04-24T07:38:12.539776Z","shell.execute_reply.started":"2023-04-24T07:38:12.531038Z","shell.execute_reply":"2023-04-24T07:38:12.538246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset['train']['start_positions'][6]","metadata":{"execution":{"iopub.status.busy":"2023-04-24T07:32:38.589474Z","iopub.execute_input":"2023-04-24T07:32:38.590346Z","iopub.status.idle":"2023-04-24T07:32:38.598552Z","shell.execute_reply.started":"2023-04-24T07:32:38.590310Z","shell.execute_reply":"2023-04-24T07:32:38.597218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenized_dataset['train']['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2023-04-24T07:32:41.770699Z","iopub.execute_input":"2023-04-24T07:32:41.771593Z","iopub.status.idle":"2023-04-24T07:32:42.894824Z","shell.execute_reply.started":"2023-04-24T07:32:41.771556Z","shell.execute_reply":"2023-04-24T07:32:42.893402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(tokenized_dataset['train']['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2023-04-24T07:32:48.303878Z","iopub.execute_input":"2023-04-24T07:32:48.304367Z","iopub.status.idle":"2023-04-24T07:32:49.404208Z","shell.execute_reply.started":"2023-04-24T07:32:48.304333Z","shell.execute_reply":"2023-04-24T07:32:49.402859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = FlaxAutoModelForQuestionAnswering.from_pretrained(model_name, config=config)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T02:18:23.850699Z","iopub.execute_input":"2023-04-24T02:18:23.851251Z","iopub.status.idle":"2023-04-24T02:18:34.665934Z","shell.execute_reply.started":"2023-04-24T02:18:23.851198Z","shell.execute_reply":"2023-04-24T02:18:34.664693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T02:10:26.247514Z","iopub.execute_input":"2023-04-24T02:10:26.248831Z","iopub.status.idle":"2023-04-24T02:10:26.261333Z","shell.execute_reply.started":"2023-04-24T02:10:26.248763Z","shell.execute_reply":"2023-04-24T02:10:26.259867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T02:11:58.327302Z","iopub.execute_input":"2023-04-24T02:11:58.327882Z","iopub.status.idle":"2023-04-24T02:11:58.346247Z","shell.execute_reply.started":"2023-04-24T02:11:58.327827Z","shell.execute_reply":"2023-04-24T02:11:58.345028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T08:04:39.127183Z","iopub.execute_input":"2023-04-24T08:04:39.127626Z","iopub.status.idle":"2023-04-24T08:04:39.183928Z","shell.execute_reply.started":"2023-04-24T08:04:39.127597Z","shell.execute_reply":"2023-04-24T08:04:39.182690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm\n\nclass RMSE(datasets.Metric):\n    def _info(self):\n        return datasets.MetricInfo(\n            description=\"Calculates Root Mean Squared Error (RMSE) metric.\",\n            citation=\"TODO: _CITATION\",\n            inputs_description=\"_KWARGS_DESCRIPTION\",\n            features=datasets.Features({\n                'predictions': datasets.Value('float32'),\n                'references': datasets.Value('float32'),\n            }),\n            codebase_urls=[],\n            reference_urls=[],\n            format='numpy'\n        )\n\n    def _compute(self, predictions, references):\n        rmse = np.sqrt(np.sum(np.square(predictions - references)) / predictions.shape[0])\n        return {\"RMSE\": rmse}\n\nclass TrainState(train_state.TrainState):\n    logits_function: Callable = flax.struct.field(pytree_node=False)\n    loss_function: Callable = flax.struct.field(pytree_node=False)\n\nlearning_rate_function = optax.cosine_onecycle_schedule(transition_steps=num_train_steps, \n                                                        peak_value=learning_rate, pct_start=0.1, \n                                                       )\ndef decay_mask_fn(params):\n    flat_params = traverse_util.flatten_dict(params)\n    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n    return traverse_util.unflatten_dict(flat_mask)\n\n# def adamw(weight_decay):\n#     return optax.adamw(learning_rate=learning_rate_function, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay, mask=decay_mask_fn)\n\n# adamw = adamw(1e-2)\n\ndef adamw(weight_decay, schedule_fn):\n    return optax.chain(\n        optax.scale_by_adam(b1=0.9, b2=0.999, eps=1e-6),\n        optax.scale_by_schedule(schedule_fn),\n        optax.scale(-1.0),\n        optax.additive_weight_decay(weight_decay, mask=decay_mask_fn)\n    )\n\n# Пример функции для создания расписания скорости обучения\ndef learning_rate_schedule(max_lr, warmup_steps, total_steps):\n    step_fn = lambda step: np.minimum((step + 1) / warmup_steps, 1.0)\n    lr_fn = lambda step: max_lr * step_fn(step) * (total_steps - step) / np.maximum(total_steps - warmup_steps, 1.0)\n    return lr_fn\n\n# Пример использования нового оптимизатора с расписанием скорости обучения\nlearning_rate_fn = learning_rate_schedule(max_lr=1e-3, warmup_steps=(num_train_steps/10), total_steps=num_train_steps)\nadamw = adamw(weight_decay=1e-2, schedule_fn=learning_rate_fn)\n\n# @jax.jit\n# def loss_function(logits, labels):\n#     return jnp.mean((logits[..., 0] - labels) ** 2)\n\n@jax.jit\ndef loss_function(predicted_positions, positions):\n#     logits = logits[..., 0] # получение массива start_logits\n#     predicted_positions = jnp.argmax(logits, axis=-1) # получение индекса с наибольшим значением в каждом батче\n    loss = jnp.mean((predicted_positions - positions) ** 2) # вычисление средней ошибки\n    return loss\n\n\n# @jax.jit    \n# def eval_function(logits):\n#     return logits[..., 0]\n\n@jax.jit    \ndef eval_function(logits):\n#     logits = logits[..., 0]\n#     l_index = jnp.argmax(logits, axis=-1)\n#     start_values = jnp.take_along_axis(logits, axis=-1)\n    return logits\n\n@jax.jit\ndef get_best_indexes(outputs):\n    \n#     mask_indices = jnp.where(batch['input_ids'][0] == 2)[0] # 2 - разделитель </sep>\n#     last_mask_index = mask_indices[-1]-1\n#     second_last_mask_index = mask_indices[-2]+1\n#     start_logits = outputs.start_logits[0][second_last_mask_index:last_mask_index]\n#     end_logits = outputs.end_logits[0][second_last_mask_index:last_mask_index]\n\n    end_logits = outputs.end_logits[0]\n    start_logits = outputs.start_logits[0]\n    \n    # Softmax\n    start_probs = jax.nn.softmax(start_logits, axis=-1)\n    end_probs = jax.nn.softmax(end_logits, axis=-1)\n\n    # Поиск индексов с наибольшим произведением вероятностей\n    max_prob = 0\n    best_pair = None\n    for start_idx in range(start_probs.shape[0]):\n        for end_idx in range(start_idx, end_probs.shape[0]):\n            prob = (start_probs[start_idx] * end_probs[end_idx]).max()\n            if prob > max_prob:\n                max_prob = prob\n                best_pair = (start_idx, end_idx)\n\n    return best_pair[0], best_pair[1]\n\n\n\nstate = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw,\n                          logits_function=eval_function, loss_function=loss_function,\n                         )\n\n\n\ndef train_step(state, batch, dropout_rng):\n    start_positions = batch.pop(\"start_positions\")\n    end_positions = batch.pop(\"end_positions\")\n    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n\n    def loss_fn(params):\n        outputs = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True, return_dict=True)\n#         mask_indices = jnp.where(batch['input_ids'][0] == 2, batch['input_ids'])[0]\n        start_pred, end_pred = get_best_indexes(outputs)\n        start_loss = state.loss_function(start_pred, start_positions)\n        end_loss = state.loss_function(end_pred, end_positions)\n        loss = (start_loss + end_loss) / 2.0\n        return loss\n\n    grad_function = jax.value_and_grad(loss_fn)\n    loss, grad = grad_function(state.params)\n    grad = jax.lax.pmean(grad, \"batch\")\n    new_state = state.apply_gradients(grads=grad)\n    new_params = new_state.params\n    new_params = jax.tree_map(lambda p, g: p - learning_rate_fn(state.step) * g, new_state.params, grad)\n    new_state = new_state.replace(params=new_params)\n    metrics = {\"loss\": loss, \"learning_rate\": learning_rate_fn(state.step)}\n    print(metrics)\n    return new_state, metrics, new_dropout_rng\n\nparallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n\ndef eval_step(state, batch):\n    outputs = state.apply_fn(**batch, params=state.params, train=False)\n#     start_logits = outputs.start_logits\n#     end_logits = outputs.end_logits\n#     start_logits = jax.lax.pmean(state.logits_function(start_logits), \"batch\")\n#     end_logits = jax.lax.pmean(state.logits_function(end_logits), \"batch\")\n    start_pred, end_pred = get_best_indexes(batch, outputs)\n    return state.logits_function(start_pred), state.logits_function(end_pred)\n\nparallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n\ndef train_data_loader(rng, dataset, batch_size):\n    steps_per_epoch = len(dataset) // batch_size\n    perms = jax.random.permutation(rng, len(dataset))\n    perms = perms[: steps_per_epoch * batch_size]\n    perms = perms.reshape((steps_per_epoch, batch_size))\n\n    for perm in perms:\n        batch = dataset[perm]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch\n        \ndef eval_data_loader(dataset, batch_size):\n    for i in range(len(dataset) // batch_size):\n        batch = dataset[i * batch_size : (i + 1) * batch_size]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch\n        \nstate = flax.jax_utils.replicate(state)\n\nrng = jax.random.PRNGKey(seed)\ndropout_rngs = jax.random.split(rng, jax.local_device_count())\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nmetric_start = RMSE()\nmetric_end = RMSE()\n    \nfor i, epoch in enumerate(tqdm(range(1, num_train_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n    rng, input_rng = jax.random.split(rng)\n    \n    metric_start = RMSE()\n    metric_end = RMSE()\n    \n    for batch in train_data_loader(input_rng, train_dataset, total_batch_size):\n        state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n\n    # evaluate\n    for batch in eval_data_loader(eval_dataset, total_batch_size):\n        start_positions = batch[\"start_positions\"]\n        end_positions = batch[\"end_positions\"]\n        inputs = {k: v for k, v in batch.items() if k not in [\"start_positions\", \"end_positions\"]}\n        start_logits, end_logits = parallel_eval_step(state, inputs)\n        predictions_start = start_logits\n        predictions_end = end_logits\n        references_start = start_positions\n        references_end = end_positions\n        print(predictions_start)\n        print(references_start)\n        metric_start.add_batch(predictions=chain(*predictions_start), references=chain(*references_start))\n        metric_end.add_batch(predictions=chain(*predictions_end), references=chain(*references_end))\n\n    start_rmse = round(metric_start.compute()['RMSE'], 3)\n    end_rmse = round(metric_end.compute()['RMSE'], 3)\n\n    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n    metric_name = \"RMSE\"\n    print(f\"{i+1}/{num_train_epochs} | Train loss: {loss} | Eval {metric_name}: start={start_rmse}, end={end_rmse}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-24T09:15:22.177684Z","iopub.execute_input":"2023-04-24T09:15:22.178071Z","iopub.status.idle":"2023-04-24T09:15:30.726105Z","shell.execute_reply.started":"2023-04-24T09:15:22.178041Z","shell.execute_reply":"2023-04-24T09:15:30.724641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question, text = \"обеспечение гарантийных обязательств\", \"Приложение 1 к заявке – заказу от 2022г изненно необходимых и важнейших лекарственных препаратов в соответствии с постановлением Правительства Российской Федерации от 30112015г 1289 Об ограничениях и условиях допуска происходящих из иностранных государств лекарственных препаратов включенных в перечень жизненно необходимых и важнейших лекарственных препаратов для целей осуществления закупок для обеспечения государственных и муниципальных нужд Нет 1810 Установление запрета на допуск товаров легкой промышленности происходящих из иностранных государств и (или) услуг по прокату таких товаров в соответствии с постановлением Правительства Российской Федерации от 11082014г 791 Об установлении запрета на допуск товаров легкой промышленности происходящих из иностранных государств и (или) услуг по прокату таких товаров в целях осуществления закупок для обеспечения федеральных нужд нужд субъектов Российской Федерации и муниципальных нужд Нет 19 Размер обеспечения заявки на участие в электронном аукционе нет 20 Размер обеспечения исполнения контракта 5% от цены контракта 21 Обеспечение исполнения контракта путем внесения денежных средств на указанный заказчиком счет Реквизиты счета для предоставления обеспечения исполнения контракта Наименование учреждения Муниципальное общеобразовательное бюджетное учреждение средняя общеобразовательная школа сТемясово муниципального района Баймакский район Республики Башкортостан Юридический адрес 453663 Республика Башкортостан Баймакский район сТемясово улСоветская 20 ИНН 0254005845 КПП 025401001 Рс 40102810045370000067 Казначейский счет 03234643806060000100 Наименование банка Отделение НБ Республика Башкортостан БИК 018073401 лс 20103020420 Получатель Муниципальное общеобразовательное бюджетное учреждение средняя общеобразовательная школа сТемясово муниципального района Баймакский район Республики Башкортостан Факт внесения денежных средств в качестве обеспечения исполнения контракта подтверждается платежным поручением с отметкой банка об оплате Денежные средства возвращаются поставщику с По согласованию с заказчиком поставка дров может быть осуществлена в полном объеме в более ранние сроки Кубм 500\"\ninputs = tokenizer(question, text, \n                   truncation=\"only_second\",\n                   max_length=max_length,\n                   stride=150,\n                   return_overflowing_tokens=True,\n                   return_offsets_mapping=True,\n                   padding=\"max_length\",\n                   return_tensors=\"jax\",)\n\noutputs = state.apply_fn(**inputs)\nstart_scores = outputs.start_logits\nend_scores = outputs.end_logits\nstart_scores","metadata":{"execution":{"iopub.status.busy":"2023-04-23T23:59:47.501113Z","iopub.execute_input":"2023-04-23T23:59:47.501830Z","iopub.status.idle":"2023-04-23T23:59:47.595007Z","shell.execute_reply.started":"2023-04-23T23:59:47.501770Z","shell.execute_reply":"2023-04-23T23:59:47.592886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### import os\nimport jax\nfrom flax.serialization import to_bytes, from_bytes\n\n# Определяем путь к файлу, в который мы хотим сохранить модель.\nmodel_path = 'my_model.jax'\n\n# Получаем параметры модели, которые мы хотим сохранить.\nmodel_params = jax.tree_map(lambda x: x.block_until_ready(), state.params)\n\n# Преобразуем параметры в байтовую строку.\nmodel_bytes = to_bytes(model_params)\n\n# Сохраняем модель в файл.\nwith open(model_path, 'wb') as f:\n    f.write(model_bytes)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:50:40.565056Z","iopub.execute_input":"2023-04-23T15:50:40.565474Z","iopub.status.idle":"2023-04-23T15:51:25.801271Z","shell.execute_reply.started":"2023-04-23T15:50:40.565445Z","shell.execute_reply":"2023-04-23T15:51:25.799958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import jax\n# from flax.serialization import from_bytes\n\n# # Определяем путь к файлу, из которого мы хотим загрузить модель.\n# model_path = 'my_model.jax'\n\n# # Загружаем модель из файла.\n# with open(model_path, 'rb') as f:\n#     model_bytes = f.read()\n\n# # Преобразуем байтовую строку в параметры модели.\n# model_params = from_bytes(model_bytes)\n\n# # Создаем новый экземпляр модели, используя загруженные параметры.\n# model = FlaxAutoModelForQuestionAnswering(**model_params)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_data_loader(dataset, batch_size):\n    if len(dataset)<batch_size:\n        batch = dataset[:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        yield batch\n    else:\n        for i in range(len(dataset) // batch_size):\n            batch = dataset[i * batch_size : (i + 1) * batch_size]\n            batch = {k: jnp.array(v) for k, v in batch.items()}\n\n            yield batch\n        batch = dataset[(i+1) * batch_size:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        yield batch\n        \nfrom flax.jax_utils import unreplicate\n\nunrep_state = unreplicate(state)\n\n\ndef test_step(unrep_state, batch):\n    start_logits, end_logits = unrep_state.apply_fn(**batch, params=unrep_state.params, train=False)[0:2]\n    return state.logits_function(start_logits), state.logits_function(end_logits)\n\nparallel_test_step = jax.pmap(test_step, axis_name=\"batch\")\n\ndef generate_results():\n    preds = []\n    for batch in test_data_loader(test_dataset, total_batch_size):\n\n        if jax.process_index() == 0:\n            inputs = {k: v for k, v in batch.items()}\n            start_logits, end_logits = parallel_test_step(state, inputs)\n            preds.append((start_logits, end_logits))\n    return preds\n","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:23:47.226124Z","iopub.execute_input":"2023-04-19T06:23:47.227060Z","iopub.status.idle":"2023-04-19T06:23:48.475211Z","shell.execute_reply.started":"2023-04-19T06:23:47.227018Z","shell.execute_reply":"2023-04-19T06:23:48.473911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = generate_results()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:23:50.086713Z","iopub.execute_input":"2023-04-19T06:23:50.087644Z","iopub.status.idle":"2023-04-19T06:23:53.131172Z","shell.execute_reply.started":"2023-04-19T06:23:50.087602Z","shell.execute_reply":"2023-04-19T06:23:53.129742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# os.environ[\"WANDB_MODE\"] = \"disabled\"\nfrom datasets import Dataset, DatasetDict\nimport datasets\nmodel_name = \"valhalla/longformer-base-4096-finetuned-squadv1\" \"AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru\"\nmax_length = 4000\ntokenizer = AutoTokenizer.from_pretrained(model_name)\npad_on_right = tokenizer.padding_side == \"right\"\nqa_dataset = QADataset(train_data, val_data)\ntokenized_dataset = qa_dataset.dataset_dict.map(prepare_train_features, batched=True, \n                                                 remove_columns=qa_dataset.dataset_dict[\"train\"].column_names)\n\nQAtrainer = QATrainer(\n    model_name=model_name,\n    train_dataset=tokenized_dataset['train'],\n    val_dataset=tokenized_dataset['validation']\n)\n\nQAtrainer.training()\n\n# use_tpu = True  # Change to True if using TPU\n\n# if use_tpu:\n#     # Create distribution strategy\n#     print('TPU used')\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n#     strategy = tf.distribute.TPUStrategy(tpu)\n\n#     with strategy.scope():\n#         # Create model\n#         print('TPU used')\n# #         model = create_model(model_name)\n#         # Create trainer\n#         QAtrainer = QATrainer(\n#             model_name=model_name,\n#             train_dataset=tokenized_dataset['train'],\n#             val_dataset=tokenized_dataset['validation']\n#         )\n# else:\n#     if tf.config.list_physical_devices('GPU'):\n# #         gpus = tf.config.experimental.list_physical_devices('GPU')\n# #         for gpu in gpus:\n# #             tf.config.experimental.set_virtual_device_configuration(\n# #                 gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n# #         tf.config.optimizer.set_jit(True)\n# #         policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n# #         tf.keras.mixed_precision.experimental.set_policy(policy)\n#         strategy = tf.distribute.MirroredStrategy()\n#     else:\n#         strategy = tf.distribute.OneDeviceStrategy(device=\"/CPU:0\")\n\n#     with strategy.scope():\n# #         model = create_model(model_name)\n#         # Create trainer\n#         QAtrainer = QATrainer(\n#             model_name=model_name,\n#             train_dataset=tokenized_dataset['train'],\n#             val_dataset=tokenized_dataset['validation']\n#         )\n\n# # Train the model\n# QAtrainer.training()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T13:06:28.834300Z","iopub.execute_input":"2023-04-18T13:06:28.834993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tokenized_dataset['train']['attention_mask'][6])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets['validation']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qa_dataset = QADataset(train_data, val_data)\nqa_dataset.dataset_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip list | grep transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataframe(data, fields, subfields):\n    main_df = pd.DataFrame(data)[fields]\n    sub_df_list = []\n    for subfield in subfields:\n        sub_df = pd.DataFrame(list(main_df[subfield]))\n        sub_df.columns = [f\"{subfield}_{col}\" for col in sub_df.columns]\n        sub_df_list.append(sub_df)\n    main_df = main_df.drop(columns=['extracted_part'])\n    return pd.concat([main_df] + sub_df_list, axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_qa_examples(train_data)[0]['context']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmax_seq_length = 4000\nmodel_name = \"valhalla/longformer-base-4096-finetuned-squadv1\"\n# LongformerTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = LongformerConfig.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')\nconfig.attention_mode = 'sliding_chunks'\n\nnum_epochs = 3\nbatch_size = 16\npad_on_right = tokenizer.padding_side == \"right\"\ntrain_dataset = QADataset(train_data, model_name, max_seq_length)\nval_dataset = QADataset(val_data, model_name, max_seq_length)\n# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\nassert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer('<s>', '<s>')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"QAtrainer = QATrainer(\n    model_name=model_name,\n    train_dataset=tokenized_dataset['train'],\n    val_dataset=tokenized_dataset['validation']\n)\n\nQAtrainer.training()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TPU used try 2","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import default_data_collator\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata_collator = default_data_collator\nargs = TrainingArguments(\n    model_name,\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    push_to_hub=True,\n)\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['validation'],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfrom transformers import BertConfig, BertModel\n\n# bert_config = BertConfig(\n#     vocab_size=32000,\n#     hidden_size=768,\n#     num_hidden_layers=12,\n#     num_attention_heads=12,\n#     intermediate_size=3072,\n#     hidden_dropout_prob=0.1,\n#     attention_probs_dropout_prob=0.1,\n#     max_position_embeddings=512,\n#     type_vocab_size=2,\n#     initializer_range=0.02,\n#     layer_norm_eps=1e-12,\n#     gradient_checkpointing=False,\n#     position_embedding_type=\"absolute\",\n#     use_cache=True,\n#     is_decoder=False,\n#     pad_token_id=0,\n#     bos_token_id=1,\n#     eos_token_id=2\n# )\n\ntokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\nmax_seq_length = 4000\nbatch_size = 16\nepochs = eps = 1\n     \ntrain_dataset = QADataset(train_data, tokenizer, max_seq_length)\nval_dataset = QADataset(val_data, tokenizer, max_seq_length)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n\n# qa_model = QAModel(bert_config)\n# qa_trainer = QATrainer(qa_model, train_dataloader, val_dataloader, lr=1e-12, eps=eps)\n# train_losses, val_losses = qa_trainer.train(epochs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, val_loader):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for step, batch in enumerate(val_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            segment_ids = batch['segment_ids'].to(device)\n            start_positions = batch['start_positions'].to(device)\n            end_positions = batch['end_positions'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=segment_ids, start_positions=start_positions, end_positions=end_positions)\n            loss = outputs.loss\n            total_loss += loss.item()\n        avg_loss = total_loss / len(val_loader)\n        return avg_loss\n\ndef predict(model, test_loader):\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for step, batch in enumerate(test_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            segment_ids = batch['segment_ids'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=segment_ids)\n            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n            start_preds = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n            end_preds = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n            for i in range(len(start_preds)):\n                start_pred = np.argmax(start_preds[i])\n                end_pred = np.argmax(end_preds[i])\n                if start_pred > end_pred:\n                    answer = \"\"\n                else:\n                    answer = tokenizer.decode(input_ids[i][start_pred:end_pred+1], skip_special_tokens=True)\n                predictions.append({\n                    \"context\": batch['context'][i],\n                    \"question\": batch['question'][i],\n                    \"extracted_part\": answer\n                })\n    with open('predictions.json', 'w', encoding='utf-8') as f:\n        json.dump(predictions, f, ensure_ascii=False, indent=4)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class QADataset(Dataset):\n    def __init__(self, data, tokenizer, max_seq_length):\n        self.examples = self.create_qa_examples(data)\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n        self.skip = False\n\n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        context = example['context']\n        question = example['question']\n        answer = example['answer']\n        answer_start = example['answer_start']\n        answer_end = example['answer_end']\n        assert answer_end <= len(example['context'])\n        is_char_in_ans = [0] * len(context)\n        for i in range(answer_start, answer_end):\n            is_char_in_ans[i] = 1\n        tokenized_context = self.tokenizer.encode_plus(context, add_special_tokens=False, return_offsets_mapping=True, return_tensors=\"tf\")\n        ans_token_idx = []\n        is_ans_token = [0] * len(tokenized_context)\n\n        for idx, token in enumerate(tokenized_context):\n            token_start = tokenized_context.token_to_chars(idx)[0]\n            token_end = tokenized_context.token_to_chars(idx)[1]\n            if sum(is_char_in_ans[token_start:token_end]) > 0:\n                ans_token_idx.append(idx)\n                for i in range(token_start, token_end):\n                    is_ans_token[i] = 1\n        if sum(is_ans_token) == 0:\n            start_token_idx, end_token_idx = 0, 0\n        else:\n            start_token_idx = ans_token_idx[0]\n            end_token_idx = ans_token_idx[-1]\n            while start_token_idx > 0 and is_ans_token[tokenized_context.token_to_chars(start_token_idx-1)[0]]:\n                start_token_idx -= 1\n            while end_token_idx < len(tokenized_context)-1 and is_ans_token[tokenized_context.token_to_chars(end_token_idx+1)[1]-1]:\n                end_token_idx += 1\n            \n\n        tokenized_question = self.tokenizer.encode_plus(question, return_offsets_mapping=True, return_tensors=\"tf\")\n        tokens = ['<s>'] + tokenized_context.tokens() + ['</s>']+ ['</s>'] + tokenized_question.tokens() + ['</s>']\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        token_type_ids = [0] * (len(tokenized_context.tokens())+2) + [1] * (len(\n            tokenized_question.tokens())+2)\n        attention_mask = [1] * len(input_ids)\n        padding_length = self.max_seq_length - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n        features = []\n#         encoded_dict = self.tokenizer.encode_plus(\n#             question,\n#             context,\n#             add_special_tokens=True,\n#             truncation='longest_first',\n#             max_length=self.max_seq_length,\n#             return_tensors='pt'\n#         )\n#         input_ids = encoded_dict['input_ids'].squeeze()\n#         attention_mask = encoded_dict['attention_mask'].squeeze()\n#         input_ids = torch.nn.functional.pad(encoded_dict['input_ids'], (0, self.max_seq_length - encoded_dict['input_ids'].shape[1]), mode='constant', value=0)\n#         attention_mask = torch.nn.functional.pad(encoded_dict['attention_mask'], (0, self.max_seq_length - encoded_dict['attention_mask'].shape[1]), mode='constant', value=0)\n        \n        features = {'input_ids': input_ids, 'attention_mask': attention_mask, \n                    'token_type_ids': token_type_ids, 'start_token_idx': start_token_idx, 'end_token_idx': end_token_idx}\n#         max_len_dict = {}\n#         for key, value in features.items():\n#             if isinstance(value, (list, tuple)):\n#                 max_len_dict[key] = max(len(seq) for seq in value)\n#         for key, value in features.items():\n#             if isinstance(value, (list, tuple)):\n#                 max_len = max_len_dict[key]\n#                 for i in range(len(value)):\n#                     pad_len = max_len - len(value[i])\n#                     value[i] = torch.cat([value[i], torch.zeros(pad_len, dtype=torch.long)])\n#                 features[key] = torch.stack(value)\n\n        return features\n    \n    def create_qa_examples(self, data):\n        examples = []\n        for row in data:\n            text = row['text']\n            question = row['label']\n            extracted_part = row.get('extracted_part', {})\n            if extracted_part and 'text' in extracted_part:\n                answer = extracted_part['text'][0].strip()\n                answer_start = extracted_part['answer_start'][0]\n                answer_end = extracted_part['answer_end'][0]\n            else:\n                answer = answer_start = answer_end = None\n\n            example = {'context': text, 'question': question, 'answer': answer, 'answer_start': answer_start, 'answer_end': answer_end}\n            examples.append(example)\n        return examples\n    \n    \n    @staticmethod\n    def prepare_test_data(data):\n        examples = []\n        for row in data:\n            text = row['text']\n            question = row['label']\n            example = {'context': text, 'question': question}\n            examples.append(example)\n        return examples\n\ndef collate_fn(batch, device):\n    input_ids = pad_sequence([torch.tensor(example['input_ids']) for example in batch], batch_first=True, padding_value=0).to(device)\n    attention_mask = pad_sequence([torch.tensor(example['attention_mask']) for example in batch], batch_first=True, padding_value=0).to(device)\n    token_type_ids = pad_sequence([torch.tensor(example['token_type_ids']) for example in batch], batch_first=True, padding_value=0).to(device)\n    start_positions = torch.tensor([example['answer_start'] for example in batch]).to(device)\n    end_positions = torch.tensor([example['answer_end'] for example in batch]).to(device)\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'token_type_ids': token_type_ids,\n        'start_positions': start_positions,\n        'end_positions': end_positions\n    }\n\n\ndef create_inputs_targets(dataset):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n        \"start_token_idx\": [],\n        \"end_token_idx\": [],\n    }\n    for idx in range(len(dataset)):\n        example = dataset[idx]\n        for key in dataset_dict:\n            if isinstance(example[key], torch.Tensor):\n                value = example[key].numpy().tolist()\n            else:\n                value = example[key]\n            dataset_dict[key].append(value)\n\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n    return x, y\n\ndef x_y_split(model_name, train_data, validation_data, batch_size):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)\n#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    train_dataset = QADataset(train_data, tokenizer, max_seq_length)\n#     train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True,\n#                                    collate_fn=lambda batch: collate_fn(batch, device))\n    x_train, y_train = create_inputs_targets(train_dataset)\n    \n    validation_dataset = QADataset(validation_data, tokenizer, max_seq_length)\n#     validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, drop_last=True,\n#                                         collate_fn=lambda batch: collate_fn(batch, device))\n    x_val, y_val = create_inputs_targets(validation_dataset)\n\n    return x_train, y_train, x_val, y_val\n\n\n\ndef create_model(model_name):\n    ## BERT encoder\n    encoder = TFLongformerForQuestionAnswering.from_pretrained(model_name)\n\n    ## QA Model\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model\n\nclass ExactMatch(keras.callbacks.Callback):\n    def __init__(self, x_eval, y_eval):\n        super().__init__()\n        self.x_eval = x_eval\n        self.y_eval = y_eval\n\n    def on_epoch_end(self, epoch, logs=None):\n        pred_start, pred_end = self.model.predict(self.x_eval)\n        count = 0\n        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n            offsets = squad_eg.context_token_to_char\n            start = np.argmax(start)\n            end = np.argmax(end)\n            if start >= len(offsets):\n                continue\n            pred_char_start = offsets[start][0]\n            if end < len(offsets):\n                pred_char_end = offsets[end][1]\n                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n            else:\n                pred_ans = squad_eg.context[pred_char_start:]\n\n            if pred_ans in squad_eg.all_answers:\n                count += 1\n        acc = count / len(self.y_eval[0])\n        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\nexamp = QADataset(train_data, tokenizer, max_seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"QADataset(train_data, tokenizer, max_seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# загружаем данные для обучения\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/train.json', 'r', encoding='utf-8') as file:\n    train_data = json.load(file)\n\n# загружаем данные для предсказания\nwith open('/kaggle/input/nlp-test-task-2023/nlp_test_task_2023/dataset/test.json', 'r', encoding='utf-8') as file:\n    test_data = json.load(file)\n\n# Разбиваем данные на обучающую и валидационную выборки\ntrain_data, validation_data = train_test_split(train_data[:50], test_size=0.2, random_state=42)\nmax_seq_length = 4000\nmodel_name = \"valhalla/longformer-base-4096-finetuned-squadv1\"\nconfiguration = LongformerConfig()\nnum_epochs = 3\n\nx_train, y_train, x_val, y_val = x_y_split(model_name = model_name, train_data = train_data, validation_data = validation_data, batch_size = 16)\nmax_len = len(x_train[0][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y_train[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train имеет структуру словаря, в котором есть 3 подсловаря - признака, в каждом из них набор примеров n-го количества, в каждом примере уже непосредственно находятся данные\nв y_train 2 словаря, которые содержат n примеров, в каждом из которых находится таргет. \nкакие модели обучения можно написать на таких данных, не пользуясь предобученными моделями и их ограничениями","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntf.debugging.set_log_device_placement(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_tpu = False  # Change to True if using TPU\n\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    strategy = tf.distribute.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        print('TPU used')\n        model = create_model(model_name)\nelse:\n#     # Use GPU if TPU is not available\n#     if tf.config.list_physical_devices('GPU'):\n#         strategy = tf.distribute.MirroredStrategy()\n        \n#     else:\n    strategy = tf.distribute.OneDeviceStrategy(device=\"/CPU:0\")\n\n    with strategy.scope():\n        model = create_model(model_name)\n\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nprint(tf.test.is_built_with_cuda())\nprint(tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx_train_np = [np.array(x_train[0]), np.array(x_train[1]), np.array(x_train[2])]\ny_train_np = [np.array(y_train[0]), np.array(y_train[1])]\nx_val_np = [np.array(x_val[0]), np.array(x_val[1]), np.array(x_val[2])]\ny_val_np = [np.array(y_val[0]), np.array(y_val[1])]\nexact_match_callback = ExactMatch(x_val_np, y_val_np)\nmodel.fit(\n    x_train_np,\n    y_train_np,\n    validation_data=(x_val_np, y_val_np),\n    epochs=1,\n    verbose=2,\n    batch_size=64,\n#     callbacks=[exact_match_callback],\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = QADataset(train_data, tokenizer, max_seq_length)\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True,\n                               collate_fn=lambda batch: collate_fn(batch, device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader.dataset[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained('cointegrated/LaBSE-en-ru')\n# examples = QADataset.create_qa_examples(train_data, train_data)\n# questions = [example['context'] for example in examples]\n# question_tokens = [tokenizer.tokenize(question) for question in questions]\n# import matplotlib.pyplot as plt\n\n# question_lengths = [len(tokens) for tokens in question_tokens]\n# plt.hist(question_lengths, bins=50)\n# plt.xlabel('Length of question tokens')\n# plt.ylabel('Frequency')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Разбиваем данные на обучающую и валидационную выборки\n# train_data, validation_data = train_test_split(train_data, test_size=0.2, random_state=42)\n# max_seq_length = 3072\n# model_name = \"allenai/longformer-large-4096-finetuned-triviaqa\"\n\n# # Число эпох обучения\n# num_epochs = 3\n# output_dir = 'my_model'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'cointegrated/LaBSE-en-ru'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import BigBirdTokenizer, BigBirdForQuestionAnswering\n\n# tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n# model = BigBirdForQuestionAnswering.from_pretrained('google/bigbird-roberta-base')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# import tensorflow as tf\n\n# train(model_name, train_data, validation_data, output_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}